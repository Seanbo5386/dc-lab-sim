{
  "examId": "ncp-aii-practice",
  "title": "NCP-AII Practice Examination",
  "description": "Practice exam for NVIDIA Certified Professional - AI Infrastructure (NCP-AII) certification. This exam covers all five domains with appropriate weighting.",
  "duration": 90,
  "passingScore": 70,
  "domainWeights": {
    "domain1": 31,
    "domain2": 5,
    "domain3": 19,
    "domain4": 33,
    "domain5": 12
  },
  "questions": [
    {
      "id": "q001",
      "domain": "domain1",
      "questionText": "Which command is used to view the BIOS version on a DGX system via IPMI?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool fru print 0",
        "ipmitool bios version",
        "ipmitool mc info",
        "dmidecode -t bios"
      ],
      "correctAnswer": 0,
      "explanation": "'ipmitool fru print 0' displays Field Replaceable Unit information including BIOS version. While 'dmidecode -t bios' also works from the OS, the question specifically asks about IPMI access.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q002",
      "domain": "domain1",
      "questionText": "What is the purpose of the BMC (Baseboard Management Controller)?",
      "type": "multiple-choice",
      "choices": [
        "To manage GPU memory allocation",
        "To provide out-of-band management and monitoring of server hardware",
        "To control network traffic between nodes",
        "To schedule workloads on the cluster"
      ],
      "correctAnswer": 1,
      "explanation": "The BMC provides out-of-band management, allowing administrators to monitor and control server hardware (power, sensors, inventory) even when the OS is down.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q003",
      "domain": "domain1",
      "questionText": "Which IPMI command shows real-time sensor readings for temperature, voltage, and fan speed?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool sel list",
        "ipmitool sensor list",
        "ipmitool chassis status",
        "ipmitool fru print"
      ],
      "correctAnswer": 1,
      "explanation": "'ipmitool sensor list' displays real-time readings from all sensors. 'ipmitool sel list' shows the System Event Log (historical events), not real-time sensors.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q004",
      "domain": "domain1",
      "questionText": "What does the nvidia-smi command 'nvidia-smi -pm 1' do?",
      "type": "multiple-choice",
      "choices": [
        "Sets GPU power limit to 1 watt",
        "Enables persistence mode",
        "Enables MIG mode",
        "Resets GPU 1"
      ],
      "correctAnswer": 1,
      "explanation": "The '-pm 1' flag enables persistence mode, which keeps the NVIDIA driver loaded even when no processes are using the GPU. This reduces job initialization latency.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q005",
      "domain": "domain1",
      "questionText": "Which command verifies that all installed memory modules are detected after POST?",
      "type": "multiple-choice",
      "choices": ["lspci", "nvidia-smi", "free -h", "ipmitool sensor list"],
      "correctAnswer": 2,
      "explanation": "'free -h' shows total available system memory. Compare this against the expected physical installation to verify all memory was detected during POST.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q006",
      "domain": "domain1",
      "questionText": "What is the NVIDIA vendor ID shown in lspci output?",
      "type": "multiple-choice",
      "choices": ["8086", "1022", "10de", "15b3"],
      "correctAnswer": 2,
      "explanation": "10de is NVIDIA's PCI vendor ID. This can be used with 'lspci -d 10de:' to list all NVIDIA devices. 8086=Intel, 1022=AMD, 15b3=Mellanox.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q007",
      "domain": "domain1",
      "questionText": "After installing a new NVIDIA driver, what must be done for the driver to load?",
      "type": "multiple-choice",
      "choices": [
        "Run nvidia-smi",
        "Reboot the system or reload the nvidia kernel module",
        "Run dcgmi discovery",
        "Enable persistence mode"
      ],
      "correctAnswer": 1,
      "explanation": "After driver installation, the system must be rebooted or the nvidia kernel modules manually unloaded and reloaded for the new driver to take effect.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q008",
      "domain": "domain1",
      "questionText": "Which file contains the NVIDIA driver version information?",
      "type": "multiple-choice",
      "choices": [
        "/proc/version",
        "/proc/driver/nvidia/version",
        "/sys/module/nvidia/version",
        "/etc/nvidia/version"
      ],
      "correctAnswer": 1,
      "explanation": "The file /proc/driver/nvidia/version contains the loaded NVIDIA driver version. This can also be checked with 'modinfo nvidia' or 'nvidia-smi'.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q009",
      "domain": "domain1",
      "questionText": "What does a 'POST Error' in the System Event Log typically indicate?",
      "type": "multiple-choice",
      "choices": [
        "GPU driver failure",
        "Hardware initialization failure during boot",
        "Network connectivity issue",
        "Slurm scheduling problem"
      ],
      "correctAnswer": 1,
      "explanation": "POST (Power-On Self-Test) errors indicate hardware failed to initialize properly during the boot sequence. This could be CPU, memory, PCIe devices, etc.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q010",
      "domain": "domain1",
      "questionText": "Which command shows detailed PCIe link information including current link speed and width?",
      "type": "multiple-choice",
      "choices": ["lspci -tv", "lspci -vv", "nvidia-smi", "dmidecode"],
      "correctAnswer": 1,
      "explanation": "'lspci -vv' shows verbose PCIe device information including link speed (Gen3/Gen4), link width (x16), and capabilities. The -tv flag shows topology only.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q011",
      "domain": "domain2",
      "questionText": "What does MIG (Multi-Instance GPU) allow you to do?",
      "type": "multiple-choice",
      "choices": [
        "Connect multiple GPUs with NVLink",
        "Partition a single GPU into multiple isolated instances",
        "Run multiple processes on one GPU",
        "Increase GPU memory capacity"
      ],
      "correctAnswer": 1,
      "explanation": "MIG allows partitioning a single GPU into up to 7 isolated instances, each with dedicated compute, memory, and bandwidth resources. This provides strong isolation for multi-tenant scenarios.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q012",
      "domain": "domain2",
      "questionText": "Which NVIDIA GPUs support MIG mode?",
      "type": "multiple-select",
      "choices": ["A100", "V100", "H100", "A30"],
      "correctAnswer": [0, 2, 3],
      "explanation": "MIG is supported on A100, A30, and H100 GPUs. V100 and earlier generations do not support MIG. MIG requires Ampere architecture or newer.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q013",
      "domain": "domain3",
      "questionText": "What is the purpose of Slurm GRES (Generic Resources)?",
      "type": "multiple-choice",
      "choices": [
        "To schedule jobs across multiple clusters",
        "To track consumable resources like GPUs for job scheduling",
        "To manage user accounts and permissions",
        "To monitor GPU health"
      ],
      "correctAnswer": 1,
      "explanation": "GRES allows Slurm to track and schedule consumable resources like GPUs. Users request GPUs with --gres=gpu:N, and Slurm ensures proper allocation and isolation.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q014",
      "domain": "domain3",
      "questionText": "Which Slurm command shows detailed information about all nodes including state and GRES?",
      "type": "multiple-choice",
      "choices": ["squeue", "sinfo -Nel", "sacct", "scontrol show jobs"],
      "correctAnswer": 1,
      "explanation": "'sinfo -Nel' shows detailed node information including state, partitions, and GRES. squeue shows jobs, sacct shows job history.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q015",
      "domain": "domain3",
      "questionText": "What does the '--gres=gpu:h100:2' sbatch option request?",
      "type": "multiple-choice",
      "choices": [
        "2 H100 GPUs",
        "GPU 2 only",
        "2 GB of GPU memory",
        "2 MIG instances"
      ],
      "correctAnswer": 0,
      "explanation": "The format is --gres=type:name:count. This requests 2 GPUs of type h100. Without the name, --gres=gpu:2 would request any 2 GPUs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q016",
      "domain": "domain3",
      "questionText": "Which tool is used to run GPU-accelerated containers with proper GPU access?",
      "type": "multiple-choice",
      "choices": [
        "Docker with --runtime=nvidia",
        "Docker with --gpus flag",
        "NVIDIA Container Toolkit",
        "Both B and C"
      ],
      "correctAnswer": 3,
      "explanation": "NVIDIA Container Toolkit enables GPU access in containers. Modern usage is 'docker run --gpus all', while older versions used --runtime=nvidia.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q017",
      "domain": "domain3",
      "questionText": "What is Enroot primarily designed for?",
      "type": "multiple-choice",
      "choices": [
        "GPU monitoring",
        "Unprivileged container execution in HPC environments",
        "Network configuration",
        "Storage management"
      ],
      "correctAnswer": 1,
      "explanation": "Enroot is designed for unprivileged container execution in HPC environments, with native Slurm integration via pyxis. It doesn't require a daemon like Docker.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q018",
      "domain": "domain3",
      "questionText": "Where are optimized AI framework containers hosted by NVIDIA?",
      "type": "multiple-choice",
      "choices": [
        "Docker Hub",
        "NVIDIA NGC (nvcr.io)",
        "GitHub Container Registry",
        "AWS ECR"
      ],
      "correctAnswer": 1,
      "explanation": "NVIDIA GPU Cloud (NGC) at nvcr.io hosts optimized containers for PyTorch, TensorFlow, and other AI frameworks with latest CUDA/cuDNN versions.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q019",
      "domain": "domain3",
      "questionText": "What Slurm command cancels a running job?",
      "type": "multiple-choice",
      "choices": [
        "srun --cancel <jobid>",
        "scancel <jobid>",
        "scontrol delete <jobid>",
        "skillall <jobid>"
      ],
      "correctAnswer": 1,
      "explanation": "'scancel <jobid>' cancels the specified job. You can also use 'scancel -u <username>' to cancel all jobs for a user.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q020",
      "domain": "domain4",
      "questionText": "What does DCGM Level 1 diagnostics test?",
      "type": "multiple-choice",
      "choices": [
        "GPU hardware stress tests",
        "Software deployment and basic GPU properties",
        "Multi-hour burn-in tests",
        "Network bandwidth"
      ],
      "correctAnswer": 1,
      "explanation": "DCGM Level 1 tests software deployment, permissions, libraries, and basic GPU properties. It's non-intrusive and completes in seconds. Levels 2-3 include stress tests.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q021",
      "domain": "domain4",
      "questionText": "How long does DCGM Level 3 diagnostics typically take per GPU?",
      "type": "multiple-choice",
      "choices": ["10-30 seconds", "1-2 minutes", "10-15 minutes", "1-2 hours"],
      "correctAnswer": 2,
      "explanation": "Level 3 diagnostics run extended stress tests and take 10-15 minutes per GPU. Use Level 3 for new hardware acceptance or after repairs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q022",
      "domain": "domain4",
      "questionText": "Which NCCL collective operation is typically used in distributed training to aggregate gradients?",
      "type": "multiple-choice",
      "choices": ["all-gather", "all-reduce", "broadcast", "reduce-scatter"],
      "correctAnswer": 1,
      "explanation": "All-reduce is used to sum gradients across all GPUs and distribute the result back to all GPUs. This is the core operation in data-parallel training.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q023",
      "domain": "domain4",
      "questionText": "What does 'algBW' represent in NCCL test output?",
      "type": "multiple-choice",
      "choices": [
        "Algorithm bandwidth - effective bandwidth for the operation",
        "Algebraic bandwidth - theoretical maximum",
        "Allocated bandwidth - reserved capacity",
        "Aggregate bandwidth - total across all GPUs"
      ],
      "correctAnswer": 0,
      "explanation": "algBW (algorithm bandwidth) is the effective bandwidth from the application's perspective. busBW accounts for actual data moved on the interconnect.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q024",
      "domain": "domain4",
      "questionText": "For optimal NCCL performance on DGX systems, which interconnect should be used for intra-node communication?",
      "type": "multiple-choice",
      "choices": ["PCIe", "NVLink", "Ethernet", "InfiniBand"],
      "correctAnswer": 1,
      "explanation": "NVLink provides the highest bandwidth for GPU-to-GPU communication within a node. NCCL automatically detects and uses NVLink when available.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q025",
      "domain": "domain4",
      "questionText": "What does setting NCCL_DEBUG=INFO do?",
      "type": "multiple-choice",
      "choices": [
        "Disables NCCL",
        "Shows detailed initialization and configuration information",
        "Enables performance profiling",
        "Forces CPU fallback mode"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL_DEBUG=INFO provides detailed logging about NCCL initialization, topology detection, and communication setup. Very useful for troubleshooting.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q026",
      "domain": "domain4",
      "questionText": "Which command runs DCGM diagnostics in Level 2 mode?",
      "type": "multiple-choice",
      "choices": [
        "dcgmi diag -r 2",
        "dcgmi test --level 2",
        "dcgmi validate -l2",
        "dcgmi health -r 2"
      ],
      "correctAnswer": 0,
      "explanation": "'dcgmi diag -r 2' (or --mode 2) runs Level 2 diagnostics. Level 2 includes short stress tests and takes 1-2 minutes per GPU.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q027",
      "domain": "domain4",
      "questionText": "What is the expected NVLink bandwidth per direction on an H100 GPU with 18 links?",
      "type": "multiple-choice",
      "choices": ["300 GB/s", "450 GB/s", "600 GB/s", "900 GB/s"],
      "correctAnswer": 1,
      "explanation": "H100 has 18 NVLink 4.0 links at 25 GB/s each = 450 GB/s per direction, 900 GB/s bidirectional total. A100 has 12 links = 300 GB/s per direction.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q028",
      "domain": "domain4",
      "questionText": "Which DCGM command monitors GPU metrics in real-time?",
      "type": "multiple-choice",
      "choices": ["dcgmi stats", "dcgmi dmon", "dcgmi watch", "dcgmi monitor"],
      "correctAnswer": 1,
      "explanation": "'dcgmi dmon' provides real-time monitoring of GPU metrics (utilization, temperature, power, etc.) similar to nvidia-smi dmon.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q029",
      "domain": "domain4",
      "questionText": "What does the BCM command 'bcm validate pod' do?",
      "type": "multiple-choice",
      "choices": [
        "Validates Kubernetes pod configuration",
        "Runs comprehensive SuperPOD configuration checks",
        "Validates container images",
        "Checks pod network connectivity"
      ],
      "correctAnswer": 1,
      "explanation": "'bcm validate pod' runs comprehensive validation checks on a SuperPOD configuration including nodes, GPUs, network, storage, and Slurm.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q030",
      "domain": "domain4",
      "questionText": "Which metric indicates GPU compute utilization in nvidia-smi?",
      "type": "multiple-choice",
      "choices": ["Pwr:Usage/Cap", "Temp", "GPU-Util", "Mem-Util"],
      "correctAnswer": 2,
      "explanation": "GPU-Util shows the percentage of time the GPU was actively processing. Mem-Util shows memory bandwidth utilization (separate metric).",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q031",
      "domain": "domain4",
      "questionText": "What is a good NCCL all-reduce bandwidth for 8x H100 GPUs at 128MB message size?",
      "type": "multiple-choice",
      "choices": ["50-80 GB/s", "100-150 GB/s", "200-300 GB/s", "400-500 GB/s"],
      "correctAnswer": 2,
      "explanation": "On DGX H100 with NVSwitch, expect 200-300 GB/s algBW for all-reduce at large message sizes. Lower values indicate topology or configuration issues.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q032",
      "domain": "domain5",
      "questionText": "What does Xid 79 indicate?",
      "type": "multiple-choice",
      "choices": [
        "GPU thermal throttling",
        "ECC memory error",
        "GPU has fallen off the bus",
        "Driver version mismatch"
      ],
      "correctAnswer": 2,
      "explanation": "Xid 79 means the GPU has fallen off the bus - PCIe communication failure. This is a critical error often requiring reboot or hardware replacement.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q033",
      "domain": "domain5",
      "questionText": "Where are XID errors logged?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi output only",
        "Kernel logs (dmesg)",
        "/var/log/nvidia/xid.log",
        "BMC System Event Log"
      ],
      "correctAnswer": 1,
      "explanation": "XID errors are logged to kernel logs viewable with dmesg. Look for 'NVRM: Xid' messages. They may also appear in journalctl output.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q034",
      "domain": "domain5",
      "questionText": "At what temperature do datacenter GPUs typically begin thermal throttling?",
      "type": "multiple-choice",
      "choices": ["70°C", "85-90°C", "100°C", "120°C"],
      "correctAnswer": 1,
      "explanation": "Datacenter GPUs typically begin thermal throttling around 85-90°C depending on the model. Maximum safe operating temperature is usually 90-95°C.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q035",
      "domain": "domain5",
      "questionText": "Which nvidia-smi flag shows the reasons for GPU clock throttling?",
      "type": "multiple-choice",
      "choices": [
        "--query-gpu=clocks.throttle_reasons",
        "--query-gpu=clocks_throttle_reasons.active",
        "--query-gpu=throttle.status",
        "--show-throttling"
      ],
      "correctAnswer": 1,
      "explanation": "'--query-gpu=clocks_throttle_reasons.active' shows active throttle reasons: thermal, power, sw_thermal, hw_slowdown, etc.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q036",
      "domain": "domain1",
      "questionText": "Which command shows the installed CUDA Toolkit version on a system?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi",
        "nvcc --version",
        "cuda-version",
        "cat /usr/local/cuda/version"
      ],
      "correctAnswer": 1,
      "explanation": "'nvcc --version' shows the CUDA compiler version, which corresponds to the installed CUDA Toolkit version. nvidia-smi shows the maximum CUDA version supported by the driver.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q037",
      "domain": "domain1",
      "questionText": "What is the purpose of the NVIDIA Fabric Manager?",
      "type": "multiple-choice",
      "choices": [
        "Manages container deployments",
        "Controls NVSwitch and manages GPU-to-GPU communication on NVLink systems",
        "Configures network fabrics for InfiniBand",
        "Manages Slurm job scheduling"
      ],
      "correctAnswer": 1,
      "explanation": "Fabric Manager controls NVSwitch and manages GPU-to-GPU communication topology on systems with NVLink. Required for DGX A100/H100 systems with NVSwitch.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q038",
      "domain": "domain1",
      "questionText": "Which log file should you check first when investigating GPU initialization failures during boot?",
      "type": "multiple-choice",
      "choices": [
        "/var/log/nvidia-installer.log",
        "dmesg output",
        "/var/log/Xorg.0.log",
        "/proc/driver/nvidia/version"
      ],
      "correctAnswer": 1,
      "explanation": "dmesg shows kernel ring buffer messages including GPU driver initialization. Look for NVIDIA driver load messages and any error codes like XID errors.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q039",
      "domain": "domain1",
      "questionText": "What does the 'ipmitool chassis power cycle' command do?",
      "type": "multiple-choice",
      "choices": [
        "Gracefully shuts down the OS then powers on",
        "Immediately cuts power, waits briefly, then powers on",
        "Puts the system into standby mode",
        "Restarts only the BMC"
      ],
      "correctAnswer": 1,
      "explanation": "Power cycle immediately cuts AC power (hard power off), waits a few seconds, then powers back on. This is a last resort for unresponsive systems.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q040",
      "domain": "domain2",
      "questionText": "Which command enables MIG mode on GPU 0?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -i 0 --mig-enable",
        "nvidia-smi -i 0 -mig 1",
        "dcgmi mig --enable --gpuId 0",
        "mig-config enable 0"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -i 0 -mig 1' enables MIG mode on GPU 0. After enabling, system reboot is required for the change to take effect.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q041",
      "domain": "domain2",
      "questionText": "What is the maximum number of MIG instances on an A100 80GB GPU?",
      "type": "multiple-choice",
      "choices": ["4", "7", "8", "16"],
      "correctAnswer": 1,
      "explanation": "An A100 can be partitioned into up to 7 MIG instances (using 1g.10gb profile). Different profiles are available: 1g, 2g, 3g, 4g, 7g representing GPU slice sizes.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q042",
      "domain": "domain2",
      "questionText": "Which command displays the current NVLink topology and connection status?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi nvlink --status",
        "nvidia-smi topo -m",
        "nvlink-config show",
        "dcgmi topo"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi topo -m' shows the NVLink/NVSwitch topology matrix, indicating which GPUs are connected and via which interconnect (NV1-NV18, SYS, etc.).",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q043",
      "domain": "domain3",
      "questionText": "What is the purpose of the 'scontrol update NodeName=node01 State=RESUME' command?",
      "type": "multiple-choice",
      "choices": [
        "Reboots the node",
        "Returns a DRAINED node to service",
        "Pauses all jobs on the node",
        "Updates node configuration"
      ],
      "correctAnswer": 1,
      "explanation": "This command returns a DRAINED node to active service. Use 'State=DRAIN' to remove a node from scheduling, 'State=RESUME' to return it.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q044",
      "domain": "domain3",
      "questionText": "Which file defines Slurm GPU GRES configuration?",
      "type": "multiple-choice",
      "choices": [
        "/etc/slurm/slurm.conf",
        "/etc/slurm/gres.conf",
        "/etc/slurm/gpu.conf",
        "/etc/slurm/topology.conf"
      ],
      "correctAnswer": 1,
      "explanation": "gres.conf defines GRES (Generic Resources) including GPU configurations with names, types, and device files. slurm.conf references this configuration.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q045",
      "domain": "domain3",
      "questionText": "What does the BCM (Base Command Manager) primarily manage?",
      "type": "multiple-choice",
      "choices": [
        "Individual DGX workstations",
        "NVIDIA SuperPOD clusters and infrastructure",
        "GPU firmware updates only",
        "Container registries"
      ],
      "correctAnswer": 1,
      "explanation": "BCM is NVIDIA's cluster management software for SuperPOD infrastructure, providing node provisioning, monitoring, health checks, and configuration management.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q046",
      "domain": "domain3",
      "questionText": "Which storage protocol is commonly used for high-performance shared storage in HPC clusters?",
      "type": "multiple-choice",
      "choices": ["NFS", "iSCSI", "Lustre or BeeGFS", "CIFS/SMB"],
      "correctAnswer": 2,
      "explanation": "Lustre and BeeGFS are parallel distributed file systems designed for HPC, providing high throughput and scalability. NFS is simpler but lower performance.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q047",
      "domain": "domain4",
      "questionText": "What does a low 'busBW' relative to 'algBW' in NCCL tests indicate?",
      "type": "multiple-choice",
      "choices": [
        "Good performance - efficient algorithm",
        "Poor performance - interconnect saturation",
        "Memory bandwidth bottleneck",
        "CPU bottleneck"
      ],
      "correctAnswer": 0,
      "explanation": "algBW is application-level bandwidth, busBW is actual data transferred. For all-reduce, each byte is sent/received once but travels across links, so busBW is typically 2x algBW. Lower ratio means efficient ring algorithm.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q048",
      "domain": "domain4",
      "questionText": "Which DCGM command discovers all GPUs in the system?",
      "type": "multiple-choice",
      "choices": [
        "dcgmi discover",
        "dcgmi discovery -l",
        "dcgmi list --gpus",
        "dcgmi gpu-list"
      ],
      "correctAnswer": 1,
      "explanation": "'dcgmi discovery -l' lists all discovered GPUs with IDs, UUIDs, and basic information. Use '-c' flag to include compute instances (MIG).",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q049",
      "domain": "domain4",
      "questionText": "What is the purpose of running HPL (High-Performance Linpack) on a cluster?",
      "type": "multiple-choice",
      "choices": [
        "Testing network latency",
        "Benchmarking peak computational performance (FLOPs)",
        "Validating storage performance",
        "Testing container deployment"
      ],
      "correctAnswer": 1,
      "explanation": "HPL measures peak floating-point performance in FLOPs (TeraFLOPS or PetaFLOPS). Used for system validation and TOP500 rankings.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q050",
      "domain": "domain4",
      "questionText": "Which environment variable forces NCCL to use a specific network interface?",
      "type": "multiple-choice",
      "choices": [
        "NCCL_NET_IF",
        "NCCL_SOCKET_IFNAME",
        "NCCL_IB_HCA",
        "NCCL_INTERFACE"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL_SOCKET_IFNAME specifies which network interface to use (e.g., 'eth0' or 'ib0'). Use NCCL_IB_HCA to select specific InfiniBand adapters.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q051",
      "domain": "domain5",
      "questionText": "What is the first step when troubleshooting an NVLink degradation?",
      "type": "multiple-choice",
      "choices": [
        "Reboot the system immediately",
        "Check 'nvidia-smi nvlink --status' for link errors and replay counts",
        "Replace the GPU",
        "Update the NVIDIA driver"
      ],
      "correctAnswer": 1,
      "explanation": "Check NVLink status and error counters first. High replay counts indicate signal integrity issues. Also verify topology with 'nvidia-smi topo -m'.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q052",
      "domain": "domain5",
      "questionText": "What does Xid 48 typically indicate?",
      "type": "multiple-choice",
      "choices": [
        "Double-bit ECC error",
        "GPU overheating",
        "PCIe bus error",
        "Driver timeout"
      ],
      "correctAnswer": 0,
      "explanation": "Xid 48 indicates a double-bit ECC error (uncorrectable memory error). This is a serious hardware issue that may require GPU replacement.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q053",
      "domain": "domain5",
      "questionText": "Which nvidia-smi query shows ECC error counts?",
      "type": "multiple-choice",
      "choices": [
        "--query-gpu=ecc.errors.corrected.aggregate.total",
        "--query-gpu=ecc.mode.current,ecc.errors.corrected.volatile.total",
        "--query-gpu=memory.ecc",
        "--show-ecc-errors"
      ],
      "correctAnswer": 1,
      "explanation": "Query 'ecc.mode.current' to verify ECC is enabled, and 'ecc.errors.corrected.volatile.total' and 'ecc.errors.uncorrected.volatile.total' for error counts since driver load.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q054",
      "domain": "domain4",
      "questionText": "What is the typical efficiency target for HPL benchmark on a DGX H100 system?",
      "type": "multiple-choice",
      "choices": ["50-60%", "70-80%", "85-92%", "95-99%"],
      "correctAnswer": 2,
      "explanation": "Well-tuned DGX H100 systems typically achieve 85-92% HPL efficiency. Values below 80% indicate potential issues with memory configuration, thermal throttling, or NVLink problems.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q055",
      "domain": "domain4",
      "questionText": "Which NCCL test is most appropriate for validating all-to-all communication patterns used in expert parallelism?",
      "type": "multiple-choice",
      "choices": [
        "all_reduce_perf",
        "all_gather_perf",
        "alltoall_perf",
        "broadcast_perf"
      ],
      "correctAnswer": 2,
      "explanation": "alltoall_perf tests all-to-all communication where each GPU sends unique data to every other GPU. This pattern is used in Mixture of Experts (MoE) models with expert parallelism.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q056",
      "domain": "domain4",
      "questionText": "What does the DCGM health check 'watches' feature do?",
      "type": "multiple-choice",
      "choices": [
        "Monitors GPU utilization in real-time",
        "Continuously monitors specified health metrics and triggers alerts on violations",
        "Records GPU temperature history",
        "Watches for driver updates"
      ],
      "correctAnswer": 1,
      "explanation": "DCGM watches continuously monitor health metrics (temperature, power, ECC errors, etc.) and can trigger alerts or actions when thresholds are exceeded.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q057",
      "domain": "domain4",
      "questionText": "When running multi-node NCCL tests, which environment variable specifies the network interface for inter-node communication?",
      "type": "multiple-choice",
      "choices": [
        "NCCL_NET_DEVICE",
        "NCCL_IB_HCA",
        "NCCL_COMM_ID",
        "NCCL_NETWORK"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL_IB_HCA specifies which InfiniBand HCA (Host Channel Adapter) to use for inter-node communication. Format: 'mlx5_0:1' for specific port.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q058",
      "domain": "domain4",
      "questionText": "What is the purpose of the 'dcgmi stats' command?",
      "type": "multiple-choice",
      "choices": [
        "Display current GPU statistics",
        "Enable job statistics collection and reporting",
        "Show historical performance data",
        "Calculate statistical averages"
      ],
      "correctAnswer": 1,
      "explanation": "'dcgmi stats' enables job-level statistics collection. Use 'dcgmi stats -e' to enable, then run workloads, and 'dcgmi stats -v' to view collected statistics.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q059",
      "domain": "domain4",
      "questionText": "Which command validates InfiniBand fabric connectivity and performance?",
      "type": "multiple-choice",
      "choices": ["ibstat", "ibdiagnet", "ibportstate", "iblinkinfo"],
      "correctAnswer": 1,
      "explanation": "'ibdiagnet' performs comprehensive fabric diagnostics including topology discovery, link health, routing verification, and performance counters analysis.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q060",
      "domain": "domain4",
      "questionText": "What does a 'Symbol Error' in InfiniBand port counters indicate?",
      "type": "multiple-choice",
      "choices": [
        "Software configuration problem",
        "Physical layer signal integrity issue (cable or connector)",
        "Routing table error",
        "Memory allocation failure"
      ],
      "correctAnswer": 1,
      "explanation": "Symbol errors indicate physical layer problems - typically damaged cables, dirty connectors, or excessive cable length. High symbol error rates require cable inspection/replacement.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q061",
      "domain": "domain4",
      "questionText": "Which metric best indicates GPU memory bandwidth utilization?",
      "type": "multiple-choice",
      "choices": [
        "GPU-Util in nvidia-smi",
        "Memory-Util in nvidia-smi or dram_active in DCGM",
        "Used memory percentage",
        "Power consumption"
      ],
      "correctAnswer": 1,
      "explanation": "Memory-Util (nvidia-smi) or dram_active (DCGM) shows what percentage of time memory was being read/written. High memory utilization with low GPU utilization indicates memory-bound workload.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q062",
      "domain": "domain4",
      "questionText": "What is the expected single-GPU memory bandwidth for an H100 SXM GPU?",
      "type": "multiple-choice",
      "choices": ["1.5 TB/s", "2.0 TB/s", "3.35 TB/s", "4.0 TB/s"],
      "correctAnswer": 2,
      "explanation": "H100 SXM with HBM3 achieves approximately 3.35 TB/s memory bandwidth. A100 achieves ~2.0 TB/s. Use bandwidth tests to verify actual vs theoretical performance.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q063",
      "domain": "domain4",
      "questionText": "Which DCGM diagnostic test validates GPU compute capabilities?",
      "type": "multiple-choice",
      "choices": [
        "Memory test",
        "Diagnostic test",
        "SM Stress test",
        "PCIe test"
      ],
      "correctAnswer": 2,
      "explanation": "SM Stress test validates streaming multiprocessor functionality by running compute workloads. It's part of Level 2 and Level 3 diagnostics.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q064",
      "domain": "domain4",
      "questionText": "What does 'perfquery' output show for InfiniBand ports?",
      "type": "multiple-choice",
      "choices": [
        "Port configuration settings",
        "Performance counters including data transmitted/received and error counts",
        "Routing table information",
        "Connected device names"
      ],
      "correctAnswer": 1,
      "explanation": "'perfquery' displays performance counters: PortXmitData, PortRcvData, PortXmitPkts, PortRcvPkts, and various error counters. Use to verify traffic and detect issues.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q065",
      "domain": "domain4",
      "questionText": "For cluster acceptance testing, what is the recommended order of validation?",
      "type": "multiple-choice",
      "choices": [
        "HPL → NCCL → DCGM → Network",
        "Network → DCGM → NCCL → HPL",
        "DCGM → HPL → Network → NCCL",
        "NCCL → Network → HPL → DCGM"
      ],
      "correctAnswer": 1,
      "explanation": "Start with basic network connectivity, then GPU health (DCGM), then collective communication (NCCL), finally full system benchmark (HPL). This finds issues early before complex tests.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q066",
      "domain": "domain4",
      "questionText": "What does 'nvidia-smi nvlink --status' show?",
      "type": "multiple-choice",
      "choices": [
        "NVLink firmware version",
        "NVLink connection status, bandwidth capability, and error counters",
        "NVLink configuration options",
        "NVLink driver version"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi nvlink --status' shows each NVLink connection's status (Active/Inactive), bandwidth capability, and error counters including CRC errors and replay counts.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q067",
      "domain": "domain4",
      "questionText": "Which storage validation test measures random I/O performance important for AI training checkpoints?",
      "type": "multiple-choice",
      "choices": [
        "Sequential read/write test",
        "Random 4K read/write IOPS test",
        "Large file copy test",
        "Metadata operation test"
      ],
      "correctAnswer": 1,
      "explanation": "Random 4K IOPS is critical for checkpoint operations which involve many small random writes. Use tools like fio with random read/write patterns to measure.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q068",
      "domain": "domain4",
      "questionText": "What does a high 'Replay Count' in NVLink status indicate?",
      "type": "multiple-choice",
      "choices": [
        "Normal operation - data is being transmitted",
        "Signal integrity issues requiring packet retransmission",
        "High bandwidth utilization",
        "Memory errors on the GPU"
      ],
      "correctAnswer": 1,
      "explanation": "High replay counts indicate NVLink signal integrity issues causing packet retransmissions. Check cable seating, NVSwitch health, or GPU thermal conditions.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q069",
      "domain": "domain4",
      "questionText": "Which command creates a DCGM GPU group for monitoring?",
      "type": "multiple-choice",
      "choices": [
        "dcgmi group -c mygroup -a 0,1,2,3",
        "dcgmi create group mygroup --gpus 0,1,2,3",
        "dcgmi monitor --group mygroup",
        "dcgmi config group add"
      ],
      "correctAnswer": 0,
      "explanation": "'dcgmi group -c mygroup -a 0,1,2,3' creates a group named 'mygroup' and adds GPUs 0-3. Groups enable collective monitoring and diagnostics.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q070",
      "domain": "domain4",
      "questionText": "What is the purpose of running 'ibportstate' command?",
      "type": "multiple-choice",
      "choices": [
        "View detailed port performance statistics",
        "Query or modify the state of an InfiniBand port",
        "Display port error counters",
        "Show port routing information"
      ],
      "correctAnswer": 1,
      "explanation": "'ibportstate' queries the current state of an IB port (Active, Init, Down) and can be used to enable/disable ports or reset port counters.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q071",
      "domain": "domain4",
      "questionText": "Which NCCL environment variable can help debug communication hangs?",
      "type": "multiple-choice",
      "choices": [
        "NCCL_DEBUG=INFO",
        "NCCL_DEBUG_SUBSYS=ALL",
        "NCCL_ASYNC_ERROR_HANDLING=1",
        "All of the above"
      ],
      "correctAnswer": 3,
      "explanation": "NCCL_DEBUG=INFO shows initialization details, NCCL_DEBUG_SUBSYS=ALL enables all subsystem logging, and NCCL_ASYNC_ERROR_HANDLING=1 helps detect and report communication errors.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q072",
      "domain": "domain4",
      "questionText": "What is the minimum DCGM diagnostic level that includes memory stress testing?",
      "type": "multiple-choice",
      "choices": ["Level 1", "Level 2", "Level 3", "Level 4"],
      "correctAnswer": 1,
      "explanation": "Level 2 includes short stress tests including memory testing. Level 1 only checks software deployment and basic properties. Level 3 includes extended stress tests.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q073",
      "domain": "domain4",
      "questionText": "When validating GPU-Direct RDMA, what should you verify?",
      "type": "multiple-choice",
      "choices": [
        "CUDA driver version",
        "nvidia-peermem module is loaded and GPUs are on correct NUMA nodes",
        "InfiniBand switch firmware",
        "System memory capacity"
      ],
      "correctAnswer": 1,
      "explanation": "GPU-Direct RDMA requires nvidia-peermem kernel module loaded and GPUs should be on the same NUMA node as InfiniBand HCAs for optimal performance.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q074",
      "domain": "domain5",
      "questionText": "What does Xid 13 typically indicate and how should it be addressed?",
      "type": "multiple-choice",
      "choices": [
        "Memory error - replace GPU immediately",
        "Graphics engine exception - usually application bug, check workload",
        "Power supply failure - check PSU",
        "Driver crash - reinstall driver"
      ],
      "correctAnswer": 1,
      "explanation": "Xid 13 is a graphics/compute engine exception, typically caused by application bugs (illegal memory access, kernel timeout). First check the application, then look for patterns across GPUs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q075",
      "domain": "domain5",
      "questionText": "A GPU shows 'ERR!' in nvidia-smi output. What is the first troubleshooting step?",
      "type": "multiple-choice",
      "choices": [
        "Replace the GPU immediately",
        "Check dmesg for XID errors and run 'nvidia-smi -r' to reset the GPU",
        "Reinstall the NVIDIA driver",
        "Power cycle the entire system"
      ],
      "correctAnswer": 1,
      "explanation": "First check dmesg for specific XID error codes to understand the failure. Then attempt GPU reset with 'nvidia-smi -r'. If reset fails, system reboot may be required.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q076",
      "domain": "domain5",
      "questionText": "What does Xid 31 indicate?",
      "type": "multiple-choice",
      "choices": [
        "NVLink failure",
        "GPU memory page fault - invalid memory access",
        "PCIe bandwidth degradation",
        "Thermal throttling"
      ],
      "correctAnswer": 1,
      "explanation": "Xid 31 indicates a GPU memory page fault - the application accessed invalid GPU memory. Usually caused by programming errors like out-of-bounds access or use-after-free.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q077",
      "domain": "domain5",
      "questionText": "How can you check if a GPU is thermally throttling?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q -d TEMPERATURE",
        "nvidia-smi --query-gpu=clocks_throttle_reasons.gpu_idle --format=csv",
        "nvidia-smi --query-gpu=clocks_throttle_reasons.sw_thermal_slowdown,clocks_throttle_reasons.hw_thermal_slowdown --format=csv",
        "dcgmi health -g 0"
      ],
      "correctAnswer": 2,
      "explanation": "Query clocks_throttle_reasons.sw_thermal_slowdown and hw_thermal_slowdown to check thermal throttling. 'Active' means throttling is occurring due to temperature.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q078",
      "domain": "domain5",
      "questionText": "What is the correct approach to diagnose intermittent GPU hangs?",
      "type": "multiple-choice",
      "choices": [
        "Replace the GPU immediately",
        "Enable persistence mode, monitor with DCGM watches, check thermal and power conditions",
        "Disable ECC memory",
        "Reduce GPU power limit to minimum"
      ],
      "correctAnswer": 1,
      "explanation": "Enable persistence mode to keep driver loaded, set up DCGM watches to capture events, monitor temperature/power during incidents. Look for patterns in timing or workload.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q079",
      "domain": "domain5",
      "questionText": "What does Xid 63 indicate and when does it occur?",
      "type": "multiple-choice",
      "choices": [
        "Driver timeout",
        "ECC page retirement - memory page permanently retired due to errors",
        "NVLink training failure",
        "PCIe replay error"
      ],
      "correctAnswer": 1,
      "explanation": "Xid 63 indicates a memory page has been permanently retired due to excessive ECC errors. Monitor retired page counts - excessive retirements may indicate failing memory.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q080",
      "domain": "domain5",
      "questionText": "How do you check the number of retired memory pages on a GPU?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q -d MEMORY",
        "nvidia-smi --query-gpu=retired_pages.pending,retired_pages.sbe,retired_pages.dbe --format=csv",
        "dcgmi diag -r 1",
        "cat /proc/driver/nvidia/gpus/*/retired_pages"
      ],
      "correctAnswer": 1,
      "explanation": "Query retired_pages.pending (awaiting retirement), retired_pages.sbe (single-bit), and retired_pages.dbe (double-bit) to see retired page counts. High counts indicate memory degradation.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q081",
      "domain": "domain5",
      "questionText": "A container reports 'CUDA_ERROR_NO_DEVICE'. What should you check first?",
      "type": "multiple-choice",
      "choices": [
        "CUDA driver version",
        "Container runtime GPU configuration (--gpus flag or NVIDIA_VISIBLE_DEVICES)",
        "GPU firmware version",
        "Container image version"
      ],
      "correctAnswer": 1,
      "explanation": "Check that the container was started with GPU access (--gpus all or NVIDIA_VISIBLE_DEVICES). Also verify nvidia-container-toolkit is installed and configured.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q082",
      "domain": "domain5",
      "questionText": "What does Xid 119 indicate?",
      "type": "multiple-choice",
      "choices": [
        "Power supply issue",
        "GSP (GPU System Processor) firmware error",
        "Memory controller failure",
        "NVLink CRC error"
      ],
      "correctAnswer": 1,
      "explanation": "Xid 119 indicates a GSP firmware error. This can occur due to driver/firmware mismatch. Ensure driver and GPU firmware are compatible versions.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q083",
      "domain": "domain5",
      "questionText": "InfiniBand shows high 'LinkDowned' counter. What does this indicate?",
      "type": "multiple-choice",
      "choices": [
        "Normal cable insertion events",
        "Link has been bouncing - physical layer instability",
        "High bandwidth utilization",
        "Routing table updates"
      ],
      "correctAnswer": 1,
      "explanation": "LinkDowned counts how many times the link went down. High counts indicate link instability - check cable, connector, switch port, or transceiver.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q084",
      "domain": "domain5",
      "questionText": "How do you reset InfiniBand port error counters?",
      "type": "multiple-choice",
      "choices": [
        "ibportstate -R",
        "perfquery -x -R",
        "ibclearcounters",
        "Both A and B"
      ],
      "correctAnswer": 3,
      "explanation": "Use 'perfquery -x -R' to reset extended counters or 'ibportstate -R' to reset port. Clearing counters before tests helps isolate new errors.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q085",
      "domain": "domain5",
      "questionText": "A job fails with 'NCCL WARN ... Connection refused'. What should you check?",
      "type": "multiple-choice",
      "choices": [
        "GPU health",
        "Network connectivity and firewall rules between nodes",
        "CUDA version",
        "Memory capacity"
      ],
      "correctAnswer": 1,
      "explanation": "Connection refused indicates network connectivity issues. Check that nodes can reach each other on the NCCL ports, verify firewall rules, and ensure correct network interface is specified.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q086",
      "domain": "domain5",
      "questionText": "What causes 'NCCL WARN Cuda failure ... out of memory'?",
      "type": "multiple-choice",
      "choices": [
        "Insufficient system RAM",
        "GPU memory exhausted - reduce batch size or model size",
        "Swap space full",
        "Too many NCCL communicators"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL out of memory means GPU memory is exhausted. Reduce batch size, use gradient checkpointing, enable memory-efficient attention, or distribute across more GPUs.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q087",
      "domain": "domain5",
      "questionText": "What does 'nvidia-smi -pm 1' do and why is it important for troubleshooting?",
      "type": "multiple-choice",
      "choices": [
        "Sets power mode to maximum",
        "Enables persistence mode - keeps driver loaded for consistent behavior",
        "Enables performance monitoring",
        "Sets GPU to maintenance mode"
      ],
      "correctAnswer": 1,
      "explanation": "Persistence mode keeps the NVIDIA driver loaded even when no applications use the GPU. This provides consistent initialization behavior and prevents driver unload issues.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q088",
      "domain": "domain5",
      "questionText": "How do you identify which process is using GPU memory?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi",
        "nvidia-smi pmon",
        "Both A and B show process information",
        "dcgmi discovery"
      ],
      "correctAnswer": 2,
      "explanation": "nvidia-smi shows processes using each GPU with PID and memory usage. 'nvidia-smi pmon' provides continuous process monitoring with utilization metrics.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q089",
      "domain": "domain5",
      "questionText": "What is the recommended approach when DCGM Level 3 diagnostics fail?",
      "type": "multiple-choice",
      "choices": [
        "Immediately replace the GPU",
        "Run Level 1 to isolate basic vs stress-test failures, check environmental conditions",
        "Disable the GPU in BIOS",
        "Reinstall the operating system"
      ],
      "correctAnswer": 1,
      "explanation": "First run Level 1 to see if basic checks pass. If Level 1 passes but Level 3 fails, check thermal conditions and power delivery. Extended stress may expose issues not seen in normal operation.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q090",
      "domain": "domain5",
      "questionText": "PCIe shows 'LnkSta: Speed 8GT/s, Width x8' for a GPU that should be x16. What is the likely cause?",
      "type": "multiple-choice",
      "choices": [
        "Normal operation - automatic power saving",
        "Physical seating issue or damaged PCIe slot/riser",
        "Driver not installed correctly",
        "Incorrect BIOS setting"
      ],
      "correctAnswer": 1,
      "explanation": "Width x8 instead of x16 usually indicates physical issues - GPU not fully seated, damaged PCIe slot, bent pins, or faulty riser card. Reseat GPU and inspect slot/riser.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q091",
      "domain": "domain5",
      "questionText": "What does Xid 94 indicate?",
      "type": "multiple-choice",
      "choices": [
        "Memory access violation",
        "Contained ECC error - correctable error that was isolated",
        "Driver crash",
        "Thermal emergency"
      ],
      "correctAnswer": 1,
      "explanation": "Xid 94 indicates a contained ECC error - the error was correctable and isolated to prevent data corruption. Monitor for increasing frequency which may indicate degrading memory.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q092",
      "domain": "domain5",
      "questionText": "A Slurm job shows 'NodeFail' but the node appears healthy. What should you investigate?",
      "type": "multiple-choice",
      "choices": [
        "Slurm configuration",
        "Slurm health check script output and node state reason",
        "Network connectivity",
        "Disk space"
      ],
      "correctAnswer": 1,
      "explanation": "Check 'scontrol show node <nodename>' for Reason field. NodeFail can be triggered by health check scripts. Check /var/log/slurm/ for health check output.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q093",
      "domain": "domain5",
      "questionText": "How do you force a GPU reset without rebooting the system?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi --gpu-reset -i <gpu_id>",
        "nvidia-smi -r -i <gpu_id>",
        "Both A and B work",
        "GPU reset requires system reboot"
      ],
      "correctAnswer": 2,
      "explanation": "Both 'nvidia-smi --gpu-reset -i <id>' and 'nvidia-smi -r -i <id>' reset the specified GPU. Note: Reset requires no active processes on the GPU and may not recover all error states.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q094",
      "domain": "domain1",
      "questionText": "What does 'dmidecode -t memory' display?",
      "type": "multiple-choice",
      "choices": [
        "GPU memory information",
        "System RAM module details including speed, manufacturer, and slot locations",
        "Memory utilization statistics",
        "Swap space configuration"
      ],
      "correctAnswer": 1,
      "explanation": "'dmidecode -t memory' shows physical memory module information from SMBIOS: manufacturer, part number, speed, size, and slot population.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q095",
      "domain": "domain1",
      "questionText": "Which service must be running for NVSwitch-based GPU communication on DGX A100/H100?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-persistenced",
        "nvidia-fabricmanager",
        "dcgm",
        "nv-hostengine"
      ],
      "correctAnswer": 1,
      "explanation": "nvidia-fabricmanager manages NVSwitch topology and GPU-to-GPU communication. Without it, NVLink connections through NVSwitch won't function on multi-GPU systems.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q096",
      "domain": "domain1",
      "questionText": "How do you check if the NVIDIA driver kernel module is loaded?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi",
        "lsmod | grep nvidia",
        "modinfo nvidia",
        "All of the above"
      ],
      "correctAnswer": 3,
      "explanation": "nvidia-smi fails if driver not loaded, lsmod shows loaded modules, modinfo shows module details. All can verify driver presence.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q097",
      "domain": "domain1",
      "questionText": "What is the correct order for DGX system bring-up validation?",
      "type": "multiple-choice",
      "choices": [
        "Install drivers → Configure network → Run benchmarks → Check BIOS",
        "Check BIOS/POST → Verify BMC → Install/verify drivers → Test GPUs",
        "Run benchmarks → Check temperatures → Configure Slurm → Install drivers",
        "Configure storage → Install drivers → Run HPL → Check BMC"
      ],
      "correctAnswer": 1,
      "explanation": "Start with BIOS/POST verification, then BMC connectivity, then driver installation/verification, finally GPU testing. This catches fundamental issues before complex configuration.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q098",
      "domain": "domain1",
      "questionText": "Which BIOS setting should be verified for optimal GPU performance on DGX systems?",
      "type": "multiple-choice",
      "choices": [
        "Boot order",
        "Above 4G Decoding enabled and SR-IOV support",
        "USB configuration",
        "Audio settings"
      ],
      "correctAnswer": 1,
      "explanation": "Above 4G Decoding must be enabled for GPUs with large BAR (Base Address Register) regions. SR-IOV may be needed for virtualization. These affect GPU memory mapping.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q099",
      "domain": "domain1",
      "questionText": "What does 'ipmitool sel elist' show that 'ipmitool sel list' doesn't?",
      "type": "multiple-choice",
      "choices": [
        "Error counts",
        "Extended event descriptions with sensor details",
        "Event timestamps only",
        "Power consumption data"
      ],
      "correctAnswer": 1,
      "explanation": "'sel elist' shows extended output including sensor names and human-readable event descriptions. 'sel list' shows compact format with hex codes.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q100",
      "domain": "domain1",
      "questionText": "How do you verify GPU PCIe generation and link width?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q -d PCIE",
        "lspci -vvv -d 10de:",
        "Both A and B provide this information",
        "dmidecode -t slot"
      ],
      "correctAnswer": 2,
      "explanation": "nvidia-smi -q -d PCIE shows current and maximum PCIe gen/width. lspci -vvv shows detailed PCIe capability and current link status. Both are useful.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q101",
      "domain": "domain1",
      "questionText": "What file should you check to verify the NVIDIA driver version matches the installed package?",
      "type": "multiple-choice",
      "choices": [
        "/etc/nvidia/version",
        "/proc/driver/nvidia/version",
        "/var/log/nvidia.log",
        "/opt/nvidia/version.txt"
      ],
      "correctAnswer": 1,
      "explanation": "/proc/driver/nvidia/version shows the currently loaded driver version. Compare with 'dpkg -l | grep nvidia' or 'rpm -qa | grep nvidia' to verify package version.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q102",
      "domain": "domain1",
      "questionText": "Which BMC command configures network settings for out-of-band management?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool lan set 1 ipaddr <ip>",
        "ipmitool network config",
        "ipmitool bmc network",
        "ipmitool chassis network"
      ],
      "correctAnswer": 0,
      "explanation": "'ipmitool lan set 1 ipaddr <ip>' sets the BMC IP address. Use 'lan set 1 netmask' and 'lan set 1 defgw ipaddr' for complete network configuration.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q103",
      "domain": "domain1",
      "questionText": "What does 'nvidia-smi -L' display?",
      "type": "multiple-choice",
      "choices": [
        "GPU log messages",
        "List of GPUs with UUID and product name",
        "License information",
        "Link status for NVLink"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -L' lists all GPUs with their index, name (e.g., 'NVIDIA A100-SXM4-80GB'), and UUID. Useful for inventory and scripting.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q104",
      "domain": "domain1",
      "questionText": "How do you check the current power limit and maximum allowed power limit for a GPU?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q -d POWER",
        "nvidia-smi --query-gpu=power.limit,power.max_limit --format=csv",
        "Both A and B",
        "ipmitool sensor list | grep -i power"
      ],
      "correctAnswer": 2,
      "explanation": "nvidia-smi -q -d POWER shows detailed power info. --query-gpu with power.limit and power.max_limit gives current and maximum limits in scriptable format.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q105",
      "domain": "domain1",
      "questionText": "Which command displays the GPU firmware version?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q | grep -i firmware",
        "nvidia-smi --query-gpu=vbios_version --format=csv",
        "Both A and B",
        "dcgmi discovery -l"
      ],
      "correctAnswer": 2,
      "explanation": "'nvidia-smi -q' includes firmware info in VBIOS Version field. --query-gpu=vbios_version gives direct output. VBIOS contains GPU firmware.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q106",
      "domain": "domain1",
      "questionText": "What is the purpose of 'nvidia-persistenced'?",
      "type": "multiple-choice",
      "choices": [
        "Saves GPU state to disk",
        "Keeps NVIDIA driver loaded to reduce application startup latency",
        "Persists power settings across reboots",
        "Manages persistent storage"
      ],
      "correctAnswer": 1,
      "explanation": "nvidia-persistenced keeps the NVIDIA driver loaded even when no GPU applications are running. This eliminates driver initialization latency for new jobs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q107",
      "domain": "domain1",
      "questionText": "Which command shows the total and available system memory on Linux?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi",
        "free -h",
        "cat /proc/cpuinfo",
        "dmidecode -t processor"
      ],
      "correctAnswer": 1,
      "explanation": "'free -h' shows total, used, free, shared, cache, and available system memory in human-readable format. Use to verify expected RAM is detected.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q108",
      "domain": "domain1",
      "questionText": "What does the Mellanox vendor ID '15b3' indicate in lspci output?",
      "type": "multiple-choice",
      "choices": [
        "NVIDIA GPU",
        "Intel network adapter",
        "NVIDIA/Mellanox InfiniBand or Ethernet adapter",
        "AMD processor"
      ],
      "correctAnswer": 2,
      "explanation": "15b3 is the Mellanox (now NVIDIA Networking) PCI vendor ID. 'lspci -d 15b3:' lists all Mellanox/NVIDIA network adapters including InfiniBand HCAs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q109",
      "domain": "domain3",
      "questionText": "Which Slurm configuration parameter specifies the GPU types available on a node?",
      "type": "multiple-choice",
      "choices": [
        "GpuType in slurm.conf",
        "Gres= in slurm.conf with gres.conf defining types",
        "GpuConfig in gpu.conf",
        "Resources= in partition.conf"
      ],
      "correctAnswer": 1,
      "explanation": "Gres= in slurm.conf specifies available GRES (e.g., 'Gres=gpu:a100:8'). gres.conf defines the actual device mapping for each GRES type.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q110",
      "domain": "domain3",
      "questionText": "How do you run a container with access to specific GPUs (e.g., GPU 0 and 1)?",
      "type": "multiple-choice",
      "choices": [
        "docker run --gpus 0,1",
        "docker run --gpus '\"device=0,1\"'",
        "docker run -e CUDA_VISIBLE_DEVICES=0,1",
        "docker run --gpu-id=0,1"
      ],
      "correctAnswer": 1,
      "explanation": "'docker run --gpus '\"device=0,1\"'' exposes only GPUs 0 and 1 to the container. The device flag requires proper quoting in shell.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q111",
      "domain": "domain3",
      "questionText": "What is the primary advantage of Enroot over Docker for HPC workloads?",
      "type": "multiple-choice",
      "choices": [
        "Better GPU support",
        "Rootless/unprivileged execution without daemon",
        "More container images available",
        "Faster image pull speeds"
      ],
      "correctAnswer": 1,
      "explanation": "Enroot runs unprivileged without requiring a daemon (unlike Docker). This is important for HPC where users shouldn't have root privileges.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q112",
      "domain": "domain3",
      "questionText": "Which pyxis option enables GPU access in Slurm containerized jobs?",
      "type": "multiple-choice",
      "choices": [
        "--container-mounts=/dev/nvidia*",
        "GPU access is automatic with pyxis",
        "--container-gpus=all",
        "--enable-gpu"
      ],
      "correctAnswer": 1,
      "explanation": "Pyxis automatically detects and exposes GPUs allocated by Slurm GRES to the container. No additional flags needed for basic GPU access.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q113",
      "domain": "domain3",
      "questionText": "What command shows current Slurm job queue with GPU allocation details?",
      "type": "multiple-choice",
      "choices": [
        "squeue",
        "squeue -o '%.18i %.9P %.8j %.8u %.2t %.10M %.6D %R %b'",
        "scontrol show jobs",
        "Both B and C show GRES details"
      ],
      "correctAnswer": 3,
      "explanation": "squeue with custom format including %b shows GRES (GPU) allocation. 'scontrol show jobs' provides complete job details including GRES.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q114",
      "domain": "domain3",
      "questionText": "How do you verify NGC container toolkit is properly configured?",
      "type": "multiple-choice",
      "choices": [
        "ngc config verify",
        "docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi",
        "nvidia-container-cli info",
        "Both B and C"
      ],
      "correctAnswer": 3,
      "explanation": "Running an nvidia/cuda container with --gpus verifies the full stack. nvidia-container-cli info shows toolkit configuration and GPU detection.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q115",
      "domain": "domain3",
      "questionText": "Which file configures the NVIDIA container runtime?",
      "type": "multiple-choice",
      "choices": [
        "/etc/docker/daemon.json",
        "/etc/nvidia-container-runtime/config.toml",
        "/etc/containerd/config.toml",
        "All of the above may need configuration"
      ],
      "correctAnswer": 3,
      "explanation": "daemon.json configures Docker, config.toml configures nvidia-container-runtime defaults, containerd config may need nvidia runtime registration. All relevant.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q116",
      "domain": "domain3",
      "questionText": "What does 'scontrol show node <nodename>' display?",
      "type": "multiple-choice",
      "choices": [
        "Node hardware specifications only",
        "Complete node state including GRES, partitions, state, reason, and features",
        "Running jobs on the node",
        "Node network configuration"
      ],
      "correctAnswer": 1,
      "explanation": "'scontrol show node' displays comprehensive node information: state, reason (if drained), GRES configuration, partitions, features, and resource limits.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q117",
      "domain": "domain3",
      "questionText": "How do you submit a Slurm batch job requesting 4 GPUs?",
      "type": "multiple-choice",
      "choices": [
        "sbatch --gpu=4 script.sh",
        "sbatch --gres=gpu:4 script.sh",
        "sbatch -G 4 script.sh",
        "Both B and C"
      ],
      "correctAnswer": 3,
      "explanation": "'--gres=gpu:4' is the standard format. '-G 4' or '--gpus=4' is shorthand. Both request 4 GPUs for the job.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q118",
      "domain": "domain3",
      "questionText": "What is the purpose of 'SelectType=select/cons_tres' in Slurm?",
      "type": "multiple-choice",
      "choices": [
        "Enables fair share scheduling",
        "Enables consumable trackable resources (GRES) including GPUs",
        "Enables container support",
        "Enables topology-aware scheduling"
      ],
      "correctAnswer": 1,
      "explanation": "select/cons_tres enables tracking and scheduling of consumable resources including GPUs (GRES). Required for proper GPU allocation in Slurm.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q119",
      "domain": "domain2",
      "questionText": "What happens when you enable MIG mode on a GPU?",
      "type": "multiple-choice",
      "choices": [
        "MIG instances are automatically created",
        "The GPU is partitioned but requires reboot, then manual instance creation",
        "Performance is automatically doubled",
        "NVLink is disabled"
      ],
      "correctAnswer": 1,
      "explanation": "Enabling MIG mode requires a reboot to take effect. After reboot, you must manually create GPU instances (GI) and compute instances (CI) using nvidia-smi mig commands.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q120",
      "domain": "domain2",
      "questionText": "Which MIG profile provides the largest memory allocation on an A100 80GB?",
      "type": "multiple-choice",
      "choices": ["1g.10gb", "3g.40gb", "4g.40gb", "7g.80gb"],
      "correctAnswer": 3,
      "explanation": "7g.80gb uses all 7 GPU slices and provides access to all 80GB memory. This is essentially the full GPU as a single MIG instance.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q121",
      "domain": "domain2",
      "questionText": "What does the 'g' number represent in MIG profile names like '3g.40gb'?",
      "type": "multiple-choice",
      "choices": [
        "GPU index",
        "Number of GPU compute slices (streaming multiprocessors)",
        "Gigabytes of memory",
        "Generation of MIG"
      ],
      "correctAnswer": 1,
      "explanation": "The number before 'g' indicates GPU compute slices. A100 has 7 slices total. '3g' means 3 of 7 compute slices. The memory amount (e.g., 40gb) follows.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q122",
      "domain": "domain2",
      "questionText": "How do you list available MIG profiles for a GPU?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi mig -lgip",
        "nvidia-smi --mig-profiles",
        "dcgmi mig list",
        "nvidia-mig --list"
      ],
      "correctAnswer": 0,
      "explanation": "'nvidia-smi mig -lgip' (List GPU Instance Profiles) shows available MIG profiles with their IDs, memory sizes, and slice configurations.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q123",
      "domain": "domain2",
      "questionText": "What is the relationship between GPU Instances (GI) and Compute Instances (CI) in MIG?",
      "type": "multiple-choice",
      "choices": [
        "They are the same thing",
        "GI provides memory isolation, CI provides compute isolation within a GI",
        "CI provides memory, GI provides compute",
        "GI is for graphics, CI is for compute"
      ],
      "correctAnswer": 1,
      "explanation": "GPU Instance (GI) partitions memory and creates a physical GPU slice. Compute Instance (CI) must be created within a GI to provide compute engine isolation.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q124",
      "domain": "domain4",
      "questionText": "What is the theoretical peak FP16 Tensor Core performance of an H100 SXM GPU?",
      "type": "multiple-choice",
      "choices": ["312 TFLOPS", "989 TFLOPS", "1,979 TFLOPS", "3,958 TFLOPS"],
      "correctAnswer": 2,
      "explanation": "H100 SXM achieves 1,979 TFLOPS for FP16 Tensor Core operations (with sparsity: 3,958 TFLOPS). This is the peak theoretical performance for AI training.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q125",
      "domain": "domain4",
      "questionText": "Which NCCL test is most appropriate for measuring point-to-point GPU bandwidth?",
      "type": "multiple-choice",
      "choices": [
        "all_reduce_perf",
        "sendrecv_perf",
        "broadcast_perf",
        "scatter_perf"
      ],
      "correctAnswer": 1,
      "explanation": "sendrecv_perf measures point-to-point send/receive bandwidth between GPU pairs. Useful for validating NVLink or InfiniBand bandwidth between specific GPUs.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q126",
      "domain": "domain4",
      "questionText": "What does 'nvidia-smi dmon' provide?",
      "type": "multiple-choice",
      "choices": [
        "Device monitoring - continuous output of GPU metrics",
        "Display monitoring - screen output",
        "Diagnostic monitoring - error logs",
        "Driver monitoring - version info"
      ],
      "correctAnswer": 0,
      "explanation": "'nvidia-smi dmon' provides continuous device monitoring output: power, temperature, utilization, memory usage, etc. Similar to 'top' for GPUs.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q127",
      "domain": "domain4",
      "questionText": "What is a healthy temperature range for datacenter GPUs under load?",
      "type": "multiple-choice",
      "choices": ["30-50°C", "50-70°C", "70-83°C", "85-95°C"],
      "correctAnswer": 2,
      "explanation": "Under load, datacenter GPUs typically run 70-83°C. Above 83°C may indicate cooling issues. Throttling begins around 85-90°C depending on model.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q128",
      "domain": "domain4",
      "questionText": "Which InfiniBand diagnostic shows the fabric topology?",
      "type": "multiple-choice",
      "choices": ["ibstat", "ibnetdiscover", "ibportstate", "perfquery"],
      "correctAnswer": 1,
      "explanation": "'ibnetdiscover' maps the InfiniBand fabric topology, showing all switches, HCAs, and their interconnections. Essential for verifying fabric configuration.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q129",
      "domain": "domain4",
      "questionText": "What does a 'Link Width Active: 4x' in ibstat indicate for an HDR InfiniBand port?",
      "type": "multiple-choice",
      "choices": [
        "Normal operation at full speed",
        "Link is operating at reduced width - potential cable or port issue",
        "Four ports are active",
        "Link speed is 4 Gbps"
      ],
      "correctAnswer": 1,
      "explanation": "HDR InfiniBand should show '4x' for full 200Gb/s or may use NDR at higher speeds. If expecting wider link (like NDR 400G needing 4x), 4x may be correct. Context matters.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q130",
      "domain": "domain4",
      "questionText": "What is the purpose of 'dcgmi profile' command?",
      "type": "multiple-choice",
      "choices": [
        "Configure MIG profiles",
        "Run profiling to measure GPU metrics during workload execution",
        "Set user profiles",
        "Display GPU profile information"
      ],
      "correctAnswer": 1,
      "explanation": "'dcgmi profile' enables metric profiling during job execution. It records detailed metrics (SM activity, memory bandwidth, etc.) for performance analysis.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q131",
      "domain": "domain4",
      "questionText": "Which tool validates NVLink peer-to-peer bandwidth between GPUs?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi nvlink --status",
        "bandwidthTest from CUDA samples or p2pBandwidthLatencyTest",
        "dcgmi diag",
        "nccl-tests"
      ],
      "correctAnswer": 1,
      "explanation": "CUDA samples' bandwidthTest and p2pBandwidthLatencyTest directly measure GPU-to-GPU bandwidth via NVLink. NCCL-tests also work but add collective overhead.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q132",
      "domain": "domain4",
      "questionText": "What does 'ECC Mode: Enabled' in nvidia-smi indicate?",
      "type": "multiple-choice",
      "choices": [
        "Extra Compute Capability is enabled",
        "Error Correcting Code memory is active, detecting and correcting memory errors",
        "Enhanced Cooling Control is active",
        "External Clock Control is enabled"
      ],
      "correctAnswer": 1,
      "explanation": "ECC (Error Correcting Code) detects and corrects single-bit memory errors and detects double-bit errors. Should be enabled for datacenter reliability.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q133",
      "domain": "domain4",
      "questionText": "What is the expected InfiniBand HDR bandwidth per port?",
      "type": "multiple-choice",
      "choices": ["100 Gb/s", "200 Gb/s", "400 Gb/s", "800 Gb/s"],
      "correctAnswer": 1,
      "explanation": "HDR (High Data Rate) InfiniBand provides 200 Gb/s per port (50 Gb/s per lane x 4 lanes). NDR provides 400 Gb/s, and NDR400/XDR provide 800 Gb/s.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q134",
      "domain": "domain4",
      "questionText": "Which DCGM field group provides memory-related metrics?",
      "type": "multiple-choice",
      "choices": [
        "DCGM_FI_PROF_GR_ENGINE_ACTIVE",
        "DCGM_FI_DEV_FB_FREE and DCGM_FI_DEV_FB_USED",
        "DCGM_FI_DEV_POWER_USAGE",
        "DCGM_FI_DEV_PCIE_TX_THROUGHPUT"
      ],
      "correctAnswer": 1,
      "explanation": "DCGM_FI_DEV_FB_FREE and DCGM_FI_DEV_FB_USED report framebuffer (GPU memory) free and used amounts. 'FB' stands for Frame Buffer.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q135",
      "domain": "domain4",
      "questionText": "How do you verify GPU-Direct Storage (GDS) is properly configured?",
      "type": "multiple-choice",
      "choices": [
        "Check nvidia-smi",
        "Run gdscheck tool or verify nvidia-fs module is loaded",
        "Check /proc/driver/nvidia/version",
        "Run dcgmi diag"
      ],
      "correctAnswer": 1,
      "explanation": "gdscheck validates GDS configuration. Also verify nvidia-fs kernel module is loaded with 'lsmod | grep nvidia_fs'. GDS enables direct storage-to-GPU data paths.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q136",
      "domain": "domain1",
      "questionText": "What does 'ipmitool sdr list' display?",
      "type": "multiple-choice",
      "choices": [
        "System data records",
        "Sensor data repository - list of available sensors and their readings",
        "Storage device records",
        "Software defined resources"
      ],
      "correctAnswer": 1,
      "explanation": "'ipmitool sdr list' shows the Sensor Data Repository - all sensors (temperature, voltage, fan, etc.) and their current readings in compact format.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q137",
      "domain": "domain1",
      "questionText": "Which command shows the system boot device order?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool chassis bootdev",
        "efibootmgr",
        "bootctl status",
        "Both B and C on UEFI systems"
      ],
      "correctAnswer": 3,
      "explanation": "On UEFI systems, 'efibootmgr' shows boot order and entries. 'bootctl status' (systemd-boot) shows current boot configuration. Both are valid.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q138",
      "domain": "domain5",
      "questionText": "What should you check if nvidia-smi shows different GPU counts than expected?",
      "type": "multiple-choice",
      "choices": [
        "CUDA version only",
        "PCIe slot seating, BIOS GPU settings, and dmesg for initialization errors",
        "Network configuration",
        "Slurm configuration"
      ],
      "correctAnswer": 1,
      "explanation": "Missing GPUs can be caused by: physical seating issues, BIOS disabling slots, driver initialization failures (check dmesg), or PCIe power issues.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q139",
      "domain": "domain5",
      "questionText": "A user reports 'CUDA out of memory' but nvidia-smi shows available memory. What could cause this?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi is wrong",
        "Memory fragmentation preventing large allocation, or wrong GPU selected",
        "Network issues",
        "CPU memory is full"
      ],
      "correctAnswer": 1,
      "explanation": "Memory fragmentation can prevent large contiguous allocations even with free memory. Also check CUDA_VISIBLE_DEVICES - application might target wrong GPU.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q140",
      "domain": "domain5",
      "questionText": "How do you check for GPU driver/CUDA version compatibility issues?",
      "type": "multiple-choice",
      "choices": [
        "Run nvidia-smi only",
        "Compare nvidia-smi CUDA version with nvcc --version and check NVIDIA compatibility matrix",
        "Check /etc/cuda/version",
        "Run dcgmi version"
      ],
      "correctAnswer": 1,
      "explanation": "nvidia-smi shows max CUDA version the driver supports. nvcc shows installed toolkit version. These must be compatible per NVIDIA's compatibility matrix.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q141",
      "domain": "domain3",
      "questionText": "What is the purpose of the 'nvidia-ctk' command?",
      "type": "multiple-choice",
      "choices": [
        "NVIDIA container toolkit CLI for configuring container runtime",
        "NVIDIA clock toolkit",
        "NVIDIA certificate toolkit",
        "NVIDIA compiler toolkit"
      ],
      "correctAnswer": 0,
      "explanation": "'nvidia-ctk' is the NVIDIA Container Toolkit CLI. Use it to configure Docker ('nvidia-ctk runtime configure'), generate CDI specs, and manage container GPU access.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q142",
      "domain": "domain3",
      "questionText": "Which Slurm command modifies a pending job's resource request?",
      "type": "multiple-choice",
      "choices": [
        "scontrol update job",
        "squeue modify",
        "sbatch --modify",
        "scancel --modify"
      ],
      "correctAnswer": 0,
      "explanation": "'scontrol update job <jobid> NumNodes=2' modifies pending job attributes. Not all attributes can be changed; some require job resubmission.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q143",
      "domain": "domain4",
      "questionText": "What does high 'PCIe Replay Count' in nvidia-smi indicate?",
      "type": "multiple-choice",
      "choices": [
        "Normal PCIe operation",
        "PCIe link errors requiring packet retransmission - signal integrity issue",
        "High PCIe bandwidth usage",
        "PCIe power saving active"
      ],
      "correctAnswer": 1,
      "explanation": "PCIe replay counts indicate transmission errors requiring packet retransmission. High counts suggest signal integrity issues - check slot seating, riser cards, or motherboard.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q144",
      "domain": "domain4",
      "questionText": "Which command exports GPU metrics in Prometheus format?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi --prometheus",
        "dcgm-exporter",
        "nvidia-exporter",
        "gpu-metrics-exporter"
      ],
      "correctAnswer": 1,
      "explanation": "dcgm-exporter exposes DCGM metrics in Prometheus format on HTTP endpoint. Used for monitoring GPU clusters with Prometheus/Grafana.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q145",
      "domain": "domain1",
      "questionText": "What is the purpose of 'nvidia-bug-report.sh'?",
      "type": "multiple-choice",
      "choices": [
        "Reports bugs to NVIDIA automatically",
        "Collects comprehensive system and GPU diagnostic information for troubleshooting",
        "Fixes known bugs",
        "Checks for driver updates"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-bug-report.sh' collects extensive diagnostic data: driver info, GPU state, dmesg logs, system configuration. Essential for NVIDIA support cases.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q146",
      "domain": "domain2",
      "questionText": "What command creates a compute instance within a GPU instance?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi mig -cci",
        "nvidia-smi mig -cgi",
        "nvidia-smi mig -C",
        "Both A and C"
      ],
      "correctAnswer": 3,
      "explanation": "'nvidia-smi mig -cci' (Create Compute Instance) or '-C' flag with -cgi creates a compute instance. CI must be created within an existing GPU instance.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q147",
      "domain": "domain4",
      "questionText": "What is the significance of 'Row Remapping' in GPU health?",
      "type": "multiple-choice",
      "choices": [
        "Memory optimization technique",
        "Hardware feature that remaps failing memory rows to spare rows",
        "GPU rendering optimization",
        "NVLink path optimization"
      ],
      "correctAnswer": 1,
      "explanation": "Row remapping uses spare memory rows to replace rows with uncorrectable errors. Check remapping status in nvidia-smi -q - excessive remapping indicates memory degradation.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q148",
      "domain": "domain5",
      "questionText": "What causes 'GPU has fallen off the bus' error?",
      "type": "multiple-choice",
      "choices": [
        "Driver bug",
        "PCIe communication failure - hardware issue with GPU, slot, or power",
        "Network disconnection",
        "Memory full"
      ],
      "correctAnswer": 1,
      "explanation": "GPU falling off bus (Xid 79) indicates PCIe communication failure. Causes: power issues, overheating, physical GPU/slot problems, or severe hardware failure.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q149",
      "domain": "domain3",
      "questionText": "How do you bind a Slurm job to specific GPUs using cgroups?",
      "type": "multiple-choice",
      "choices": [
        "--gpu-bind=closest",
        "GRES handles binding automatically when configured correctly",
        "--constraint=gpu",
        "--nodelist with GPU specification"
      ],
      "correctAnswer": 1,
      "explanation": "When Slurm GRES is properly configured with cgroup task affinity, GPU binding happens automatically. Jobs see only allocated GPUs via CUDA_VISIBLE_DEVICES.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q150",
      "domain": "domain4",
      "questionText": "What is the primary purpose of running STREAM benchmark on DGX systems?",
      "type": "multiple-choice",
      "choices": [
        "GPU memory bandwidth testing",
        "System (CPU) memory bandwidth validation",
        "Network bandwidth testing",
        "Storage bandwidth testing"
      ],
      "correctAnswer": 1,
      "explanation": "STREAM measures sustainable system memory (CPU RAM) bandwidth. Important for AI workloads that need to transfer data between CPU and GPU memory.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q151",
      "domain": "domain1",
      "questionText": "What does 'modprobe nvidia' do?",
      "type": "multiple-choice",
      "choices": [
        "Unloads NVIDIA driver",
        "Loads NVIDIA kernel module into the running kernel",
        "Updates NVIDIA driver",
        "Configures NVIDIA module options"
      ],
      "correctAnswer": 1,
      "explanation": "'modprobe nvidia' loads the NVIDIA kernel module. Typically done automatically at boot, but useful for manual driver loading after installation or troubleshooting.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q152",
      "domain": "domain5",
      "questionText": "How do you identify which application caused an XID error?",
      "type": "multiple-choice",
      "choices": [
        "XID errors don't include application information",
        "Check dmesg for PID and correlation with running processes at error time",
        "Run nvidia-smi --xid-app",
        "Check /var/log/nvidia/applications.log"
      ],
      "correctAnswer": 1,
      "explanation": "dmesg XID messages may include PID. Correlate timestamp with 'ps' output or logging systems. Some XID types include process information directly.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q153",
      "domain": "domain5",
      "questionText": "What does XID error code 54 indicate?",
      "type": "multiple-choice",
      "choices": [
        "GPU memory ECC error",
        "Hardware watchdog timeout - GPU failed internal health check",
        "NVLink communication failure",
        "Display engine error"
      ],
      "correctAnswer": 1,
      "explanation": "XID 54 indicates the GPU's internal hardware watchdog detected a timeout. This is more severe than XID 43 (external driver timeout) and typically requires a system reboot.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q154",
      "domain": "domain5",
      "questionText": "Which XID error code indicates NVLink training failure?",
      "type": "multiple-choice",
      "choices": ["XID 72", "XID 74", "XID 76", "XID 79"],
      "correctAnswer": 2,
      "explanation": "XID 76 indicates NVLink failed to complete link training. This is a physical connection problem requiring GPU reseat or hardware replacement.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q155",
      "domain": "domain5",
      "questionText": "What is the correct response to XID 27 (GPU memory interface error)?",
      "type": "multiple-choice",
      "choices": [
        "Reset the GPU with nvidia-smi --gpu-reset",
        "Clear ECC counters and continue",
        "Run memory diagnostics and plan for GPU replacement",
        "Update the NVIDIA driver"
      ],
      "correctAnswer": 2,
      "explanation": "XID 27 indicates memory interface failure (not ECC error). This is a hardware failure where communication between GPU core and VRAM failed. GPU replacement is typically required.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q156",
      "domain": "domain5",
      "questionText": "XID 72 indicates what type of NVLink problem?",
      "type": "multiple-choice",
      "choices": [
        "Physical cable disconnection",
        "Flow control credits exhausted or protocol error",
        "ECC error on NVLink data path",
        "Link training failure"
      ],
      "correctAnswer": 1,
      "explanation": "XID 72 indicates NVLink flow control issues - congestion or protocol errors. It often precedes more severe NVLink errors and may indicate early link degradation.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q157",
      "domain": "domain4",
      "questionText": "What NCCL environment variable forces communication over PCIe instead of NVLink?",
      "type": "multiple-choice",
      "choices": [
        "NCCL_NVLINK_DISABLE=1",
        "NCCL_P2P_DISABLE=1",
        "NCCL_PCIE_ONLY=1",
        "NCCL_IB_DISABLE=1"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL_P2P_DISABLE=1 disables peer-to-peer communication including NVLink, forcing traffic through PCIe. Useful for diagnosing NVLink performance issues.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q158",
      "domain": "domain4",
      "questionText": "What is the expected all-reduce bandwidth for 8 A100 GPUs over NVLink?",
      "type": "multiple-choice",
      "choices": ["50-80 GB/s", "100-150 GB/s", "200-250 GB/s", "400-500 GB/s"],
      "correctAnswer": 2,
      "explanation": "8 A100 GPUs with NVSwitch achieve approximately 200-250 GB/s all-reduce bandwidth. H100 systems achieve 400-450 GB/s with their faster NVLink 4.0.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q159",
      "domain": "domain4",
      "questionText": "In HPL benchmarking, what does the parameter 'N' represent?",
      "type": "multiple-choice",
      "choices": [
        "Number of GPUs",
        "Problem size (matrix dimension)",
        "Number of iterations",
        "Block size"
      ],
      "correctAnswer": 1,
      "explanation": "N is the problem size - the dimension of the dense matrix being solved. Larger N typically gives better GFLOPS but requires more memory. Optimal N uses ~90% of available GPU memory.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q160",
      "domain": "domain4",
      "questionText": "What does NCCL_DEBUG=INFO show in the output?",
      "type": "multiple-choice",
      "choices": [
        "Only error messages",
        "Transport selection, ring/tree topology, and connection details",
        "GPU memory usage",
        "Network packet traces"
      ],
      "correctAnswer": 1,
      "explanation": "NCCL_DEBUG=INFO shows informational messages including transport selection (NVLink, PCIe, IB), collective algorithm choice, and connection topology.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q161",
      "domain": "domain3",
      "questionText": "In BCM High Availability, what happens to the Virtual IP when failover occurs?",
      "type": "multiple-choice",
      "choices": [
        "VIP is deleted and must be manually reassigned",
        "VIP automatically migrates to the new active node",
        "Both nodes share the VIP",
        "VIP remains on the failed node"
      ],
      "correctAnswer": 1,
      "explanation": "In BCM HA, the Virtual IP (VIP) automatically migrates to the new active node during failover, allowing clients to maintain connectivity through the same IP address.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q162",
      "domain": "domain3",
      "questionText": "Which command deploys the NVIDIA GPU Operator on Kubernetes?",
      "type": "multiple-choice",
      "choices": [
        "kubectl apply -f gpu-operator.yaml",
        "helm install gpu-operator nvidia/gpu-operator -n gpu-operator",
        "docker run nvidia/gpu-operator",
        "kubeadm init --gpu-operator"
      ],
      "correctAnswer": 1,
      "explanation": "The GPU Operator is deployed using Helm: 'helm install gpu-operator nvidia/gpu-operator -n gpu-operator'. The chart is from the nvidia Helm repository.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q163",
      "domain": "domain3",
      "questionText": "Why set '--set driver.enabled=false' when installing GPU Operator on DGX?",
      "type": "multiple-choice",
      "choices": [
        "DGX doesn't support containerized drivers",
        "DGX has pre-installed NVIDIA drivers that should be used",
        "It improves GPU performance",
        "It's required for MIG mode"
      ],
      "correctAnswer": 1,
      "explanation": "DGX systems come with pre-installed, validated NVIDIA drivers. Setting driver.enabled=false tells the GPU Operator to use the host drivers instead of deploying containerized ones.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q164",
      "domain": "domain4",
      "questionText": "What is the formula to calculate optimal HPL problem size (N)?",
      "type": "multiple-choice",
      "choices": [
        "N = total_memory / 8",
        "N = sqrt(0.9 * total_gpu_memory / 8)",
        "N = number_of_gpus * 1024",
        "N = memory_in_GB * 100"
      ],
      "correctAnswer": 1,
      "explanation": "N = sqrt(0.9 * total_gpu_memory / 8) where memory is in bytes and 8 accounts for double-precision (8 bytes per element). This uses ~90% of GPU memory.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q165",
      "domain": "domain5",
      "questionText": "After an XID 79 error, what is the correct immediate action?",
      "type": "multiple-choice",
      "choices": [
        "Run nvidia-smi --gpu-reset",
        "Clear ECC counters",
        "Reboot the node - GPU reset won't work as GPU is unreachable",
        "Update NVIDIA drivers"
      ],
      "correctAnswer": 2,
      "explanation": "XID 79 means the GPU has 'fallen off the bus' - it's completely unreachable. GPU reset is impossible because the device cannot be communicated with. A full node reboot is required.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q166",
      "domain": "domain1",
      "questionText": "What does 'dmidecode -t system' display?",
      "type": "multiple-choice",
      "choices": [
        "System memory information",
        "System manufacturer, product name, serial number, and UUID",
        "System temperature sensors",
        "System network configuration"
      ],
      "correctAnswer": 1,
      "explanation": "'dmidecode -t system' shows SMBIOS system information including manufacturer (NVIDIA for DGX), product name (DGX A100/H100), serial number, and UUID.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q167",
      "domain": "domain4",
      "questionText": "Which NCCL collective operation is most commonly used in distributed deep learning training?",
      "type": "multiple-choice",
      "choices": ["Broadcast", "All-gather", "All-reduce", "Reduce-scatter"],
      "correctAnswer": 2,
      "explanation": "All-reduce is most common in DL training as it sums gradients across all GPUs and distributes the result back. This is the core operation for synchronous distributed training.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q168",
      "domain": "domain5",
      "questionText": "What distinguishes XID 78 from other NVLink errors?",
      "type": "multiple-choice",
      "choices": [
        "It indicates flow control problems",
        "It indicates uncorrectable ECC error on NVLink data path",
        "It indicates link training failure",
        "It indicates timeout during communication"
      ],
      "correctAnswer": 1,
      "explanation": "XID 78 specifically indicates an uncorrectable ECC error on the NVLink data path - data corruption occurred during GPU-to-GPU communication. This requires hardware replacement.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q169",
      "domain": "domain1",
      "questionText": "What is the primary purpose of enabling persistence mode with 'nvidia-smi -pm 1'?",
      "type": "multiple-choice",
      "choices": [
        "It saves GPU settings across reboots",
        "It keeps the NVIDIA driver loaded even when no client applications are running",
        "It enables persistent logging of GPU errors to syslog",
        "It locks the GPU clock speed to maximum frequency"
      ],
      "correctAnswer": 1,
      "explanation": "Persistence mode keeps the NVIDIA kernel driver loaded even when no applications are using the GPU. Without it, the driver unloads between uses, causing significant latency on the next GPU operation due to driver re-initialization.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q170",
      "domain": "domain1",
      "questionText": "Which command sets a GPU to 'EXCLUSIVE_PROCESS' compute mode, allowing only one CUDA context per GPU?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi --compute-mode=1",
        "nvidia-smi -c EXCLUSIVE_PROCESS",
        "nvidia-smi --set-exclusive",
        "nvidia-smi -e 1"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -c EXCLUSIVE_PROCESS' (or '-c 3') sets the compute mode so only one CUDA context can be created on the GPU at a time. This is commonly used in HPC and multi-user environments to prevent GPU sharing conflicts.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q171",
      "domain": "domain1",
      "questionText": "During DGX system BIOS configuration, which setting must be enabled for GPU direct RDMA (GPUDirect) to function?",
      "type": "multiple-choice",
      "choices": [
        "Hyper-Threading",
        "SR-IOV (Single Root I/O Virtualization)",
        "Above 4G Decoding and MMIO High Size",
        "Intel VT-d / AMD IOMMU"
      ],
      "correctAnswer": 2,
      "explanation": "Above 4G Decoding must be enabled along with sufficient MMIO High Size to allow the system to map the large BAR (Base Address Register) spaces of GPUs. Without this, GPUDirect RDMA cannot map GPU memory to PCIe address space.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q172",
      "domain": "domain1",
      "questionText": "What does 'nvidia-smi topo -m' display?",
      "type": "multiple-choice",
      "choices": [
        "GPU memory topology showing HBM stack layout",
        "GPU interconnect topology matrix showing NVLink and PCIe connections between GPUs",
        "Network topology of InfiniBand switches",
        "CPU-to-GPU NUMA topology only"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi topo -m' outputs a matrix showing the interconnect type between each pair of GPUs (e.g., NVLink, PIX, PHB, SYS). It also shows CPU affinity for each GPU, which is critical for optimizing multi-GPU workloads.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q173",
      "domain": "domain1",
      "questionText": "Which command applies a power limit of 300 watts to GPU 0?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -i 0 --power-limit=300",
        "nvidia-smi -i 0 -pl 300",
        "nvidia-smi --set-power 0 300W",
        "nvidia-smi -pm 300 -i 0"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -i 0 -pl 300' sets the power limit for GPU 0 to 300 watts. The -pl flag accepts the value in watts. The power limit must be within the min/max range supported by the GPU (visible with nvidia-smi -q -d POWER).",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q174",
      "domain": "domain1",
      "questionText": "What is the recommended approach for installing the OS on a new DGX A100 system?",
      "type": "multiple-choice",
      "choices": [
        "Install a standard Ubuntu Server image and add GPU drivers manually",
        "Use the NVIDIA DGX OS ISO image which includes pre-configured drivers and firmware",
        "Install CentOS and compile CUDA drivers from source",
        "Use PXE boot with a generic Linux kernel"
      ],
      "correctAnswer": 1,
      "explanation": "NVIDIA provides a DGX OS ISO (based on Ubuntu) that includes pre-configured NVIDIA drivers, CUDA toolkit, container runtime, and firmware utilities specifically tuned for DGX hardware. This ensures all components are compatible and optimized.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q175",
      "domain": "domain1",
      "questionText": "During initial bring-up, which command verifies the firmware version of InfiniBand HCAs?",
      "type": "multiple-choice",
      "choices": [
        "ibstat -v",
        "mlxfwmanager --query",
        "ibdiagnet --fw-check",
        "iblinkinfo --firmware"
      ],
      "correctAnswer": 1,
      "explanation": "'mlxfwmanager --query' lists all Mellanox/NVIDIA HCAs in the system along with their current firmware versions and available updates. This is the standard tool for firmware management on ConnectX adapters.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q176",
      "domain": "domain1",
      "questionText": "Which command checks PCIe link speed and width for all NVIDIA GPUs in the system?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi -q -d PCIE",
        "lspci -vvv | grep -i nvidia",
        "nvidia-smi --query-gpu=pcie.link.gen.current,pcie.link.width.current --format=csv",
        "All of the above can show PCIe link information"
      ],
      "correctAnswer": 3,
      "explanation": "All three commands can provide PCIe link information. 'nvidia-smi -q -d PCIE' shows detailed PCIe info, 'lspci -vvv' shows kernel-level PCI device details, and the query flag provides CSV-formatted output. Using multiple tools helps cross-verify link training.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q177",
      "domain": "domain1",
      "questionText": "What type of GPU memory does the DGX A100 use, and how does it differ from system RAM?",
      "type": "multiple-choice",
      "choices": [
        "GDDR6 - faster clock speed than DDR4 system RAM",
        "HBM2e - stacked memory with much higher bandwidth than DDR4 system RAM",
        "LPDDR5 - lower power consumption than DDR4 system RAM",
        "HBM3 - identical architecture to system RAM but on the GPU package"
      ],
      "correctAnswer": 1,
      "explanation": "The A100 GPU uses HBM2e (High Bandwidth Memory 2e) which provides up to 2 TB/s bandwidth per GPU. HBM uses a 3D-stacked architecture on the GPU package, delivering dramatically higher bandwidth than DDR4 system RAM (~200 GB/s).",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q178",
      "domain": "domain1",
      "questionText": "What is the correct command to set the BMC network configuration on a DGX system using ipmitool?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool lan set 1 ipaddr 10.0.0.100",
        "ipmitool bmc config --ip 10.0.0.100",
        "ipmitool network set eth0 10.0.0.100",
        "ipmitool chassis bootdev network"
      ],
      "correctAnswer": 0,
      "explanation": "'ipmitool lan set 1 ipaddr <IP>' configures the BMC's network IP address on LAN channel 1. Additional settings like netmask ('ipmitool lan set 1 netmask') and gateway are also configured via the 'lan set' subcommand.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q179",
      "domain": "domain1",
      "questionText": "Which nvidia-smi flag displays the current GPU driver version and CUDA version?",
      "type": "multiple-choice",
      "choices": [
        "nvidia-smi --version",
        "nvidia-smi (no flags - shown in the header)",
        "nvidia-smi -q -d VERSION",
        "nvidia-smi --driver-info"
      ],
      "correctAnswer": 1,
      "explanation": "Running 'nvidia-smi' with no flags displays the main dashboard which includes the NVIDIA driver version and the maximum supported CUDA version in the header row. This is the quickest way to verify driver initialization after bring-up.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q180",
      "domain": "domain1",
      "questionText": "During GPU firmware update, what is the critical step after flashing new firmware with 'nvidia-smi --gpu-reset'?",
      "type": "multiple-choice",
      "choices": [
        "Restart the NVIDIA driver with 'systemctl restart nvidia-persistenced'",
        "Cold reboot the entire node to fully activate the new firmware",
        "Run 'nvidia-smi -pm 1' to re-enable persistence mode",
        "Execute 'modprobe -r nvidia && modprobe nvidia' to reload the driver"
      ],
      "correctAnswer": 1,
      "explanation": "After a GPU firmware update, a cold reboot (full power cycle) is required to activate the new firmware in the GPU's ROM. A warm reboot or driver reload is insufficient because the firmware is loaded from ROM during the hardware power-on sequence.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q181",
      "domain": "domain1",
      "questionText": "What does 'nvidia-smi -q -d POWER' report for a healthy DGX A100 GPU?",
      "type": "multiple-choice",
      "choices": [
        "Only the current power draw in watts",
        "Power draw, power limit, default power limit, min/max power limit, and power state",
        "Power supply unit voltage and amperage",
        "Cumulative energy consumption since last boot in kilowatt-hours"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -q -d POWER' shows comprehensive power information including current draw, enforced power limit, default limit, minimum and maximum configurable limits, and the current power state (P0-P12). This is essential for validating power capping configurations.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q182",
      "domain": "domain1",
      "questionText": "Which ipmitool command reads the system event log (SEL) to check for hardware errors during bring-up?",
      "type": "multiple-choice",
      "choices": [
        "ipmitool sel list",
        "ipmitool event log",
        "ipmitool sdr list",
        "ipmitool chassis status"
      ],
      "correctAnswer": 0,
      "explanation": "'ipmitool sel list' displays the System Event Log which records hardware events such as thermal warnings, power supply failures, and memory errors. Reviewing the SEL during bring-up is critical for identifying pre-existing hardware issues.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q183",
      "domain": "domain1",
      "questionText": "What NVSwitch topology does 'nvidia-smi topo -m' show for a properly configured DGX A100?",
      "type": "multiple-choice",
      "choices": [
        "All GPUs connected via PCIe switches only (PIX)",
        "All 8 GPUs connected to all other GPUs via NVLink through 6 NVSwitches",
        "4 GPUs in one NVLink domain and 4 in another, connected via PCIe",
        "GPUs connected in a ring topology via NVLink"
      ],
      "correctAnswer": 1,
      "explanation": "A properly configured DGX A100 has 6 NVSwitches providing full all-to-all NVLink connectivity between all 8 A100 GPUs. The topology matrix should show 'NV12' between every GPU pair, indicating 12 NVLink connections per pair through the switches.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q184",
      "domain": "domain1",
      "questionText": "Which 'sensors' command output is most important to verify during DGX system bring-up?",
      "type": "multiple-choice",
      "choices": [
        "Fan RPM speeds only",
        "CPU temperature, inlet/exhaust temperatures, and voltage rails",
        "Hard drive SMART status",
        "Network interface link speed"
      ],
      "correctAnswer": 1,
      "explanation": "The 'sensors' command (from lm-sensors) shows thermal readings (CPU, inlet, exhaust) and voltage rail status. During bring-up, verifying that temperatures are within range and voltages are stable ensures the cooling system and power delivery are functioning correctly.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q185",
      "domain": "domain1",
      "questionText": "What does 'nvidia-smi -q -d ECC' show, and why is it checked during server bring-up?",
      "type": "multiple-choice",
      "choices": [
        "ECC memory prices and vendor information",
        "Volatile and aggregate single-bit and double-bit ECC error counts for GPU memory",
        "Whether ECC is supported by the system BIOS",
        "ECC status of system DDR memory modules"
      ],
      "correctAnswer": 1,
      "explanation": "'nvidia-smi -q -d ECC' displays ECC error counts for GPU HBM memory, separated into volatile (since last driver load) and aggregate (lifetime) categories. Checking this during bring-up ensures the GPU memory has no pre-existing uncorrectable errors that would indicate faulty hardware.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q186",
      "domain": "domain1",
      "questionText": "During DGX system BIOS configuration, why is NUMA (Non-Uniform Memory Access) balancing important for GPU workloads?",
      "type": "multiple-choice",
      "choices": [
        "It enables GPU memory to be shared across nodes in a cluster",
        "It ensures GPUs are accessed by the CPU in the same NUMA domain to minimize memory access latency",
        "It allows more system RAM to be allocated to GPU processes",
        "It distributes GPU compute tasks evenly across all NUMA domains"
      ],
      "correctAnswer": 1,
      "explanation": "Proper NUMA configuration ensures each GPU is accessed by its local CPU socket, minimizing PCIe traversal and memory access latency. Mismatched NUMA affinity (e.g., CPU socket 0 accessing a GPU on socket 1) can degrade performance by 10-30%.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q187",
      "domain": "domain3",
      "questionText": "What is the purpose of the NVIDIA GPU device plugin for Kubernetes?",
      "type": "multiple-choice",
      "choices": [
        "It installs NVIDIA drivers on Kubernetes nodes automatically",
        "It exposes GPU resources to Kubernetes so pods can request 'nvidia.com/gpu' in resource limits",
        "It provides a web dashboard for monitoring GPU utilization in the cluster",
        "It manages GPU firmware updates across Kubernetes nodes"
      ],
      "correctAnswer": 1,
      "explanation": "The NVIDIA device plugin for Kubernetes implements the Kubernetes Device Plugin API. It advertises GPU resources (nvidia.com/gpu) to the kubelet, enabling pods to request GPUs via resource limits and ensuring proper GPU allocation and isolation.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q188",
      "domain": "domain3",
      "questionText": "Which component does the NVIDIA GPU Operator install and manage as part of its stack?",
      "type": "multiple-choice",
      "choices": [
        "Kubernetes API server and etcd",
        "NVIDIA drivers, container toolkit, device plugin, DCGM exporter, and MIG manager",
        "Only the NVIDIA container runtime and nothing else",
        "Prometheus and Grafana monitoring stack"
      ],
      "correctAnswer": 1,
      "explanation": "The GPU Operator automates deployment and lifecycle management of all NVIDIA software components needed for GPU workloads in Kubernetes: drivers, container toolkit, device plugin, DCGM/DCGM-exporter, GPU Feature Discovery, MIG manager, and validator.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q189",
      "domain": "domain3",
      "questionText": "What is the role of nvidia-container-toolkit in the container runtime stack?",
      "type": "multiple-choice",
      "choices": [
        "It replaces Docker entirely with a GPU-optimized container engine",
        "It provides a custom OCI runtime hook that injects GPU libraries and devices into containers",
        "It converts CUDA applications into container images automatically",
        "It manages GPU memory allocation within running containers"
      ],
      "correctAnswer": 1,
      "explanation": "nvidia-container-toolkit provides an OCI prestart hook (nvidia-container-runtime-hook) that modifies the container spec to inject NVIDIA driver libraries, CUDA libraries, and GPU device files (/dev/nvidia*) into the container at launch time.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q190",
      "domain": "domain3",
      "questionText": "In the NVIDIA DGX Software Stack (formerly BCM), which component handles cluster-wide job scheduling?",
      "type": "multiple-choice",
      "choices": [
        "NVIDIA GPU Operator",
        "Slurm Workload Manager or Kubernetes with GPU scheduling",
        "DCGM (Data Center GPU Manager)",
        "NCCL (NVIDIA Collective Communications Library)"
      ],
      "correctAnswer": 1,
      "explanation": "The DGX Software Stack uses either Slurm (for traditional HPC) or Kubernetes (for cloud-native) as the cluster-wide job scheduler. Slurm manages job queues, resource allocation, and multi-node GPU job scheduling across the DGX cluster.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q191",
      "domain": "domain3",
      "questionText": "How does the DCGM exporter integrate with Prometheus for GPU monitoring?",
      "type": "multiple-choice",
      "choices": [
        "It writes GPU metrics directly to Prometheus storage files",
        "It exposes GPU metrics as a Prometheus-compatible HTTP endpoint that Prometheus scrapes",
        "It sends GPU metrics to Prometheus via gRPC streaming",
        "It requires a separate Prometheus plugin to query DCGM APIs"
      ],
      "correctAnswer": 1,
      "explanation": "The DCGM exporter runs as a service (typically a DaemonSet in Kubernetes) that collects GPU metrics via DCGM and exposes them on an HTTP endpoint (default port 9400) in Prometheus exposition format. Prometheus then scrapes this endpoint at configured intervals.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q192",
      "domain": "domain1",
      "questionText": "Select ALL commands that can display GPU temperature information:",
      "type": "multiple-select",
      "choices": [
        "nvidia-smi -q -d TEMPERATURE",
        "nvidia-smi --query-gpu=temperature.gpu --format=csv",
        "dcgmi diag -r 1",
        "ipmitool sensor list | grep -i gpu"
      ],
      "correctAnswer": [0, 1, 3],
      "explanation": "'nvidia-smi -q -d TEMPERATURE' shows detailed thermal info, '--query-gpu=temperature.gpu' provides CSV output, and 'ipmitool sensor list' exposes BMC thermal sensors including GPU temps. 'dcgmi diag -r 1' runs a diagnostic but does not primarily display temperature readings.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q193",
      "domain": "domain4",
      "questionText": "Select ALL dcgmi diag run levels that include NVLink bandwidth and stress testing:",
      "type": "multiple-select",
      "choices": [
        "dcgmi diag -r 1 (Quick - ~30 seconds)",
        "dcgmi diag -r 2 (Medium - ~2 minutes)",
        "dcgmi diag -r 3 (Long - ~15 minutes)",
        "dcgmi diag -r 4 (Extended - ~30 minutes)"
      ],
      "correctAnswer": [2, 3],
      "explanation": "NVLink stress and bandwidth tests are only included in level 3 (Long) and level 4 (Extended) diagnostics. Level 1 runs only basic health checks, and level 2 adds PCIe and short memory tests but does not include full NVLink bandwidth testing.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q194",
      "domain": "domain5",
      "questionText": "Select ALL indicators that a GPU is experiencing thermal throttling:",
      "type": "multiple-select",
      "choices": [
        "nvidia-smi shows 'SW Thermal Slowdown: Active'",
        "GPU clock frequency drops below the default base clock",
        "nvidia-smi shows power draw at exactly 0W",
        "GPU performance state changes from P0 to P2 or lower under sustained load"
      ],
      "correctAnswer": [0, 1, 3],
      "explanation": "Thermal throttling manifests as the SW Thermal Slowdown flag becoming Active, GPU clocks reducing below base frequency, and the performance state dropping (P0 is max, P2+ indicates throttling). A power draw of 0W would indicate the GPU is off or unresponsive, not throttling.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q195",
      "domain": "domain3",
      "questionText": "Select ALL components that are part of the nvidia-container-toolkit ecosystem:",
      "type": "multiple-select",
      "choices": [
        "nvidia-container-runtime (OCI-compliant runtime wrapper)",
        "nvidia-container-cli (low-level CLI for GPU container config)",
        "nvidia-gpu-scheduler (Kubernetes scheduler extender)",
        "libnvidia-container (library for GPU container setup)"
      ],
      "correctAnswer": [0, 1, 3],
      "explanation": "The nvidia-container-toolkit consists of nvidia-container-runtime (wraps runc/crun), nvidia-container-cli (CLI for configuring GPU access), and libnvidia-container (C library that handles the actual device and driver injection). The GPU scheduler extender is not part of the container toolkit.",
      "points": 1,
      "difficulty": "advanced"
    },
    {
      "id": "q196",
      "domain": "domain4",
      "questionText": "Select ALL valid NCCL environment variables used for performance tuning:",
      "type": "multiple-select",
      "choices": [
        "NCCL_DEBUG=INFO",
        "NCCL_IB_DISABLE=1",
        "NCCL_SOCKET_IFNAME=eth0",
        "NCCL_GPU_COUNT=8"
      ],
      "correctAnswer": [0, 1, 2],
      "explanation": "NCCL_DEBUG controls logging verbosity, NCCL_IB_DISABLE disables InfiniBand transport, and NCCL_SOCKET_IFNAME specifies which network interface to use for socket-based communication. NCCL_GPU_COUNT is not a valid NCCL environment variable; NCCL auto-detects GPU count.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q197",
      "domain": "domain1",
      "questionText": "Select ALL ipmitool subcommands that can retrieve hardware inventory information:",
      "type": "multiple-select",
      "choices": [
        "ipmitool fru print",
        "ipmitool sdr list",
        "ipmitool mc info",
        "ipmitool power status"
      ],
      "correctAnswer": [0, 1, 2],
      "explanation": "'fru print' shows Field Replaceable Unit data (serial numbers, part numbers, manufacturer), 'sdr list' shows all Sensor Data Records (hardware sensors), and 'mc info' shows BMC management controller details. 'power status' only shows on/off state, not inventory.",
      "points": 1,
      "difficulty": "intermediate"
    },
    {
      "id": "q198",
      "domain": "domain4",
      "questionText": "Select ALL valid Slurm node states that indicate a node is unavailable for new jobs:",
      "type": "multiple-select",
      "choices": ["down", "drained", "idle", "draining"],
      "correctAnswer": [0, 1, 3],
      "explanation": "'down' means the node is unresponsive or marked offline, 'drained' means the admin has removed it from scheduling and all jobs have completed, and 'draining' means no new jobs are accepted but existing jobs continue running. 'idle' means the node is available and ready for work.",
      "points": 1,
      "difficulty": "beginner"
    },
    {
      "id": "q199",
      "domain": "domain2",
      "questionText": "Select ALL valid InfiniBand port physical states as reported by 'ibstat':",
      "type": "multiple-select",
      "choices": ["LinkUp", "Polling", "Disabled", "LinkReady"],
      "correctAnswer": [0, 1, 2],
      "explanation": "Valid IB physical port states include LinkUp (fully operational), Polling (attempting to establish link with remote port), and Disabled (administratively shut down). 'LinkReady' is not a valid InfiniBand physical state; the correct progression is Polling -> LinkUp.",
      "points": 1,
      "difficulty": "intermediate"
    }
  ]
}
