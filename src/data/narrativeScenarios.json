{
  "scenarios": [
    {
      "id": "domain0-linux-navigation",
      "domain": 0,
      "title": "Terminal Bootcamp: Getting Around",
      "narrative": {
        "hook": "Your first day at the datacenter — you need to learn the terminal before your shift begins.",
        "setting": "You've just been hired as a junior systems engineer at a GPU cluster facility. Before you can troubleshoot anything, you need to master the Linux terminal. Your mentor has set up a training session on one of the DGX nodes.",
        "resolution": "You now know how to navigate the Linux filesystem, read files, and find your way around a DGX node. These skills are the foundation for every datacenter task ahead."
      },
      "commandFamilies": ["linux-basics"],
      "estimatedMinutes": 12,
      "difficulty": "beginner",
      "tier": 1,
      "skippable": true,
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Your mentor starts by explaining the Linux directory structure on the DGX node.",
          "task": "The Linux Directory Tree",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Everything in Linux lives under a single root directory /. On a DGX system you'll frequently visit these paths:\n\n/ — Root of the entire filesystem\n/home — User home directories\n/var/log — System and application logs\n/etc — Configuration files\n/dev — Device files (including GPUs)\n/usr/bin — Installed programs like nvidia-smi\n/opt — Optional/third-party software\n\nDatacenter troubleshooting almost always starts with navigating to the right log or config file.",
          "tips": [
            "GPU device files appear under /dev/nvidia*",
            "NVIDIA driver logs land in /var/log"
          ]
        },
        {
          "id": "step-2",
          "type": "concept",
          "situation": "Your mentor asks: 'Do you know where you are right now?' Try it yourself.",
          "task": "Where Am I? (pwd)",
          "expectedCommands": ["pwd"],
          "hints": ["Type: pwd"],
          "validation": {
            "type": "command",
            "command": "pwd"
          },
          "conceptContent": "pwd (print working directory) shows your current location in the filesystem.\n\nWhen you first log into a DGX node you'll typically land in your home directory:\n\n$ pwd\n/home/admin\n\nKnowing where you are is critical before running relative commands like cd .. or ls logs/.",
          "tips": [
            "The shell prompt often shows your current directory",
            "The ~ symbol is shorthand for your home directory"
          ]
        },
        {
          "id": "step-3",
          "type": "observe",
          "situation": "Your mentor says: 'Let me show you what's in the log directory — this is where you'll find GPU driver messages.'",
          "task": "Listing Files (ls)",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "observeCommand": "ls -la /var/log",
          "conceptContent": "ls lists directory contents. The -la flags add detail:\n-l long format (permissions, owner, size, date)\n-a show hidden files (starting with .)"
        },
        {
          "id": "step-4",
          "type": "concept",
          "situation": "Your mentor explains how to move between directories. Try navigating to the log directory.",
          "task": "Changing Directories (cd)",
          "expectedCommands": ["cd /var/log"],
          "hints": ["Type: cd /var/log"],
          "validation": {
            "type": "command",
            "command": "cd"
          },
          "conceptContent": "cd changes your working directory. There are several ways to specify where to go:\n\ncd /var/log — Absolute path, go directly there\ncd logs — Relative path, enter a subdirectory\ncd .. — Go up one level\ncd ~ — Go to your home directory\ncd - — Go back to previous directory\n\nAbsolute paths start with / and work from anywhere. Relative paths start from your current location.",
          "tips": [
            "Tab completion works with cd — press Tab to auto-complete directory names",
            "cd with no arguments also takes you home"
          ],
          "quiz": {
            "question": "A technician needs to check NVIDIA driver logs. Which path would they navigate to?",
            "options": ["/var/log", "/home/nvidia", "/dev/gpu", "/usr/bin"],
            "correctIndex": 0,
            "explanation": "System and driver logs are stored under /var/log. NVIDIA-specific logs such as kernel messages about GPU drivers appear here."
          }
        },
        {
          "id": "step-5",
          "type": "concept",
          "situation": "Now your mentor teaches you how to read files — critical for checking configs and logs. Try reading the hostname file.",
          "task": "Dumping Files with cat",
          "expectedCommands": ["cat /etc/hostname"],
          "hints": ["Type: cat /etc/hostname"],
          "validation": {
            "type": "command",
            "command": "cat"
          },
          "conceptContent": "cat (concatenate) outputs the entire contents of a file. It's great for short files like configs:\n\n$ cat /etc/hostname\ndgx-node-01\n\nWhen to use cat:\n- Short config files (/etc/hostname, /etc/hosts)\n- Checking a single setting quickly\n\nWhen NOT to use cat:\n- Large log files (thousands of lines will flood your terminal)\n- Binary files (produces garbage output)\n\nFor large files, use head, tail, or less instead.",
          "tips": [
            "cat -n adds line numbers to the output",
            "You can cat multiple files at once: cat file1 file2"
          ]
        },
        {
          "id": "step-6",
          "type": "observe",
          "situation": "Your mentor demonstrates cat on a network configuration file.",
          "task": "cat in Action",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "observeCommand": "cat /etc/hosts"
        },
        {
          "id": "step-7",
          "type": "concept",
          "situation": "Your mentor warns: 'Never cat a huge log file. Use head and tail instead.' Try viewing the last 20 lines of the syslog.",
          "task": "Previewing with head and tail",
          "expectedCommands": ["tail -20 /var/log/syslog"],
          "hints": ["Type: tail -20 /var/log/syslog"],
          "validation": {
            "type": "command",
            "command": "tail"
          },
          "conceptContent": "When files are too large for cat, use head or tail to see just part of the file:\n\nhead -20 file — First 20 lines\ntail -20 file — Last 20 lines\ntail -f file — Follow new lines in real-time\n\ntail -f is especially powerful for watching live logs — for example, monitoring GPU errors as a training job runs:\n\n$ tail -f /var/log/syslog\n\nPress Ctrl+C to stop following.",
          "tips": [
            "Default is 10 lines if you omit the number",
            "tail -f is your best friend for live debugging"
          ]
        },
        {
          "id": "step-8",
          "type": "observe",
          "situation": "Your mentor shows you the last lines of the system log.",
          "task": "tail in Action",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "observeCommand": "tail -20 /var/log/syslog",
          "quiz": {
            "question": "You need to watch a log file in real-time as a GPU training job runs. Which command?",
            "options": [
              "cat /var/log/syslog",
              "head -f /var/log/syslog",
              "tail -f /var/log/syslog",
              "less /var/log/syslog"
            ],
            "correctIndex": 2,
            "explanation": "tail -f follows a file, showing new lines as they are appended. This is essential for monitoring live GPU training logs."
          }
        },
        {
          "id": "step-9",
          "type": "command",
          "situation": "Your mentor says: 'Now try it yourself. Check the node name and kernel version.'",
          "task": "Run hostname to verify the node name, then run uname -a to check kernel info.",
          "expectedCommands": ["hostname", "uname -a"],
          "hints": ["Type: hostname", "Type: uname -a"],
          "validation": {
            "type": "command",
            "command": "hostname"
          }
        },
        {
          "id": "step-10",
          "type": "command",
          "situation": "Your mentor nods approvingly. 'Good. Now check the system log.'",
          "task": "Run nvidia-smi to see the GPUs on this training node.",
          "expectedCommands": ["nvidia-smi"],
          "hints": ["Type: nvidia-smi", "This shows all GPUs and their status"],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU"
          }
        }
      ]
    },
    {
      "id": "domain0-linux-output",
      "domain": 0,
      "title": "Terminal Bootcamp: Output Filtering",
      "narrative": {
        "hook": "Your second day — master output filtering to triage a cluster alert.",
        "setting": "Day two at the datacenter. An alert fired overnight about potential GPU issues across the cluster. Your mentor wants you to use pipes and filtering to quickly triage the situation before escalating.",
        "resolution": "You can now pipe commands together, filter with grep, and count results with wc. These skills let you quickly triage large outputs from GPU monitoring and cluster management tools."
      },
      "commandFamilies": ["linux-basics"],
      "estimatedMinutes": 13,
      "difficulty": "beginner",
      "tier": 1,
      "skippable": true,
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Your mentor explains the most powerful concept in Linux: the pipe operator.",
          "task": "The Pipe Operator",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "The pipe | takes the output of one command and feeds it as input to the next. This is one of the most powerful ideas in Linux:\n\ncommand1 | command2 | command3\n\nEach command in the pipeline processes the data and passes its output along. For example:\n\nnvidia-smi | grep GPU\n\nThis runs nvidia-smi, then filters its output to show only lines containing 'GPU'.\n\nIn datacenter work you'll use pipes constantly — large outputs from monitoring tools need to be filtered down to the information you actually need.",
          "tips": [
            "You can chain as many pipes as you need",
            "Each command in the pipe runs as a separate process"
          ]
        },
        {
          "id": "step-2",
          "type": "command",
          "situation": "Your mentor says: 'Let's triage that alert. Start by filtering nvidia-smi output.'",
          "task": "Use a pipe to filter nvidia-smi output to show only GPU lines.",
          "expectedCommands": ["nvidia-smi | grep GPU"],
          "hints": [
            "Try: nvidia-smi | grep GPU",
            "The pipe | sends nvidia-smi output to grep"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "grep"
          }
        },
        {
          "id": "step-3",
          "type": "concept",
          "situation": "Your mentor shows you useful grep flags for more precise filtering.",
          "task": "grep Flags",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "grep searches for patterns in text. Useful flags:\n\n-i — Case-insensitive search (grep -i error logfile)\n-c — Count matches (grep -c GPU output)\n-v — Invert / exclude matches (grep -v idle nodes)\n-n — Show line numbers (grep -n error logfile)\n\nThese flags combine with pipes for powerful filtering:\n\ndmesg | grep -i nvidia    # Find NVIDIA kernel messages\nsinfo | grep -v down      # Show only non-down nodes",
          "tips": [
            "grep supports regular expressions for advanced patterns",
            "-i is very useful since error messages vary in capitalization"
          ],
          "quiz": {
            "question": "How would you count the number of GPUs showing errors in nvidia-smi output?",
            "options": [
              "nvidia-smi > grep error > wc -l",
              "nvidia-smi | grep -i error | wc -l",
              "count nvidia-smi errors",
              "nvidia-smi --count-errors"
            ],
            "correctIndex": 1,
            "explanation": "Pipe nvidia-smi output through grep to filter for error lines, then through wc -l to count them. The -i flag makes the search case-insensitive."
          }
        },
        {
          "id": "step-4",
          "type": "command",
          "situation": "Time to check the cluster. Are there any idle nodes available?",
          "task": "Use sinfo piped to grep to find idle cluster nodes.",
          "expectedCommands": ["sinfo | grep idle"],
          "hints": [
            "Try: sinfo | grep idle",
            "sinfo shows all nodes, grep filters to idle ones"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "grep"
          }
        },
        {
          "id": "step-5",
          "type": "concept",
          "situation": "Your mentor introduces counting and sorting — essential for triage.",
          "task": "Counting and Sorting",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Two more essential pipe companions:\n\nwc (word count) — count lines, words, or characters:\n-l — Lines\n-w — Words\n-c — Characters/bytes\n\nsort — sort lines of output:\n-n — Numeric sort\n-r — Reverse order\n(none) — Alphabetical\n\nCommon pattern: command | grep pattern | wc -l tells you 'how many matches?'",
          "tips": [
            "wc -l is the quickest way to count results",
            "sort -rn gives you largest-first numeric ordering"
          ]
        },
        {
          "id": "step-6",
          "type": "command",
          "situation": "Your mentor asks: 'How many jobs are currently in the queue?'",
          "task": "Count the number of lines in the job queue output.",
          "expectedCommands": ["squeue | wc -l"],
          "hints": [
            "Try: squeue | wc -l",
            "Each line represents a queued or running job"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "wc"
          }
        },
        {
          "id": "step-7",
          "type": "concept",
          "situation": "Your mentor moves on to permissions — you'll need these when scripts won't execute.",
          "task": "Reading Permissions",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "When you run ls -l, the first column shows permissions:\n\n-rwxr-xr-x 1 root root 12345 Jan 15 09:30 nvidia-smi\n\nBreaking down -rwxr-xr-x:\n- — File type (- = file, d = directory)\nrwx — Owner can read, write, execute\nr-x — Group can read and execute\nr-x — Others can read and execute\n\nThe three permission types:\nr (read) — view file contents or list directory\nw (write) — modify file or add/remove files in directory\nx (execute) — run as a program or enter directory",
          "tips": [
            "Most system binaries are owned by root",
            "Scripts need execute permission to run directly"
          ]
        },
        {
          "id": "step-8",
          "type": "observe",
          "situation": "Your mentor shows you the permissions on nvidia-smi.",
          "task": "Permissions in Practice",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "observeCommand": "ls -l /usr/bin/nvidia-smi",
          "conceptContent": "Notice nvidia-smi is owned by root and has execute permission for all users — that's why any user on the system can run it."
        },
        {
          "id": "step-9",
          "type": "concept",
          "situation": "Your mentor explains how to fix permission issues.",
          "task": "Changing Permissions",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "chmod changes file permissions. Two notation styles:\n\nSymbolic:\nchmod +x script.sh — Add execute for everyone\nchmod u+w file — Add write for owner\nchmod go-w file — Remove write for group and others\n\nNumeric (octal):\n7 = rwx (read + write + execute)\n5 = r-x (read + execute)\n4 = r-- (read only)\n0 = --- (no access)\n\nExample: chmod 755 script.sh = owner rwx, group r-x, others r-x\n\nchown changes ownership:\nchown admin:gpu-users /opt/scripts/gpu-check.sh",
          "tips": [
            "chmod and chown usually require root privileges",
            "755 is the standard permission for executable scripts"
          ],
          "quiz": {
            "question": "A script at /opt/scripts/gpu-check.sh won't execute. Permissions show -rw-r--r--. What fixes this?",
            "options": [
              "chown root gpu-check.sh",
              "chmod +x /opt/scripts/gpu-check.sh",
              "mv gpu-check.sh /usr/bin/",
              "cat gpu-check.sh | bash"
            ],
            "correctIndex": 1,
            "explanation": "The file lacks execute (x) permission. chmod +x adds execute permission, allowing the script to be run directly."
          }
        },
        {
          "id": "step-10",
          "type": "command",
          "situation": "An alert just fired. Your mentor says: 'Quick, check if any GPUs have errors.'",
          "task": "Use nvidia-smi to check GPU status.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "Try: nvidia-smi",
            "Look for any errors or warnings in the output"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU"
          }
        },
        {
          "id": "step-11",
          "type": "command",
          "situation": "Your mentor says: 'Check what sensors are reporting for temperatures.'",
          "task": "Run sensors to see hardware temperature readings.",
          "expectedCommands": ["sensors"],
          "hints": [
            "Type: sensors",
            "Look for temperature readings across components"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Temp"
          }
        }
      ]
    },
    {
      "id": "domain1-midnight-deployment",
      "domain": 1,
      "title": "The Midnight Deployment",
      "narrative": {
        "hook": "New DGX H100 cluster arrives at 11 PM - deployment deadline is 8 AM.",
        "setting": "You're the lead systems engineer responsible for bringing up a 4-node DGX H100 cluster before the AI team's Monday morning deadline.",
        "resolution": "Successfully configure BMC, validate firmware, and bring all GPUs online with proper driver installation."
      },
      "commandFamilies": ["bmc-hardware", "gpu-monitoring", "diagnostics"],
      "estimatedMinutes": 25,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "situation": "The shipping crates are open. Four pristine DGX H100 nodes sit on the rack rails, power cables ready.",
          "task": "Before powering on, use ipmitool to verify BMC connectivity and check the System Event Log for any shipping damage alerts.",
          "expectedCommands": ["ipmitool sel list", "ipmitool sel elist"],
          "hints": [
            "Use ipmitool to access BMC",
            "Check SEL for hardware events",
            "Look for temperature or vibration alerts"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sel|SEL|event"
          }
        },
        {
          "id": "step-2",
          "situation": "BMC responds. SEL shows normal power-off events from factory. Time to verify hardware inventory.",
          "task": "Use dmidecode to check memory configuration and verify all DIMMs are detected.",
          "expectedCommands": ["dmidecode -t memory"],
          "hints": [
            "dmidecode -t memory shows DIMM info",
            "Look for 2TB total system memory",
            "Check all slots are populated"
          ],
          "validation": {
            "type": "command",
            "command": "dmidecode",
            "pattern": "memory|Memory|DIMM"
          },
          "quiz": {
            "question": "Why check dmidecode before GPU validation?",
            "options": [
              "GPU drivers need memory info",
              "Detect shipping damage to DIMMs first",
              "Memory errors can mask GPU issues",
              "BIOS requires memory validation"
            ],
            "correctIndex": 2,
            "explanation": "System memory errors can cause GPU initialization failures and misleading error messages. Validating memory first ensures a clean baseline."
          }
        },
        {
          "id": "step-3",
          "situation": "All 2TB of system memory detected across 32 DIMMs. OS boots cleanly.",
          "task": "Run nvidia-smi to verify all 8 GPUs are detected and check their initial state.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows all GPUs",
            "Look for 8x H100 GPUs",
            "Check for any ERR! or N/A values"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "H100|GPU"
          }
        },
        {
          "id": "step-4",
          "situation": "nvidia-smi shows 8 GPUs but GPU 6 shows 'ERR!' in the temperature column.",
          "task": "Use nvidia-smi -q to get detailed information about GPU 6 and identify the issue.",
          "expectedCommands": ["nvidia-smi -q -i 6"],
          "hints": [
            "nvidia-smi -q -i 6 for detailed info",
            "Check for driver or hardware errors",
            "Look at ECC memory status"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-q|-i|query"
          },
          "quiz": {
            "question": "What does 'ERR!' typically indicate in nvidia-smi output?",
            "options": [
              "Driver not loaded",
              "GPU powered off",
              "Communication failure with GPU",
              "Normal initialization state"
            ],
            "correctIndex": 2,
            "explanation": "ERR! indicates the driver cannot communicate with the GPU properly, often due to PCIe issues, driver problems, or hardware faults."
          }
        },
        {
          "id": "step-5",
          "situation": "Detailed query shows GPU 6 has a pending GPU reset required flag.",
          "task": "Check the sensors output to verify thermal conditions before attempting any GPU reset.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors command shows temperatures",
            "Check ambient and GPU temps",
            "Ensure cooling is working"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Core|GPU"
          }
        },
        {
          "id": "step-6",
          "situation": "Temperatures are normal. The issue appears to be a post-shipping initialization problem.",
          "task": "Use dcgmi diag to run a quick health check on all GPUs before proceeding.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag -r 1 for quick check",
            "Level 1 is fastest diagnostic",
            "Check all GPUs pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|health"
          }
        },
        {
          "id": "step-7",
          "situation": "Diagnostics show GPU 6 needs a reset. Other 7 GPUs pass all tests.",
          "task": "Document the GPU state using nvidia-bug-report before attempting any reset operations.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh creates log bundle",
            "Document before making changes",
            "Save for support ticket if needed"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report|log"
          },
          "quiz": {
            "question": "Why create a bug report before GPU reset?",
            "options": [
              "Required by NVIDIA warranty",
              "Captures pre-reset state for analysis",
              "Automatically fixes issues",
              "Enables remote support access"
            ],
            "correctIndex": 1,
            "explanation": "Creating a bug report captures the complete system state before changes, providing crucial debugging information if the reset doesn't resolve the issue."
          }
        },
        {
          "id": "step-8",
          "situation": "Bug report saved. Time to attempt recovery of GPU 6.",
          "task": "Use nvidia-smi to attempt a GPU reset on GPU 6 and verify recovery.",
          "expectedCommands": ["nvidia-smi -r -i 6"],
          "hints": [
            "nvidia-smi -r resets GPUs",
            "May need to specify GPU index",
            "Verify with nvidia-smi after"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-r|reset"
          }
        },
        {
          "id": "step-9",
          "situation": "GPU 6 recovered after reset. All 8 GPUs now showing healthy status.",
          "task": "Run a comprehensive DCGM diagnostic to validate the cluster is ready for production.",
          "expectedCommands": ["dcgmi diag -r 3"],
          "hints": [
            "dcgmi diag -r 3 for thorough test",
            "Takes longer but more complete",
            "All GPUs should pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 3|level 3"
          }
        },
        {
          "id": "step-10",
          "situation": "All diagnostics pass. The cluster is ready for the AI team's Monday morning deadline.",
          "task": "Final verification: check nvidia-smi to confirm all GPUs are operational and document the deployment.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for final check",
            "All 8 GPUs should show healthy",
            "Note temperatures and memory usage"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Memory"
          }
        }
      ]
    },
    {
      "id": "domain1-firmware-emergency",
      "domain": 1,
      "title": "The Firmware Emergency",
      "narrative": {
        "hook": "Security bulletin requires immediate firmware updates across 16 production nodes.",
        "setting": "A critical security vulnerability has been disclosed in the BMC firmware. You have a 4-hour maintenance window to update all systems.",
        "resolution": "Successfully update BMC firmware, verify GPU firmware compatibility, and bring nodes back online."
      },
      "commandFamilies": [
        "bmc-hardware",
        "gpu-monitoring",
        "diagnostics",
        "cluster-tools"
      ],
      "estimatedMinutes": 20,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "situation": "The security team has flagged 16 nodes for immediate BMC firmware update. Downtime approved.",
          "task": "Use sinfo to check which nodes are currently running jobs before starting updates.",
          "expectedCommands": ["sinfo"],
          "hints": [
            "sinfo shows node states",
            "Check for allocated vs idle nodes",
            "Identify nodes to drain first"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "STATE|alloc|idle"
          }
        },
        {
          "id": "step-2",
          "situation": "8 nodes are running jobs. You need to gracefully drain them for maintenance.",
          "task": "Use scontrol to drain nodes with a maintenance reason before starting firmware updates.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='BMC firmware update'"
          ],
          "hints": [
            "scontrol update nodename state=drain",
            "Add reason for audit trail",
            "Wait for jobs to complete"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|update|state"
          },
          "quiz": {
            "question": "Why drain nodes instead of immediately forcing them offline?",
            "options": [
              "Slurm requires it",
              "Preserves running job data",
              "Faster than canceling jobs",
              "Required for firmware updates"
            ],
            "correctIndex": 1,
            "explanation": "Draining allows running jobs to complete gracefully, preserving computation results and avoiding data loss from interrupted training runs."
          }
        },
        {
          "id": "step-3",
          "situation": "Nodes are draining. While waiting, verify current BMC firmware versions.",
          "task": "Use ipmitool to check the current BMC firmware version on the first node.",
          "expectedCommands": ["ipmitool mc info"],
          "hints": [
            "ipmitool mc info shows BMC version",
            "Document current version",
            "Compare to security bulletin"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "mc info|version|firmware"
          }
        },
        {
          "id": "step-4",
          "situation": "Current BMC version confirmed as vulnerable. Jobs have completed, nodes fully drained.",
          "task": "Before firmware update, capture the current BMC sensor baselines using ipmitool.",
          "expectedCommands": ["ipmitool sensor list"],
          "hints": [
            "ipmitool sensor list shows all sensors",
            "Document normal values",
            "Useful for post-update comparison"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sensor"
          }
        },
        {
          "id": "step-5",
          "situation": "Sensor baselines captured. Time to verify GPU state before BMC update.",
          "task": "Run nvidia-smi to document GPU state before the BMC firmware change.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi captures GPU state",
            "Note driver version",
            "Check for any existing issues"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Driver|GPU"
          },
          "quiz": {
            "question": "Why document GPU state before BMC updates?",
            "options": [
              "BMC controls GPU power",
              "BMC firmware can affect GPU detection",
              "NVIDIA requires it",
              "GPU drivers depend on BMC"
            ],
            "correctIndex": 1,
            "explanation": "BMC firmware changes can affect PCIe enumeration and power management, potentially impacting GPU detection. Documenting beforehand helps identify any post-update issues."
          }
        },
        {
          "id": "step-6",
          "situation": "GPU state documented. Ready to proceed with BMC firmware update.",
          "task": "Check BMC event log for any hardware warnings before rebooting for firmware update.",
          "expectedCommands": ["ipmitool sel elist"],
          "hints": [
            "ipmitool sel elist shows events",
            "Look for recent warnings",
            "Clear log after review"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sel|event"
          }
        },
        {
          "id": "step-7",
          "situation": "Event log clean. Firmware update completed via BMC web interface. Node rebooting.",
          "task": "After reboot, verify the new BMC firmware version is installed correctly.",
          "expectedCommands": ["ipmitool mc info"],
          "hints": [
            "ipmitool mc info shows version",
            "Verify against security bulletin",
            "Document new version"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "mc info|version"
          }
        },
        {
          "id": "step-8",
          "situation": "BMC firmware updated successfully. Now verify GPU subsystem survived the reboot.",
          "task": "Run nvidia-smi to verify all GPUs are detected after the BMC update and reboot.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU status",
            "Compare to pre-update state",
            "Check for any new errors"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Driver"
          },
          "quiz": {
            "question": "What should you verify first after firmware update and reboot?",
            "options": [
              "Network connectivity",
              "GPU count matches pre-update",
              "Slurm service status",
              "User home directories"
            ],
            "correctIndex": 1,
            "explanation": "Verifying GPU detection confirms the PCIe subsystem is working correctly after BMC firmware changes, which is critical for HPC/AI workloads."
          }
        },
        {
          "id": "step-9",
          "situation": "All GPUs detected. Run quick diagnostics to verify GPU health post-update.",
          "task": "Use dcgmi diag to run a health check on all GPUs after the firmware update.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag -r 1 for quick check",
            "Verify all tests pass",
            "Document any warnings"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        },
        {
          "id": "step-10",
          "situation": "Diagnostics pass. Node ready to return to production.",
          "task": "Use scontrol to return the node to service and verify its state.",
          "expectedCommands": ["scontrol update nodename=dgx-01 state=resume"],
          "hints": [
            "scontrol update state=resume",
            "Verify with sinfo",
            "Node should show idle"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "resume|idle|state"
          }
        }
      ]
    },
    {
      "id": "domain1-driver-disaster",
      "domain": 1,
      "title": "The Driver Disaster",
      "narrative": {
        "hook": "Driver update breaks GPU communication on 8 production nodes during business hours.",
        "setting": "A well-intentioned driver update has left half your cluster unable to see GPUs. Users are impacted and management is watching.",
        "resolution": "Diagnose the driver mismatch, perform controlled rollback, and restore GPU functionality."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "diagnostics",
        "bmc-hardware",
        "cluster-tools"
      ],
      "estimatedMinutes": 22,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "Users report 'CUDA error: no CUDA-capable device' on dgx-01 through dgx-08. Panic ensues.",
          "task": "Start triage by checking nvidia-smi on an affected node to understand the GPU state.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi is the first check",
            "Look for error messages",
            "Check if driver is loaded"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|ERR|failed"
          }
        },
        {
          "id": "step-2",
          "situation": "nvidia-smi shows 'NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.'",
          "task": "Check if the NVIDIA kernel modules are loaded using system commands.",
          "expectedCommands": ["lsmod | grep nvidia"],
          "hints": [
            "lsmod shows loaded modules",
            "grep for nvidia modules",
            "Should see nvidia, nvidia_uvm, etc."
          ],
          "validation": {
            "type": "command",
            "command": "lsmod",
            "pattern": "nvidia"
          },
          "quiz": {
            "question": "What does 'couldn't communicate with NVIDIA driver' typically mean?",
            "options": [
              "GPUs are physically disconnected",
              "Kernel module not loaded or crashed",
              "CUDA toolkit not installed",
              "Insufficient permissions"
            ],
            "correctIndex": 1,
            "explanation": "This error indicates the nvidia kernel module is either not loaded, failed to load, or has crashed. The user-space nvidia-smi can't communicate with kernel-space driver."
          }
        },
        {
          "id": "step-3",
          "situation": "lsmod shows nvidia module is loaded but version 550.54 doesn't match expected 535.154.",
          "task": "Check system logs for driver initialization errors using journalctl or dmesg.",
          "expectedCommands": ["dmesg | grep -i nvrm"],
          "hints": [
            "dmesg shows kernel messages",
            "Look for NVRM errors",
            "Check module load time"
          ],
          "validation": {
            "type": "command",
            "command": "dmesg",
            "pattern": "NVRM|nvidia|GPU"
          }
        },
        {
          "id": "step-4",
          "situation": "dmesg shows NVRM: GPU firmware mismatch - driver expects newer GSP firmware.",
          "task": "Document the current driver and CUDA versions for the rollback plan.",
          "expectedCommands": [
            "cat /proc/driver/nvidia/version",
            "nvcc --version"
          ],
          "hints": [
            "cat /proc/driver/nvidia/version",
            "nvcc --version for CUDA",
            "Document both versions"
          ],
          "validation": {
            "type": "command",
            "pattern": "nvidia/version|nvcc"
          }
        },
        {
          "id": "step-5",
          "situation": "Driver 550.54 requires GSP firmware update, but cluster policy prohibits firmware changes without approval.",
          "task": "Check what driver packages are available for rollback using package manager.",
          "expectedCommands": ["apt list --installed nvidia-driver*"],
          "hints": [
            "apt list or dnf list for packages",
            "Look for older nvidia-driver versions",
            "535.154 was the previous version"
          ],
          "validation": {
            "type": "command",
            "pattern": "nvidia|driver|list"
          },
          "quiz": {
            "question": "Why might a newer driver require firmware updates?",
            "options": [
              "Marketing requirements",
              "New features need GSP support",
              "Licensing changes",
              "Bug fixes only"
            ],
            "correctIndex": 1,
            "explanation": "Newer drivers may require updated GPU System Processor (GSP) firmware to support new features, security patches, or architectural changes in driver-GPU communication."
          }
        },
        {
          "id": "step-6",
          "situation": "Previous driver package 535.154 is still cached in the repository. Rollback is possible.",
          "task": "Use scontrol to drain the affected nodes before starting the driver rollback.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='driver rollback'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Add maintenance reason",
            "Prevent new jobs from starting"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|state"
          }
        },
        {
          "id": "step-7",
          "situation": "Nodes draining. Time to document the current state before making changes.",
          "task": "Create a bug report to capture system state before the rollback.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh creates bundle",
            "Useful for post-incident review",
            "Document before changing"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report"
          }
        },
        {
          "id": "step-8",
          "situation": "Bug report saved. Driver rollback completed via package manager. Reboot required.",
          "task": "After reboot, verify nvidia-smi now communicates with the GPUs successfully.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi should work now",
            "Check all 8 GPUs visible",
            "Verify driver version 535.154"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "535|GPU"
          },
          "quiz": {
            "question": "After a driver rollback, what should you verify first?",
            "options": [
              "User applications work",
              "All GPUs are detected",
              "Network connectivity",
              "Slurm configuration"
            ],
            "correctIndex": 1,
            "explanation": "Verifying GPU detection confirms the driver rollback was successful and the driver-firmware compatibility is restored before testing higher-level functionality."
          }
        },
        {
          "id": "step-9",
          "situation": "nvidia-smi shows all 8 GPUs with driver 535.154. Basic communication restored.",
          "task": "Run DCGM diagnostics to verify GPU health after the driver change.",
          "expectedCommands": ["dcgmi diag -r 2"],
          "hints": [
            "dcgmi diag -r 2 for moderate test",
            "Verify compute and memory",
            "All GPUs should pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        },
        {
          "id": "step-10",
          "situation": "All diagnostics pass. Nodes ready to return to production.",
          "task": "Resume the nodes in Slurm and verify they return to idle state.",
          "expectedCommands": ["scontrol update nodename=dgx-01 state=resume"],
          "hints": [
            "scontrol update state=resume",
            "sinfo to verify state",
            "Nodes should show idle"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "resume"
          }
        }
      ]
    },
    {
      "id": "domain1-rack-expansion",
      "domain": 1,
      "title": "The Rack Expansion",
      "narrative": {
        "hook": "Four new DGX nodes arrive to expand the training cluster capacity by 50%.",
        "setting": "Budget approved, hardware delivered. Your job is to integrate 4 new DGX A100 nodes into the existing 8-node cluster seamlessly.",
        "resolution": "Configure BMC networking, validate GPU health, and integrate nodes into Slurm."
      },
      "commandFamilies": [
        "bmc-hardware",
        "gpu-monitoring",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 28,
      "difficulty": "beginner",
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Before touching any hardware, your senior engineer briefs you on the BMC — the key to out-of-band management.",
          "task": "Understanding BMC and IPMI",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "IPMI (Intelligent Platform Management Interface) is a standardized interface for hardware management. The BMC (Baseboard Management Controller) is a specialized processor implementing IPMI.\n\nBMC provides:\n- Out-of-band management (works even when OS is down)\n- Hardware sensor monitoring (temperature, voltage, fan speeds)\n- System event logging (SEL)\n- Remote power control\n- Serial-over-LAN console access\n\nKey ipmitool subcommands:\n- sensor — read hardware sensors\n- sel — System Event Log\n- power — power control\n- chassis — chassis status",
          "tips": [
            "BMC works even when the server OS is crashed — that's 'out-of-band'",
            "Always check the SEL first when integrating new hardware"
          ]
        },
        {
          "id": "step-2",
          "type": "observe",
          "situation": "The first new DGX server has arrived and been racked. Your mentor wants to verify it powered on correctly before proceeding with full integration.",
          "task": "Observe how your mentor uses the BMC to remotely check the server power status without physically accessing the machine.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "observeCommand": "ipmitool power status"
        },
        {
          "id": "step-3",
          "situation": "Four new DGX A100 nodes are racked, cabled, and powered on. BMC LEDs show network activity.",
          "task": "Use ipmitool to verify BMC connectivity and check system power state.",
          "expectedCommands": ["ipmitool chassis status"],
          "hints": [
            "ipmitool chassis status",
            "Verify power is on",
            "Check BMC is responsive"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "chassis|power|status"
          }
        },
        {
          "id": "step-4",
          "situation": "BMC responds. System is powered on. Time to verify hardware inventory.",
          "task": "Use dmidecode to verify the system model and memory configuration.",
          "expectedCommands": ["dmidecode -t system", "dmidecode -t memory"],
          "hints": [
            "dmidecode -t system for model",
            "dmidecode -t memory for RAM",
            "DGX A100 should have 1TB or 2TB"
          ],
          "validation": {
            "type": "command",
            "command": "dmidecode",
            "pattern": "system|memory|DGX"
          },
          "quiz": {
            "question": "Why verify hardware specs on new nodes before integration?",
            "options": [
              "Warranty requirements",
              "Ensure config matches cluster standards",
              "NVIDIA licensing",
              "Slurm requires it"
            ],
            "correctIndex": 1,
            "explanation": "Verifying hardware configuration ensures the new nodes match cluster standards for memory, storage, and network, preventing heterogeneous cluster issues."
          }
        },
        {
          "id": "step-5",
          "situation": "Hardware matches spec: DGX A100 with 1TB RAM, 8 GPUs expected.",
          "task": "Run nvidia-smi to verify all 8 A100 GPUs are detected.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU inventory",
            "Should see 8 A100 GPUs",
            "Check memory: 40GB or 80GB per GPU"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "A100|GPU"
          }
        },
        {
          "id": "step-6",
          "situation": "All 8 GPUs detected: A100-SXM4-80GB. Time for health validation.",
          "task": "Check GPU topology to verify NVLink connectivity between GPUs.",
          "expectedCommands": ["nvidia-smi topo -m"],
          "hints": [
            "nvidia-smi topo -m shows topology",
            "All GPUs should connect via NVLink",
            "NV12 for A100 systems"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "topo"
          },
          "quiz": {
            "question": "What does 'NV12' in topology output indicate?",
            "options": [
              "12 NVLink connections",
              "NVLink version 1.2",
              "12th generation GPU",
              "PCIe gen 4 x12"
            ],
            "correctIndex": 0,
            "explanation": "NV12 indicates 12 NVLink connections between the GPU pair, providing high-bandwidth communication for multi-GPU workloads like distributed training."
          }
        },
        {
          "id": "step-7",
          "situation": "NVLink topology looks correct. Run diagnostics to validate GPU health.",
          "task": "Use dcgmi diag to run a comprehensive health check on all GPUs.",
          "expectedCommands": ["dcgmi diag -r 3"],
          "hints": [
            "dcgmi diag -r 3 for full test",
            "Takes 10-15 minutes",
            "All tests should pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r"
          }
        },
        {
          "id": "step-8",
          "situation": "All 8 GPUs pass diagnostics. Time to verify thermal performance.",
          "task": "Check system thermal sensors to ensure cooling is adequate.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors shows temperatures",
            "GPU temps should be under 45C at idle",
            "Check CPU and system temps too"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Core|GPU"
          }
        },
        {
          "id": "step-9",
          "situation": "Thermals look good. Now verify the node can join the Slurm cluster.",
          "task": "Check sinfo to see the current cluster configuration before adding new nodes.",
          "expectedCommands": ["sinfo"],
          "hints": [
            "sinfo shows all nodes",
            "Note current node count",
            "Identify partition structure"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "PARTITION|NODE"
          }
        },
        {
          "id": "step-10",
          "situation": "Current cluster has 8 nodes in 'gpu' partition. New nodes need to be added.",
          "task": "Verify the new node's hostname and IP are configured correctly for Slurm.",
          "expectedCommands": ["hostname"],
          "hints": [
            "hostname command shows name",
            "Should follow naming convention",
            "Check /etc/hosts or DNS"
          ],
          "validation": {
            "type": "command",
            "pattern": "hostname|host"
          },
          "quiz": {
            "question": "Why is consistent hostname naming important for Slurm?",
            "options": [
              "User convenience only",
              "Slurm uses hostnames for job scheduling",
              "Required by NVIDIA",
              "Network performance"
            ],
            "correctIndex": 1,
            "explanation": "Slurm relies on consistent hostname patterns for job scheduling, node selection, and resource management. Inconsistent naming causes scheduling failures."
          }
        },
        {
          "id": "step-11",
          "situation": "Hostname dgx-09 configured correctly. Slurm client configured.",
          "task": "Use scontrol to add the new node to the cluster and verify its state.",
          "expectedCommands": ["scontrol show node dgx-09"],
          "hints": [
            "scontrol update may be needed",
            "Admin may need to update slurm.conf",
            "Check node state after"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "node|update|show"
          },
          "autoFaults": [
            { "nodeId": "dgx-08", "type": "add-node", "severity": "warning" },
            { "nodeId": "dgx-09", "type": "add-node", "severity": "warning" },
            { "nodeId": "dgx-10", "type": "add-node", "severity": "warning" },
            { "nodeId": "dgx-11", "type": "add-node", "severity": "warning" }
          ]
        },
        {
          "id": "step-12",
          "situation": "New nodes added to Slurm. Final validation required.",
          "task": "Use sinfo to verify all 12 nodes (8 original + 4 new) are visible and idle.",
          "expectedCommands": ["sinfo -N -l"],
          "hints": [
            "sinfo -N -l for detailed view",
            "All nodes should show idle",
            "GPU count should be correct"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "idle|gpu"
          }
        }
      ]
    },
    {
      "id": "domain2-nvlink-mystery",
      "domain": 2,
      "title": "The NVLink Mystery",
      "narrative": {
        "hook": "Multi-GPU training suddenly drops from 8-GPU scaling to 4-GPU performance.",
        "setting": "Users report their 8-GPU jobs are running at half the expected speed. Network team says fabric is fine.",
        "resolution": "Discover degraded NVLink connections between GPU pairs and identify failing NVSwitch."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "diagnostics",
        "infiniband-tools",
        "bmc-hardware"
      ],
      "estimatedMinutes": 25,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "User reports 8-GPU PyTorch training is 2x slower than last week. Same code, same data.",
          "task": "Start by checking nvidia-smi to verify all 8 GPUs are detected and healthy.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU status",
            "Check for errors or warnings",
            "Verify memory usage"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Memory"
          }
        },
        {
          "id": "step-2",
          "situation": "All 8 GPUs show healthy, but you notice GPU utilization varies widely during training.",
          "task": "Check the GPU topology to understand the NVLink connectivity matrix.",
          "expectedCommands": ["nvidia-smi topo -m"],
          "hints": [
            "nvidia-smi topo -m shows connections",
            "Look for NVLink vs PCIe paths",
            "NV should appear between all GPUs"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "topo"
          },
          "quiz": {
            "question": "In topology output, what does 'PIX' between two GPUs indicate?",
            "options": [
              "Pixel shader connection",
              "PCIe switch connection",
              "Physical inspection required",
              "Performance index rating"
            ],
            "correctIndex": 1,
            "explanation": "PIX indicates GPUs are connected through a PCIe switch rather than NVLink, resulting in significantly lower bandwidth for GPU-to-GPU communication."
          }
        },
        {
          "id": "step-3",
          "situation": "Topology shows GPUs 0-3 and 4-7 are connected via NVLink, but 0-4, 1-5, etc. show 'SYS' (PCIe)!",
          "task": "Check NVLink status in detail to identify which links are degraded.",
          "expectedCommands": ["nvidia-smi nvlink --status"],
          "hints": [
            "nvidia-smi nvlink --status",
            "Check all link states",
            "Look for inactive links"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "nvlink|status"
          }
        },
        {
          "id": "step-4",
          "situation": "NVLink status shows links through NVSwitch 2 are all inactive. The switch may be failing.",
          "task": "Check NVLink error counters to see if there are physical layer errors.",
          "expectedCommands": ["nvidia-smi nvlink -e"],
          "hints": [
            "nvidia-smi nvlink -e shows errors",
            "Look for CRC or replay errors",
            "High counts indicate problems"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "nvlink|-e"
          },
          "quiz": {
            "question": "What do high NVLink replay counts typically indicate?",
            "options": [
              "Normal operation",
              "Signal integrity issues",
              "Driver bugs",
              "Memory errors"
            ],
            "correctIndex": 1,
            "explanation": "High replay counts indicate signal integrity problems causing packet retransmissions, often due to cable issues, connector problems, or failing NVSwitch components."
          }
        },
        {
          "id": "step-5",
          "situation": "Error counters show CRC errors on all links going through NVSwitch 2.",
          "task": "Use nvsm to check NVSwitch health status directly.",
          "expectedCommands": ["nvsm show nvswitch"],
          "hints": [
            "nvsm show nvswitch",
            "Check health status",
            "Look for degraded switches"
          ],
          "validation": {
            "type": "command",
            "command": "nvsm",
            "pattern": "nvswitch|health"
          }
        },
        {
          "id": "step-6",
          "situation": "nvsm shows NVSwitch 2 in 'Degraded' state with thermal warning.",
          "task": "Check system thermal sensors to understand the temperature situation.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors shows temperatures",
            "Look for NVSwitch temps",
            "Compare to healthy switches"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|NVSwitch"
          }
        },
        {
          "id": "step-7",
          "situation": "NVSwitch 2 is running 15C hotter than others. Possible cooling issue.",
          "task": "Check BMC sensor data for fan speeds and cooling system status.",
          "expectedCommands": ["ipmitool sensor list"],
          "hints": [
            "ipmitool sensor list",
            "Look for fan RPM",
            "Check cooling zone status"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sensor|fan"
          },
          "quiz": {
            "question": "Why would one NVSwitch run significantly hotter than others?",
            "options": [
              "Higher utilization",
              "Blocked airflow or fan failure",
              "Manufacturing variance",
              "Software bug"
            ],
            "correctIndex": 1,
            "explanation": "Significant temperature differences between identical components usually indicate airflow obstruction, failed fans, or thermal paste degradation rather than utilization differences."
          }
        },
        {
          "id": "step-8",
          "situation": "Fan 4 (cooling NVSwitch 2 zone) is running at 50% of normal RPM. Bearing failure suspected.",
          "task": "Document the issue using nvidia-bug-report before scheduling maintenance.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh captures state",
            "Include thermal data",
            "Needed for RMA"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report"
          }
        },
        {
          "id": "step-9",
          "situation": "Bug report captured. Need to take node offline for fan replacement.",
          "task": "Use scontrol to drain the node for maintenance.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='fan replacement'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Add reason for records",
            "Jobs will complete first"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain"
          }
        },
        {
          "id": "step-10",
          "situation": "Node draining. Document final GPU state before shutdown.",
          "task": "Run dcgmi diag to document current GPU health for comparison after repair.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag -r 1 quick check",
            "Document any failures",
            "Compare after repair"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        }
      ]
    },
    {
      "id": "domain2-pcie-puzzle",
      "domain": 2,
      "title": "The PCIe Puzzle",
      "narrative": {
        "hook": "Intermittent 'GPU fell off the bus' errors appearing in system logs.",
        "setting": "Production node showing random GPU disconnections. Users complain of jobs crashing unpredictably.",
        "resolution": "Trace PCIe link training failures to marginal power delivery and thermal cycling issues."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "bmc-hardware",
        "diagnostics",
        "cluster-tools"
      ],
      "estimatedMinutes": 23,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "System logs show 'NVRM: GPU at PCI:0000:41:00.0 has fallen off the bus' errors, seemingly random.",
          "task": "Check nvidia-smi to see current GPU state and identify which GPU is affected.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU status",
            "Look for ERR! or missing GPUs",
            "Note PCI address mapping"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|ERR"
          }
        },
        {
          "id": "step-2",
          "situation": "nvidia-smi shows 7 GPUs instead of 8. GPU 3 (PCI 41:00.0) is missing.",
          "task": "Check system logs for the exact error messages and timestamps.",
          "expectedCommands": ["dmesg | grep -i nvrm"],
          "hints": [
            "dmesg shows kernel messages",
            "grep for NVRM or nvidia",
            "Note when errors started"
          ],
          "validation": {
            "type": "command",
            "command": "dmesg",
            "pattern": "NVRM|nvidia|GPU|pci"
          },
          "quiz": {
            "question": "What does 'GPU fell off the bus' typically indicate?",
            "options": [
              "Driver crash only",
              "PCIe link failure or GPU reset",
              "Power cable disconnected",
              "Thermal shutdown"
            ],
            "correctIndex": 1,
            "explanation": "This error indicates the PCIe link between CPU and GPU has failed, either due to hardware issues, power problems, or the GPU entering a non-recoverable state."
          }
        },
        {
          "id": "step-3",
          "situation": "Logs show the error occurs during high-power training phases. Possible power issue.",
          "task": "Check BMC power sensor readings to understand power delivery health.",
          "expectedCommands": ["ipmitool sensor list"],
          "hints": [
            "ipmitool sensor list",
            "Look for power readings",
            "Check voltage levels"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sensor|power|volt"
          }
        },
        {
          "id": "step-4",
          "situation": "Power readings show PSU 2 output voltage is 0.3V below nominal during peaks.",
          "task": "Check the BMC System Event Log for any power-related warnings.",
          "expectedCommands": ["ipmitool sel elist"],
          "hints": [
            "ipmitool sel elist",
            "Look for power events",
            "Check timestamps"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sel|event"
          },
          "quiz": {
            "question": "Why might marginal power cause PCIe link failures?",
            "options": [
              "PCIe is power-hungry",
              "GPU PCIe PHY needs stable voltage",
              "Software power management bug",
              "Cable length too long"
            ],
            "correctIndex": 1,
            "explanation": "The PCIe physical layer (PHY) requires stable voltage for reliable high-speed signaling. Voltage dips during load can cause link training failures or bit errors."
          }
        },
        {
          "id": "step-5",
          "situation": "SEL shows multiple 'PSU 2 Assert' events correlating with GPU dropout times.",
          "task": "Document PSU configuration and check for redundancy status.",
          "expectedCommands": ["ipmitool fru print"],
          "hints": [
            "ipmitool fru print for PSU info",
            "Check redundancy status",
            "Document serial numbers"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "fru|psu"
          }
        },
        {
          "id": "step-6",
          "situation": "PSU 2 FRU shows it's an older unit - possible degradation. Need thermal check.",
          "task": "Check system temperatures to rule out thermal contribution to the issue.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors shows temperatures",
            "Check all zones",
            "Compare to spec limits"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Core"
          }
        },
        {
          "id": "step-7",
          "situation": "Temperatures normal. Issue isolated to PSU 2. Need to schedule replacement.",
          "task": "Create bug report documenting the power and GPU dropout correlation.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh",
            "Document power events",
            "Include timestamps"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report"
          }
        },
        {
          "id": "step-8",
          "situation": "Bug report created. The GPU is still missing. Attempt recovery.",
          "task": "Try to reset the GPUs to see if the missing one recovers.",
          "expectedCommands": ["nvidia-smi -r"],
          "hints": [
            "nvidia-smi -r for reset",
            "May need reboot if fails",
            "Check if GPU 3 returns"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-r|reset"
          },
          "quiz": {
            "question": "When should you attempt a GPU reset vs. a full reboot?",
            "options": [
              "Always try reset first",
              "Reset for driver issues, reboot for PCIe",
              "Always reboot for reliability",
              "Based on error message only"
            ],
            "correctIndex": 1,
            "explanation": "GPU resets work for driver-level issues, but PCIe link failures often require a full system reboot to re-enumerate the PCI bus and retrain links."
          }
        },
        {
          "id": "step-9",
          "situation": "Reset didn't recover GPU 3. Full reboot required, but must drain first.",
          "task": "Drain the node from Slurm before scheduling the reboot.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='PSU replacement'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Add reason for maintenance",
            "Wait for jobs to complete"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain"
          }
        },
        {
          "id": "step-10",
          "situation": "Node drained. After reboot, verify all 8 GPUs are back.",
          "task": "Run nvidia-smi after reboot to verify all GPUs are detected.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi after reboot",
            "Should show all 8 GPUs",
            "Schedule PSU replacement"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "8|GPU"
          }
        }
      ]
    },
    {
      "id": "domain2-fiber-optic-fault",
      "domain": 2,
      "title": "The Fiber Optic Fault",
      "narrative": {
        "hook": "A newly racked DGX node's InfiniBand ports refuse to train at full NDR 400 Gb/s speed.",
        "setting": "The datacenter team just installed a new DGX H100 node. Cabling is complete, but the fabric manager reports degraded link widths on two ports.",
        "resolution": "You traced the issue to a faulty QSFP-DD transceiver with dirty fiber connectors, confirmed via mlxlink eye diagram analysis and mlxcables diagnostics. After reseating and cleaning the optics, all ports trained at full 4x NDR speed."
      },
      "commandFamilies": ["infiniband-tools", "gpu-monitoring", "bmc-hardware"],
      "estimatedMinutes": 15,
      "difficulty": "beginner",
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Before diagnosing the link issue, your lead engineer gives you a quick primer on InfiniBand in the datacenter.",
          "task": "InfiniBand and NDR Networking",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "InfiniBand is a high-bandwidth, low-latency network fabric used in GPU clusters for inter-node communication.\n\nDGX H100 uses NDR (Next Data Rate) InfiniBand:\n- 400 Gb/s per port (4x lanes at 100 Gb/s each)\n- Sub-microsecond latency\n- RDMA (Remote Direct Memory Access) for GPU-to-GPU transfers\n\nKey concepts:\n- Link Width: Number of lanes (1x, 4x). Full speed requires 4x.\n- Link Rate: Speed per lane. NDR = 100 Gb/s per lane.\n- HCA: Host Channel Adapter — the InfiniBand NIC in each node.\n- Transceiver: QSFP-DD module that converts electrical signals to optical.\n\nDegraded link width (1x instead of 4x) usually points to a cable or transceiver issue.",
          "tips": [
            "ibstat is the first command to check port status and link speed",
            "NDR at 1x width runs at only 100 Gb/s instead of 400 Gb/s — a 75% bandwidth loss"
          ]
        },
        {
          "id": "step-2",
          "type": "concept",
          "situation": "Before checking port status, your lead engineer explains the two state indicators that InfiniBand ports report and what each combination means.",
          "task": "Understand the difference between Physical State and Logical State on InfiniBand ports, and learn which combinations indicate healthy vs. problematic links.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "InfiniBand ports have two state indicators: Physical State and Logical State. Physical State: LinkUp means cable is connected and signal is good. Polling means the port is trying to establish a link. Disabled means the port is administratively disabled. Logical State: Active means the port is fully operational. Init means the Subnet Manager hasn't configured it yet. Down means the port is not functional. On DGX systems, each GPU has an associated IB port — always check both states when diagnosing connectivity issues.",
          "tips": [
            "ibstat shows both physical and logical state",
            "An Init port often means the Subnet Manager is not running or hasn't reached this port"
          ]
        },
        {
          "id": "step-3",
          "situation": "The new DGX H100 node has been racked and cabled. The network team says two InfiniBand ports are showing reduced bandwidth.",
          "task": "Start by checking the InfiniBand HCA port status to see which ports are active and their link rates.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows HCA port status",
            "Look at the 'Rate' field for each port",
            "NDR should show 400 Gb/s"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Rate"
          }
        },
        {
          "id": "step-4",
          "situation": "ibstat shows ports 1 and 2 are Active but at only 100 Gb/s instead of 400 Gb/s. The ports are training at reduced width (1x instead of 4x).",
          "task": "Check the fabric-wide link information to see how these ports appear to the rest of the network.",
          "expectedCommands": ["iblinkinfo"],
          "hints": [
            "iblinkinfo shows all fabric links",
            "Look for links with reduced width",
            "4x means full width, 1x means degraded"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "link|width"
          },
          "quiz": {
            "question": "What does a link training at 1x width instead of 4x typically indicate?",
            "options": [
              "Normal speed negotiation",
              "A physical cable or transceiver problem",
              "Software driver misconfiguration",
              "Insufficient power to the HCA"
            ],
            "correctIndex": 1,
            "explanation": "Reduced link width (1x instead of 4x) almost always points to a physical layer issue — damaged fiber lanes, dirty connectors, or a failing transceiver. The link trains on whatever lanes still work."
          }
        },
        {
          "id": "step-5",
          "situation": "iblinkinfo confirms two links on the new node are at 1x width. All other nodes are at 4x. The problem is isolated to this node's ports.",
          "task": "Use mlxcables to inspect the transceiver module information on the affected ports.",
          "expectedCommands": ["mlxcables"],
          "hints": [
            "mlxcables shows cable and transceiver details",
            "Check the transceiver type and status",
            "Look for temperature warnings or faults"
          ],
          "validation": {
            "type": "command",
            "command": "mlxcables",
            "pattern": "cable|transceiver|QSFP"
          }
        },
        {
          "id": "step-6",
          "situation": "mlxcables shows the QSFP-DD transceivers are installed, but one reports elevated temperature and the other shows a 'high bit error rate' flag.",
          "task": "Run mlxlink to check the physical signal quality and eye diagram metrics on the degraded port.",
          "expectedCommands": ["mlxlink -d mlx5_0 -p 1 -c"],
          "hints": [
            "mlxlink -d <device> -p <port> shows link details",
            "Add -c for cable/eye info",
            "Look at signal-to-noise ratio"
          ],
          "validation": {
            "type": "command",
            "command": "mlxlink",
            "pattern": "eye|signal|cable"
          },
          "quiz": {
            "question": "What does an 'eye diagram' measure in a high-speed link?",
            "options": [
              "Power consumption over time",
              "Signal quality and timing margins",
              "Number of connected devices",
              "Cable length in meters"
            ],
            "correctIndex": 1,
            "explanation": "Eye diagrams show signal quality by overlaying bit transitions. A 'wide open' eye means clean signals with good timing margins. A 'closed' eye means poor signal integrity — the receiver can't distinguish 0s from 1s reliably."
          }
        },
        {
          "id": "step-7",
          "situation": "mlxlink shows the eye height is critically low on lanes 2 and 3, indicating poor signal quality on those fiber strands.",
          "task": "Check the PCIe connectivity to make sure the HCA itself is healthy and properly seated.",
          "expectedCommands": ["lspci | grep Mellanox"],
          "hints": [
            "lspci lists PCI devices",
            "Grep for Mellanox or ConnectX",
            "Verify link speed and width"
          ],
          "validation": {
            "type": "command",
            "command": "lspci",
            "pattern": "Mellanox|ConnectX"
          }
        },
        {
          "id": "step-8",
          "situation": "lspci confirms the ConnectX-7 HCA is properly seated at PCIe Gen5 x16. The HCA hardware is fine — the problem is definitely in the optics or cabling.",
          "task": "Check the BMC sensors to see if the node's ambient temperature could be affecting the transceivers.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors shows thermal data",
            "Check ambient temperature",
            "Transceivers are sensitive to heat"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp"
          }
        },
        {
          "id": "step-9",
          "situation": "Temperatures are within normal range. The issue is isolated to the transceiver optics. The fiber connectors likely got contaminated during installation.",
          "task": "After cleaning the fiber connectors and reseating the transceivers, verify the ports have come up at full speed.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat to re-check port rates",
            "Should now show 400 Gb/s",
            "Both ports should be 4x width"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "Rate|400"
          }
        },
        {
          "id": "step-10",
          "situation": "Both ports now show 400 Gb/s at full 4x width. The fiber cleaning resolved the issue.",
          "task": "Run a final iblinkinfo to confirm the node is fully integrated into the fabric at full bandwidth.",
          "expectedCommands": ["iblinkinfo"],
          "hints": [
            "iblinkinfo for fabric-wide view",
            "All links should show 4x",
            "No degraded links remaining"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "4x|link"
          },
          "quiz": {
            "question": "What is the most common cause of degraded InfiniBand link width on newly installed nodes?",
            "options": [
              "Firmware mismatch between switches",
              "Dirty or damaged fiber connectors",
              "Incorrect subnet manager configuration",
              "Insufficient PCIe bandwidth"
            ],
            "correctIndex": 1,
            "explanation": "Contaminated fiber connectors are the #1 cause of link issues in new installations. Even microscopic dust particles can block enough light to degrade individual lanes, causing the link to train at reduced width."
          }
        }
      ]
    },
    {
      "id": "domain2-fabric-health-check",
      "domain": 2,
      "title": "The Fabric Health Check",
      "narrative": {
        "hook": "A brand new 8-node GPU pod needs physical layer sign-off before going into production.",
        "setting": "You are the validation engineer responsible for certifying that the InfiniBand fabric meets NVIDIA's specifications before handing the pod over to the operations team.",
        "resolution": "You completed a comprehensive physical layer audit using ibdiagnet, perfquery, and mlxlink. You caught two marginal links with elevated symbol error rates and one misconfigured port speed, preventing production issues before they started."
      },
      "commandFamilies": [
        "infiniband-tools",
        "gpu-monitoring",
        "bmc-hardware",
        "diagnostics"
      ],
      "estimatedMinutes": 20,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "situation": "The new 8-node DGX pod installation is complete. Before handing it off to production, you need to validate the InfiniBand fabric physical layer.",
          "task": "Start with a high-level fabric topology scan to verify all expected nodes and switches are present.",
          "expectedCommands": ["ibnetdiscover"],
          "hints": [
            "ibnetdiscover maps the fabric topology",
            "Count nodes and switches",
            "Verify expected node count"
          ],
          "validation": {
            "type": "command",
            "command": "ibnetdiscover",
            "pattern": "Switch|HCA"
          }
        },
        {
          "id": "step-2",
          "situation": "ibnetdiscover shows all 8 DGX nodes and the expected leaf/spine switches. Topology looks correct.",
          "task": "Check all fabric link states and widths to identify any degraded connections.",
          "expectedCommands": ["iblinkinfo"],
          "hints": [
            "iblinkinfo shows link status fabric-wide",
            "Every link should be 4x NDR",
            "Look for any reduced-width links"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "link|4x|width"
          },
          "quiz": {
            "question": "During fabric validation, why is checking link width as important as checking link speed?",
            "options": [
              "Width affects latency more than speed",
              "A link can negotiate full speed but reduced width, hiding bandwidth loss",
              "Width determines the number of connected nodes",
              "Speed and width are always the same value"
            ],
            "correctIndex": 1,
            "explanation": "A link can train at NDR speed but only 1x or 2x width instead of 4x, delivering a fraction of expected bandwidth. This is why checking both speed and width is critical during validation — speed alone can be misleading."
          }
        },
        {
          "id": "step-3",
          "situation": "iblinkinfo reveals all links are at 4x width, but you need to check for physical layer errors that might indicate marginal connections.",
          "task": "Run ibdiagnet to perform a comprehensive fabric diagnostic including error counter checks.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet runs full fabric diagnostics",
            "Check for port counter errors",
            "Look at the summary report"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|error|check"
          }
        },
        {
          "id": "step-4",
          "situation": "ibdiagnet flagged two ports on node dgx-03 with non-zero SymbolErrorCounter values. These need deeper investigation.",
          "task": "Use perfquery to get the detailed error counters on the flagged ports.",
          "expectedCommands": ["perfquery"],
          "hints": [
            "perfquery reads port performance counters",
            "SymbolErrors indicate signal problems",
            "LinkDowned means link dropped completely"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "Symbol|Error|counter"
          },
          "quiz": {
            "question": "What do non-zero SymbolErrorCounter values on a brand new installation indicate?",
            "options": [
              "Normal break-in period for new hardware",
              "Potential cable or transceiver quality issues",
              "Software driver needs updating",
              "Switch firmware is outdated"
            ],
            "correctIndex": 1,
            "explanation": "Symbol errors on new installations point to physical layer problems — marginal cables, dirty connectors, or defective transceivers. New hardware should have zero symbol errors. Any non-zero count warrants investigation before going to production."
          }
        },
        {
          "id": "step-5",
          "situation": "perfquery shows 847 SymbolErrors on port 1 and 1,203 on port 2 of dgx-03. These are accumulating — the links are marginal.",
          "task": "Use mlxlink to inspect signal quality on the affected ports of dgx-03.",
          "expectedCommands": ["mlxlink -d mlx5_0 -p 1"],
          "hints": [
            "mlxlink -d <device> -p <port>",
            "Check signal-to-noise ratio",
            "Look at eye height/width metrics"
          ],
          "validation": {
            "type": "command",
            "command": "mlxlink",
            "pattern": "eye|signal|link"
          }
        },
        {
          "id": "step-6",
          "situation": "mlxlink shows marginal eye height on 2 of 4 lanes. The cables on these ports likely have dirty connectors or micro-bends.",
          "task": "Check the cable information with mlxcables to document which cables need replacement.",
          "expectedCommands": ["mlxcables"],
          "hints": [
            "mlxcables identifies cable types",
            "Note serial numbers for replacement",
            "Check cable length and part number"
          ],
          "validation": {
            "type": "command",
            "command": "mlxcables",
            "pattern": "cable|serial|QSFP"
          }
        },
        {
          "id": "step-7",
          "situation": "You've documented the marginal cables. The cabling team has cleaned and reseated the connectors. Now verify the fix.",
          "task": "Clear the error counters and re-check after a brief soak period to verify errors have stopped.",
          "expectedCommands": ["perfquery -x"],
          "hints": [
            "perfquery -x shows extended counters",
            "Counters should be zero after reset",
            "Monitor for new errors"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "counter|Symbol"
          }
        },
        {
          "id": "step-8",
          "situation": "Error counters are now clean. Time to verify the GPUs on all nodes can see each other properly.",
          "task": "Run nvidia-smi on one of the nodes to confirm GPU health before the final validation step.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for GPU status",
            "All 8 GPUs should be healthy",
            "Check for any ECC errors"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|MiB"
          }
        },
        {
          "id": "step-9",
          "situation": "GPUs look healthy. Final step: run a comprehensive fabric diagnostic to confirm everything passes.",
          "task": "Run ibdiagnet one final time to generate a clean validation report for the handoff.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet for final validation",
            "All checks should pass",
            "Save report for records"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "pass|check|error"
          }
        },
        {
          "id": "step-10",
          "situation": "ibdiagnet reports all clear — zero errors, full-width links across the entire pod.",
          "task": "Check the node health via ibstat one final time and document the validated link rates for the sign-off report.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat for final port status",
            "Document link rates",
            "All ports should be 400 Gb/s"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "Active|Rate"
          },
          "quiz": {
            "question": "Why is it important to run a full ibdiagnet validation before production handoff?",
            "options": [
              "It's required by NVIDIA licensing",
              "It catches marginal links that work now but will fail under production load",
              "It configures the subnet manager automatically",
              "It updates firmware on all switches"
            ],
            "correctIndex": 1,
            "explanation": "ibdiagnet catches marginal physical layer issues — links with low error margins that may work during idle testing but fail under the thermal and electrical stress of production GPU workloads. Catching these before handoff prevents costly production outages."
          }
        }
      ]
    },
    {
      "id": "domain3-slurm-setup",
      "domain": 3,
      "title": "The Slurm Setup",
      "narrative": {
        "hook": "New AI research team needs their GPU cluster integrated with the shared HPC environment.",
        "setting": "A 12-node DGX cluster must be configured with Slurm for GPU job scheduling with proper resource allocation.",
        "resolution": "Configure Slurm GRES for GPUs, set up partitions, and validate job submission."
      },
      "commandFamilies": [
        "cluster-tools",
        "gpu-monitoring",
        "container-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 25,
      "difficulty": "beginner",
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Before configuring anything, the cluster admin gives the new research team a quick overview of how Slurm manages GPU resources.",
          "task": "Understanding Slurm and GPU Scheduling",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Slurm (Simple Linux Utility for Resource Management) is the workload manager used on DGX clusters. It handles job scheduling, resource allocation, and node management.\n\nKey components:\n- slurmctld: Central controller daemon\n- slurmd: Node daemon (runs on each compute node)\n- slurmdbd: Database daemon for job accounting\n\nKey concepts:\n- Partition: Group of nodes (like a job queue)\n- GRES: Generic Resources — how Slurm tracks GPUs\n- Job: A user workload submitted with sbatch or srun\n- Allocation: Reserved resources (nodes, GPUs, memory) for a job\n\nSlurm uses GRES (Generic Resources) to manage GPU allocation. Each node declares its GPUs as gres/gpu:8, and users request GPUs with --gres=gpu:N.",
          "tips": [
            "sinfo shows partition and node status at a glance",
            "GRES configuration lives in gres.conf on each compute node"
          ]
        },
        {
          "id": "step-2",
          "type": "concept",
          "situation": "Before verifying the configuration, the cluster admin digs deeper into how the three Slurm daemons work together to manage the cluster.",
          "task": "Learn the Slurm daemon architecture and understand how slurmctld, slurmd, and slurmdbd coordinate to handle job scheduling and GPU resource management.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Slurm uses three main daemons: slurmctld (central controller on the management node), slurmd (runs on every compute node), and slurmdbd (database daemon for job accounting). The controller handles job scheduling, resource allocation, and node state management. Key concepts: a Partition groups nodes like a queue, GRES (Generic Resources) manages GPUs, and GresTypes=gpu enables GPU-aware scheduling. Understanding this architecture helps you verify each component is properly configured.",
          "tips": [
            "slurmctld must be running for any Slurm commands to work",
            "GPU scheduling requires GRES configuration in slurm.conf"
          ]
        },
        {
          "id": "step-3",
          "situation": "Fresh 12-node DGX A100 cluster with Slurm installed but not configured for GPUs.",
          "task": "Check the current Slurm configuration to understand what's already set up.",
          "expectedCommands": ["scontrol show config"],
          "hints": [
            "sinfo shows partitions",
            "scontrol show config",
            "Check for GRES settings"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "config|show"
          }
        },
        {
          "id": "step-4",
          "situation": "Basic Slurm is running but no GPU resources (GRES) are configured.",
          "task": "First verify GPU inventory on a compute node using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi on compute node",
            "Count GPUs per node",
            "Note GPU model"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|A100"
          },
          "quiz": {
            "question": "Why verify GPU inventory before Slurm GRES configuration?",
            "options": [
              "Documentation requirement",
              "GRES count must match actual hardware",
              "Driver dependency check",
              "License validation"
            ],
            "correctIndex": 1,
            "explanation": "Slurm GRES configuration must match the actual number of GPUs on each node. Misconfiguration leads to scheduling failures or resource conflicts."
          }
        },
        {
          "id": "step-5",
          "situation": "Each node has 8 A100-80GB GPUs. Need to configure Slurm to recognize them.",
          "task": "Check if dcgmi is running to provide GPU telemetry for Slurm.",
          "expectedCommands": ["dcgmi discovery -l"],
          "hints": [
            "dcgmi discovery -l lists GPUs",
            "DCGM provides GPU metrics",
            "Useful for accounting"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "discovery|list"
          }
        },
        {
          "id": "step-6",
          "situation": "DCGM is running and sees all GPUs. Slurm config files need editing.",
          "task": "Check current node information in Slurm to see feature configuration.",
          "expectedCommands": ["scontrol show node"],
          "hints": [
            "scontrol show node",
            "Look for Gres entries",
            "Check features field"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show node"
          },
          "quiz": {
            "question": "What does the Slurm GRES field specify for GPU scheduling?",
            "options": [
              "GPU driver version",
              "GPU memory and count per node",
              "GPU temperature limits",
              "GPU brand preference"
            ],
            "correctIndex": 1,
            "explanation": "GRES (Generic RESource) in Slurm specifies consumable resources like GPUs, including count and optionally type/memory, enabling precise GPU job scheduling."
          }
        },
        {
          "id": "step-7",
          "situation": "Nodes show Gres=gpu:8 after config update. Time to test job submission.",
          "task": "Check sinfo to verify the GPU partition is available with correct resources.",
          "expectedCommands": ["sinfo -N -l"],
          "hints": [
            "sinfo -N -l for detailed view",
            "Check GPU counts",
            "Verify partition state"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "gpu|GRES"
          }
        },
        {
          "id": "step-8",
          "situation": "GPU partition shows 96 total GPUs (12 nodes x 8 GPUs). Ready for testing.",
          "task": "Check squeue to see current job activity on the cluster.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows job queue",
            "Should be empty on new cluster",
            "Note format options"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "JOBID|USER"
          }
        },
        {
          "id": "step-9",
          "situation": "Queue is empty. Need to configure container support for AI workloads.",
          "task": "Verify Docker/Enroot is available for containerized workloads.",
          "expectedCommands": ["docker --version", "enroot version"],
          "hints": [
            "docker --version for Docker",
            "enroot version for Enroot",
            "Check container runtime"
          ],
          "validation": {
            "type": "command",
            "pattern": "docker|enroot|version"
          }
        },
        {
          "id": "step-10",
          "situation": "Enroot is installed for HPC containers. Test GPU visibility in containers.",
          "task": "Use docker to verify GPUs are visible inside containers.",
          "expectedCommands": [
            "docker run --gpus all nvidia/cuda:12.0-base nvidia-smi"
          ],
          "hints": [
            "docker run --gpus all nvidia-smi",
            "Should see all GPUs",
            "Tests nvidia-container-toolkit"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "gpus|nvidia"
          },
          "quiz": {
            "question": "What makes GPUs visible inside Docker containers?",
            "options": [
              "Slurm GRES configuration",
              "NVIDIA Container Toolkit",
              "CUDA driver installation",
              "Docker GPU plugin"
            ],
            "correctIndex": 1,
            "explanation": "The NVIDIA Container Toolkit (nvidia-container-toolkit) enables GPU visibility in containers by mounting driver components and device files into the container."
          }
        },
        {
          "id": "step-11",
          "situation": "Container GPU access works. Run a final validation test.",
          "task": "Use dcgmi diag to run a quick health check before declaring cluster ready.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag -r 1 quick test",
            "Run on sample node",
            "Verify no issues"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        },
        {
          "id": "step-12",
          "situation": "All systems validated. Document cluster readiness.",
          "task": "Final sinfo check to confirm cluster is ready for production workloads.",
          "expectedCommands": ["sinfo -N -l"],
          "hints": [
            "sinfo -N -l",
            "All nodes should be idle",
            "GPU resources visible"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "idle|gpu"
          }
        }
      ]
    },
    {
      "id": "domain3-container-crisis",
      "domain": 3,
      "title": "The Container Crisis",
      "narrative": {
        "hook": "Containerized ML training jobs fail with 'CUDA initialization error' after cluster update.",
        "setting": "A routine system update broke GPU access in containers across the entire cluster.",
        "resolution": "Identify nvidia-container-toolkit version mismatch and restore container GPU functionality."
      },
      "commandFamilies": [
        "container-tools",
        "gpu-monitoring",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 20,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "situation": "Users report 'CUDA initialization error' when running GPU containers after yesterday's update.",
          "task": "Verify GPUs work outside containers using nvidia-smi directly.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi on host",
            "Should show all GPUs",
            "Compare to container behavior"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Driver"
          }
        },
        {
          "id": "step-2",
          "situation": "nvidia-smi works on host - all 8 GPUs visible with driver 535.154.",
          "task": "Test GPU visibility inside a Docker container.",
          "expectedCommands": [
            "docker run --gpus all nvidia/cuda:12.0-base nvidia-smi"
          ],
          "hints": [
            "docker run --gpus all nvidia-smi",
            "Should show GPUs if working",
            "Note any errors"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "gpus|nvidia-smi"
          },
          "quiz": {
            "question": "Why might GPUs work on host but fail in containers?",
            "options": [
              "Container resource limits",
              "nvidia-container-toolkit version mismatch",
              "Docker daemon crash",
              "Network configuration"
            ],
            "correctIndex": 1,
            "explanation": "The nvidia-container-toolkit must be compatible with the host driver version. Updates that change driver versions can break container GPU access."
          }
        },
        {
          "id": "step-3",
          "situation": "Container test shows 'Failed to initialize NVML: Unknown Error'. Version mismatch suspected.",
          "task": "Check the nvidia-container-toolkit version installed on the system.",
          "expectedCommands": ["dpkg -l | grep nvidia-container-toolkit"],
          "hints": [
            "dpkg -l or rpm -q for package",
            "Look for nvidia-container-toolkit",
            "Note version number"
          ],
          "validation": {
            "type": "command",
            "pattern": "nvidia-container|toolkit|version"
          }
        },
        {
          "id": "step-4",
          "situation": "nvidia-container-toolkit is version 1.13.0 but driver is 535.154 requiring 1.14.x.",
          "task": "Check Docker daemon logs for more details about the GPU initialization failure.",
          "expectedCommands": ["journalctl -u docker --no-pager | grep nvidia"],
          "hints": [
            "journalctl -u docker",
            "Look for nvidia runtime errors",
            "Check recent logs"
          ],
          "validation": {
            "type": "command",
            "command": "journalctl",
            "pattern": "docker|nvidia"
          },
          "quiz": {
            "question": "Where are NVIDIA container runtime errors typically logged?",
            "options": [
              "nvidia-smi output",
              "Docker daemon logs or syslog",
              "CUDA error files",
              "Container stdout only"
            ],
            "correctIndex": 1,
            "explanation": "NVIDIA container runtime errors appear in Docker daemon logs (journalctl -u docker) or syslog, not in nvidia-smi which only monitors the host."
          }
        },
        {
          "id": "step-5",
          "situation": "Logs confirm: 'nvidia-container-cli: driver mismatch'. Need to update toolkit.",
          "task": "Check which Slurm jobs are currently affected by this issue.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows running jobs",
            "Look for job states",
            "Identify affected users"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "RUNNING|PENDING|STATE"
          }
        },
        {
          "id": "step-6",
          "situation": "Multiple container jobs are failing. Need to communicate and fix quickly.",
          "task": "Drain nodes for toolkit update while allowing current non-container jobs to finish.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='toolkit update'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Add reason for users",
            "Schedule update window"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|state"
          }
        },
        {
          "id": "step-7",
          "situation": "Nodes draining. The toolkit update is being installed by sysadmin team.",
          "task": "After toolkit update, verify the new version is installed.",
          "expectedCommands": ["dpkg -l | grep nvidia-container-toolkit"],
          "hints": [
            "Check package version again",
            "Should be 1.14.x or later",
            "Compatible with driver 535"
          ],
          "validation": {
            "type": "command",
            "pattern": "nvidia-container|version|1.14"
          }
        },
        {
          "id": "step-8",
          "situation": "Toolkit updated to 1.14.3. Test container GPU access again.",
          "task": "Run a Docker test to verify GPUs are now visible in containers.",
          "expectedCommands": [
            "docker run --gpus all nvidia/cuda:12.0-base nvidia-smi"
          ],
          "hints": [
            "docker run --gpus all nvidia-smi",
            "Should show all 8 GPUs",
            "No errors expected"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "gpus|nvidia-smi"
          },
          "quiz": {
            "question": "After fixing container GPU access, what should you test next?",
            "options": [
              "Reboot all nodes",
              "Run DCGM diagnostics",
              "Test Slurm container job submission",
              "Update all containers"
            ],
            "correctIndex": 2,
            "explanation": "After fixing container runtime, test the complete workflow including Slurm job submission with containers to ensure the production path works end-to-end."
          }
        },
        {
          "id": "step-9",
          "situation": "Docker test passes. Now verify Enroot (HPC container runtime) also works.",
          "task": "Test that Enroot can import and run a container with GPU access.",
          "expectedCommands": ["enroot import dockerd://nvidia/cuda:12.0-base"],
          "hints": [
            "enroot import or run",
            "Check GPU visibility",
            "Used by Slurm+Pyxis"
          ],
          "validation": {
            "type": "command",
            "command": "enroot",
            "pattern": "import|run|nvidia"
          }
        },
        {
          "id": "step-10",
          "situation": "Enroot works. Resume nodes and notify users.",
          "task": "Resume nodes in Slurm and verify they return to service.",
          "expectedCommands": ["scontrol update nodename=dgx-01 state=resume"],
          "hints": [
            "scontrol update state=resume",
            "sinfo to verify",
            "All nodes should be idle"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "resume"
          }
        }
      ]
    },
    {
      "id": "domain3-storage-showdown",
      "domain": 3,
      "title": "The Storage Showdown",
      "narrative": {
        "hook": "Parallel filesystem performance drops 80%, causing AI training jobs to stall on data loading.",
        "setting": "The shared Lustre filesystem that serves training data has slowed dramatically, impacting all users.",
        "resolution": "Identify failed OST, rebalance data, and restore filesystem performance."
      },
      "commandFamilies": [
        "cluster-tools",
        "diagnostics",
        "gpu-monitoring",
        "bmc-hardware"
      ],
      "estimatedMinutes": 22,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "situation": "Users complain GPU utilization is stuck at 10% - jobs are waiting for data.",
          "task": "Verify the GPU utilization issue using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows utilization",
            "Low GPU util with jobs running",
            "Data loading bottleneck"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Utilization|%"
          }
        },
        {
          "id": "step-2",
          "situation": "Confirmed: GPUs at 8% utilization during active training. I/O wait is the bottleneck.",
          "task": "Check Slurm job status to see how many jobs are affected.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows job status",
            "Look for running jobs",
            "Note resource usage"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "RUNNING|JOBID"
          },
          "quiz": {
            "question": "Why would low GPU utilization indicate a storage problem?",
            "options": [
              "GPUs need storage for memory",
              "Training is I/O bound waiting for data",
              "Storage provides GPU cooling",
              "CUDA requires local storage"
            ],
            "correctIndex": 1,
            "explanation": "AI training pipelines load data from storage into GPU memory. Slow storage causes GPUs to idle waiting for the next batch, appearing as low utilization."
          }
        },
        {
          "id": "step-3",
          "situation": "45 jobs running across 12 nodes, all showing low GPU utilization.",
          "task": "Check system I/O statistics to confirm the storage bottleneck.",
          "expectedCommands": ["iostat -x 1 3"],
          "hints": [
            "iostat or sar for I/O stats",
            "Look for high wait times",
            "Check read/write throughput"
          ],
          "validation": {
            "type": "command",
            "pattern": "iostat|iowait|stat"
          }
        },
        {
          "id": "step-4",
          "situation": "I/O wait is at 60%. Lustre filesystem is the bottleneck. Need to check OST health.",
          "task": "Check Lustre filesystem status to identify any failed or degraded components.",
          "expectedCommands": ["lfs df", "lfs check servers"],
          "hints": [
            "lfs check servers",
            "lfs df shows OST status",
            "Look for offline or degraded OSTs"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "check|df|status"
          },
          "quiz": {
            "question": "What is an OST in Lustre filesystem terminology?",
            "options": [
              "Object Storage Target - stores file data",
              "Operating System Thread",
              "Optimized Storage Tier",
              "Output Stream Terminal"
            ],
            "correctIndex": 0,
            "explanation": "OST (Object Storage Target) stores actual file data in Lustre. Failed or slow OSTs directly impact read/write performance for all clients."
          }
        },
        {
          "id": "step-5",
          "situation": "lfs df shows OST0003 is offline. This explains the performance degradation.",
          "task": "Check the storage server logs or BMC for hardware issues.",
          "expectedCommands": ["ipmitool sel elist"],
          "hints": [
            "ipmitool on storage server",
            "Check sel for events",
            "Look for disk errors"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sel|event|sensor"
          }
        },
        {
          "id": "step-6",
          "situation": "Storage server BMC shows multiple disk failure events on OST0003.",
          "task": "Document the storage server status for the storage team.",
          "expectedCommands": ["ipmitool sensor list"],
          "hints": [
            "ipmitool sensor list",
            "dmidecode for hardware info",
            "Document disk locations"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sensor"
          }
        },
        {
          "id": "step-7",
          "situation": "Storage team notified. Meanwhile, check if data can be migrated away from failed OST.",
          "task": "Check which directories are most affected by the OST failure.",
          "expectedCommands": ["lfs getstripe /data/training"],
          "hints": [
            "lfs getstripe shows file layout",
            "Check training data directories",
            "Identify affected datasets"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "getstripe|layout"
          }
        },
        {
          "id": "step-8",
          "situation": "Critical training datasets are striped across all OSTs including the failed one.",
          "task": "Check dcgmi to verify GPUs are healthy while waiting for storage fix.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag quick check",
            "Ensure GPUs aren't the issue",
            "Rule out GPU problems"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|health"
          },
          "quiz": {
            "question": "Why verify GPU health during a storage investigation?",
            "options": [
              "GPUs might cause storage errors",
              "Rule out GPU issues as contributing factor",
              "Storage and GPU are linked",
              "Required for documentation"
            ],
            "correctIndex": 1,
            "explanation": "When troubleshooting performance issues, verify all components are healthy. GPUs might have their own issues masked by the primary storage problem."
          }
        },
        {
          "id": "step-9",
          "situation": "GPUs are healthy. Storage team has recovered OST0003. Verify performance.",
          "task": "Check Lustre status to confirm all OSTs are back online.",
          "expectedCommands": ["lfs df"],
          "hints": [
            "lfs df shows all OSTs",
            "All should show active",
            "Check capacity distribution"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "df|OST"
          }
        },
        {
          "id": "step-10",
          "situation": "All OSTs online. Verify GPU utilization has recovered.",
          "task": "Check nvidia-smi to confirm GPU utilization has returned to normal.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows utilization",
            "Should be high now",
            "Data loading restored"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Util|GPU|%"
          }
        }
      ]
    },
    {
      "id": "domain4-silent-cluster",
      "domain": 4,
      "title": "The Silent Cluster",
      "narrative": {
        "hook": "Production cluster showing intermittent NCCL timeouts during distributed training.",
        "setting": "You're the on-call engineer for a 32-node DGX A100 cluster experiencing random training hangs.",
        "resolution": "Identify ECC errors on GPU 3 causing NVLink degradation and intermittent communication failures."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "infiniband-tools",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 25,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "You receive a ticket about NCCL all-reduce hanging on multi-node training jobs.",
          "task": "Check which nodes are running jobs using Slurm commands.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows running jobs",
            "Check nodelist for jobs",
            "Identify affected nodes"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "RUNNING|NODELIST"
          }
        },
        {
          "id": "step-2",
          "situation": "You see 4 multi-node jobs running. The user's job uses dgx-01 through dgx-08.",
          "task": "Check overall GPU health across the cluster using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU status",
            "Look for ERR! or N/A",
            "Check memory and temp"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Memory"
          },
          "quiz": {
            "question": "When investigating intermittent cluster issues, why start with nvidia-smi?",
            "options": [
              "It's the only tool available",
              "Quick snapshot shows obvious problems",
              "DCGM requires permissions",
              "nvidia-smi is more accurate"
            ],
            "correctIndex": 1,
            "explanation": "nvidia-smi provides a quick snapshot that can reveal obvious issues like missing GPUs, memory errors, or temperature problems before deeper investigation."
          }
        },
        {
          "id": "step-3",
          "situation": "nvidia-smi shows all GPUs present, but GPU 3 on dgx-04 shows high memory usage even when idle.",
          "task": "SSH to dgx-04 and check GPU 3 for ECC memory errors.",
          "expectedCommands": ["ssh dgx-04", "nvidia-smi -q -i 3"],
          "hints": [
            "SSH to dgx-04 to inspect the GPU locally",
            "nvidia-smi -q for details",
            "Look for ECC errors section",
            "Check volatile and aggregate counts"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-q|ECC|error"
          },
          "autoFaults": [
            {
              "nodeId": "dgx-04",
              "gpuId": 3,
              "type": "ecc-error",
              "severity": "critical",
              "parameters": {
                "singleBit": 150,
                "doubleBit": 8
              }
            }
          ]
        },
        {
          "id": "step-4",
          "situation": "GPU 3 shows 47 ECC single-bit errors. This could cause intermittent issues.",
          "task": "Use dcgmi to get detailed GPU health information.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag for health check",
            "Check specific GPU",
            "Look for memory issues"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|health"
          },
          "quiz": {
            "question": "What do ECC single-bit errors indicate about GPU health?",
            "options": [
              "Immediate failure imminent",
              "Normal operation",
              "Memory cells degrading but correctable",
              "Driver software bug"
            ],
            "correctIndex": 2,
            "explanation": "Single-bit ECC errors are automatically corrected but indicate memory cell degradation. High counts suggest the GPU may need attention or replacement."
          }
        },
        {
          "id": "step-5",
          "situation": "DCGM shows GPU 3 has elevated error rates. Check if InfiniBand is also affected.",
          "task": "Check InfiniBand port status on the affected node.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows IB port status",
            "Look for active/inactive",
            "Check link width and speed"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Active|Width"
          }
        },
        {
          "id": "step-6",
          "situation": "IB ports look healthy. The issue seems isolated to GPU 3 memory.",
          "task": "Check NVLink status to see if GPU 3 affects NVLink communication.",
          "expectedCommands": ["nvidia-smi nvlink --status"],
          "hints": [
            "nvidia-smi nvlink --status",
            "Check GPU 3 links",
            "Look for inactive or errors"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "nvlink|status"
          }
        },
        {
          "id": "step-7",
          "situation": "NVLink shows elevated replay counts on links connected to GPU 3.",
          "task": "Check NVLink error counters for details on the communication issues.",
          "expectedCommands": ["nvidia-smi nvlink -e"],
          "hints": [
            "nvidia-smi nvlink -e",
            "Check replay and CRC errors",
            "Compare GPU 3 to others"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "nvlink|-e"
          },
          "quiz": {
            "question": "How do ECC errors affect NVLink communication?",
            "options": [
              "No relation between ECC and NVLink",
              "Memory errors can corrupt NVLink packets",
              "NVLink has its own ECC",
              "Only affects PCIe"
            ],
            "correctIndex": 1,
            "explanation": "GPU memory errors can corrupt data being sent over NVLink, causing increased replay counts as the link protocol detects and retries corrupted transmissions."
          }
        },
        {
          "id": "step-8",
          "situation": "Confirmed: GPU 3 memory errors are causing NVLink retries. Need to isolate the GPU.",
          "task": "Create a bug report documenting the GPU 3 issues before taking action.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh",
            "Captures full state",
            "Needed for RMA"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report"
          }
        },
        {
          "id": "step-9",
          "situation": "Bug report saved. Need to drain the node for GPU replacement.",
          "task": "Drain dgx-04 from Slurm to prepare for maintenance.",
          "expectedCommands": [
            "scontrol update nodename=dgx-04 state=drain reason='GPU 3 ECC errors'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Reason: GPU 3 ECC errors",
            "Jobs will migrate"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|dgx-04"
          }
        },
        {
          "id": "step-10",
          "situation": "Node drained. Run final diagnostics to document GPU 3 failure mode.",
          "task": "Run comprehensive DCGM diagnostics on the affected GPU.",
          "expectedCommands": ["dcgmi diag -r 3 -i 3"],
          "hints": [
            "dcgmi diag -r 3 -i 3",
            "Level 3 for thorough test",
            "Document failure pattern"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 3"
          }
        }
      ]
    },
    {
      "id": "domain4-bandwidth-bottleneck",
      "domain": 4,
      "title": "The Bandwidth Bottleneck",
      "narrative": {
        "hook": "NCCL benchmark shows 50% of expected all-reduce bandwidth on the new cluster.",
        "setting": "A newly deployed cluster is underperforming. Acceptance testing reveals poor multi-node communication.",
        "resolution": "Discover InfiniBand port running at reduced width due to cable issue."
      },
      "commandFamilies": [
        "infiniband-tools",
        "gpu-monitoring",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 23,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "A machine-learning researcher reports that multi-node training jobs are running 3x slower than expected. The cluster was recently deployed and acceptance testing passed.",
          "task": "Before diving into diagnostics, understand the key bandwidth layers in a DGX cluster and how each can become a bottleneck.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "A DGX cluster has multiple bandwidth layers that can become bottlenecks: NVLink (600 GB/s per GPU on DGX A100) connects GPUs within a node via NVSwitch. InfiniBand (200 Gb/s per port, 8 ports = 1.6 Tb/s per node) connects nodes across the fabric. PCIe Gen4 (32 GB/s per x16 slot) connects GPUs to the host CPU. Each layer has different bandwidth, latency, and failure modes. Performance issues often stem from a single weak link in this chain. Systematic diagnosis starts with checking each layer.",
          "tips": [
            "NVLink is 12x faster than PCIe for GPU-to-GPU transfers",
            "InfiniBand uses RDMA for low-latency cross-node communication"
          ]
        },
        {
          "id": "step-2",
          "situation": "NCCL all-reduce test shows 120 GB/s instead of expected 240 GB/s across nodes.",
          "task": "Verify GPU health is not the bottleneck using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi on affected nodes",
            "Check GPU health",
            "Verify NVLink status"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|NVLink"
          }
        },
        {
          "id": "step-3",
          "situation": "GPUs look healthy. Intra-node NVLink bandwidth tests pass. Issue is inter-node.",
          "task": "Check InfiniBand port status on the first node.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows port status",
            "Check rate and width",
            "Look for 4x HDR expected"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Rate|Width"
          },
          "quiz": {
            "question": "What does '4x' in InfiniBand link width mean?",
            "options": [
              "4 times normal speed",
              "4 lanes active",
              "4 ports total",
              "Generation 4"
            ],
            "correctIndex": 1,
            "explanation": "InfiniBand links use multiple lanes. 4x means 4 lanes are active. Full width is typically 4x; reduced width like 1x indicates cable or port issues."
          }
        },
        {
          "id": "step-4",
          "situation": "ibstat shows all ports at 4x HDR (200 Gb/s) on node 1. Check other nodes.",
          "task": "Use iblinkinfo to see the entire fabric topology and link states.",
          "expectedCommands": ["iblinkinfo"],
          "hints": [
            "iblinkinfo shows all links",
            "Look for reduced width",
            "Check switch connections"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "Width|Rate|Switch"
          }
        },
        {
          "id": "step-5",
          "situation": "iblinkinfo shows dgx-07 port 2 is running at 1x width instead of 4x!",
          "task": "SSH to dgx-07 and check detailed port status for the degraded port.",
          "expectedCommands": ["ssh dgx-07", "ibstat"],
          "hints": [
            "SSH to dgx-07 to check its local port status",
            "ibstat on dgx-07",
            "Check port 2 specifically",
            "Note physical state"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "Port 2|Width"
          },
          "quiz": {
            "question": "What typically causes an IB port to run at reduced width?",
            "options": [
              "Software configuration",
              "Cable damage or connection issue",
              "Switch firmware bug",
              "GPU driver problem"
            ],
            "correctIndex": 1,
            "explanation": "Reduced link width usually indicates physical layer problems - damaged cables, dirty connectors, or loose connections preventing all lanes from training."
          }
        },
        {
          "id": "step-6",
          "situation": "Port 2 on dgx-07 shows physical state 'LinkUp' but width 1x. Cable suspect.",
          "task": "Check InfiniBand error counters for this port.",
          "expectedCommands": ["perfquery"],
          "hints": [
            "perfquery for port counters",
            "Look for symbol errors",
            "High errors indicate cable issue"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "error|counter|symbol"
          }
        },
        {
          "id": "step-7",
          "situation": "perfquery shows elevated symbol error count on this port. Cable issue confirmed.",
          "task": "Run full fabric diagnostic to check for any other issues.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet for full fabric test",
            "Check for other degraded links",
            "Document all issues"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|fabric"
          }
        },
        {
          "id": "step-8",
          "situation": "ibdiagnet confirms single cable issue on dgx-07 port 2. No other issues found.",
          "task": "Check which jobs are using dgx-07 before scheduling cable replacement.",
          "expectedCommands": ["squeue -w dgx-07"],
          "hints": [
            "squeue shows jobs",
            "Filter by node",
            "Identify affected users"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "dgx-07|NODELIST"
          },
          "quiz": {
            "question": "Why is a 1x link causing 50% bandwidth loss cluster-wide?",
            "options": [
              "All traffic goes through that link",
              "NCCL uses ring topology",
              "One slow link bottlenecks collective operations",
              "Hardware limitation"
            ],
            "correctIndex": 2,
            "explanation": "In collective operations like all-reduce, the slowest link becomes the bottleneck for the entire operation, reducing overall performance significantly."
          }
        },
        {
          "id": "step-9",
          "situation": "Jobs on dgx-07 completed. Ready to drain for cable replacement.",
          "task": "Drain dgx-07 for cable maintenance.",
          "expectedCommands": [
            "scontrol update nodename=dgx-07 state=drain reason='IB cable replacement'"
          ],
          "hints": [
            "scontrol update state=drain",
            "Reason: IB cable replacement",
            "Node will stop accepting jobs"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|dgx-07"
          }
        },
        {
          "id": "step-10",
          "situation": "Node drained. Cable replaced. Verify link comes up at full width.",
          "task": "Check ibstat to verify the link is now at full 4x width.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows link width",
            "Should show 4x now",
            "Check physical state"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "4x|Width"
          }
        },
        {
          "id": "step-11",
          "situation": "Link now at 4x HDR. Resume node and verify bandwidth.",
          "task": "Resume the node in Slurm after successful cable replacement.",
          "expectedCommands": ["scontrol update nodename=dgx-07 state=resume"],
          "hints": [
            "scontrol update state=resume",
            "sinfo to verify state",
            "Run bandwidth test next"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "resume"
          }
        }
      ]
    },
    {
      "id": "domain4-nccl-nightmare",
      "domain": 4,
      "title": "The NCCL Nightmare",
      "narrative": {
        "hook": "Multi-node training jobs hang at random points with no error messages.",
        "setting": "Large language model training keeps freezing. Logs show nothing. Users are frustrated.",
        "resolution": "Enable NCCL debug logging to discover network timeout from misconfigured IB subnet."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "infiniband-tools",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 24,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "8-node LLM training hangs randomly after 2-6 hours. No errors in application logs.",
          "task": "Check Slurm job status to see the current state of running jobs.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows job status",
            "Look for run time",
            "Check resource allocation"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "RUNNING|TIME"
          }
        },
        {
          "id": "step-2",
          "situation": "Job shows RUNNING for 3 hours but users report it's frozen. Check GPU activity.",
          "task": "Use nvidia-smi to check GPU utilization on the allocated nodes.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows utilization",
            "Low util means waiting",
            "Check all allocated GPUs"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Util|GPU|%"
          },
          "quiz": {
            "question": "What does 0% GPU utilization during 'running' training indicate?",
            "options": [
              "Training complete",
              "Data loading phase",
              "Communication deadlock",
              "GPU power saving"
            ],
            "correctIndex": 2,
            "explanation": "Zero GPU utilization during training typically indicates a communication deadlock where processes are waiting for collective operations that will never complete."
          }
        },
        {
          "id": "step-3",
          "situation": "All GPUs at 0% utilization. Classic NCCL deadlock symptoms.",
          "task": "Check InfiniBand connectivity between the nodes.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat on multiple nodes",
            "Verify ports are active",
            "Check for any down ports"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "Active|State"
          }
        },
        {
          "id": "step-4",
          "situation": "All IB ports show Active. But are they seeing each other correctly?",
          "task": "Use iblinkinfo to verify full mesh connectivity.",
          "expectedCommands": ["iblinkinfo"],
          "hints": [
            "iblinkinfo shows topology",
            "Verify all nodes connected",
            "Look for missing links"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "dgx|Switch"
          },
          "quiz": {
            "question": "Why check full mesh connectivity for NCCL issues?",
            "options": [
              "NCCL requires mesh topology",
              "Missing paths cause routing failures",
              "Performance optimization",
              "NVIDIA requirement"
            ],
            "correctIndex": 1,
            "explanation": "NCCL relies on the subnet manager's routing tables. Missing connectivity or routing issues can cause collective operations to fail or hang."
          }
        },
        {
          "id": "step-5",
          "situation": "Topology looks complete. Check for network performance issues.",
          "task": "Check IB error counters across the fabric.",
          "expectedCommands": ["perfquery"],
          "hints": [
            "perfquery for port errors",
            "Check multiple nodes",
            "Look for high error counts"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "error|Rcv|Xmt"
          }
        },
        {
          "id": "step-6",
          "situation": "Error counters clean. Issue might be at a higher level. Check routing tables.",
          "task": "Verify the subnet manager configuration and LID assignments.",
          "expectedCommands": ["smpquery nodeinfo", "sminfo"],
          "hints": [
            "smpquery or sminfo",
            "Check LID assignments",
            "Verify SM is running"
          ],
          "validation": {
            "type": "command",
            "pattern": "smpquery|sminfo|LID"
          }
        },
        {
          "id": "step-7",
          "situation": "SM running but some nodes have unusual LID assignments in a different range.",
          "task": "Check detailed subnet manager logs for routing decisions.",
          "expectedCommands": ["journalctl -u opensmd --no-pager"],
          "hints": [
            "opensm logs if accessible",
            "Look for routing warnings",
            "Check for subnet issues"
          ],
          "validation": {
            "type": "command",
            "pattern": "opensm|log|routing"
          }
        },
        {
          "id": "step-8",
          "situation": "Logs show two subnets accidentally merged. Some nodes on different IPoIB subnet.",
          "task": "Document current network configuration for the network team.",
          "expectedCommands": ["ip addr show ib0"],
          "hints": [
            "ip addr for IPoIB config",
            "Note subnet masks",
            "Document for network team"
          ],
          "validation": {
            "type": "command",
            "pattern": "ip addr|inet|ib0"
          },
          "quiz": {
            "question": "How can IPoIB subnet misconfiguration cause NCCL hangs?",
            "options": [
              "NCCL uses IPoIB for all communication",
              "Socket communication during setup fails",
              "GPUDirect requires matching subnets",
              "CUDA driver limitation"
            ],
            "correctIndex": 1,
            "explanation": "NCCL uses IPoIB for initial setup and coordination. Nodes on different subnets can't establish initial TCP connections, causing setup hangs."
          }
        },
        {
          "id": "step-9",
          "situation": "Network team confirms subnet misconfiguration. Fix being deployed.",
          "task": "Run dcgmi diagnostics to verify GPU subsystem while waiting for fix.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag for health check",
            "Ensure no GPU issues",
            "Rule out other problems"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        },
        {
          "id": "step-10",
          "situation": "GPU health verified. Network fix deployed. Verify connectivity.",
          "task": "Check ibstat to confirm all nodes are on the correct subnet after fix.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows port info",
            "Verify LID assignments",
            "Should be sequential now"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "LID|port"
          }
        }
      ]
    },
    {
      "id": "domain4-benchmark-battle",
      "domain": 4,
      "title": "The Benchmark Battle",
      "narrative": {
        "hook": "HPL benchmark achieves only 60% of theoretical peak performance on new supercomputer.",
        "setting": "The acceptance test for a major HPC installation depends on hitting performance targets.",
        "resolution": "Optimize HPL parameters, fix NUMA binding, and achieve 90% of theoretical peak."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "cluster-tools",
        "diagnostics",
        "bmc-hardware"
      ],
      "estimatedMinutes": 26,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "HPL benchmark on 16-node DGX H100 cluster achieves 23.4 PFLOPS instead of expected 39 PFLOPS.",
          "task": "Verify basic GPU configuration and availability using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi across all nodes",
            "Verify 128 total GPUs",
            "Check clocks and memory"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|H100|Memory"
          }
        },
        {
          "id": "step-2",
          "situation": "All 128 H100 GPUs detected. Clocks show max frequency. Memory available.",
          "task": "Check GPU topology to verify optimal NVLink connectivity.",
          "expectedCommands": ["nvidia-smi topo -m"],
          "hints": [
            "nvidia-smi topo -m",
            "All GPUs should use NVLink",
            "NV18 for H100"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "topo|-m|NV"
          },
          "quiz": {
            "question": "Why does NVLink connectivity matter for HPL performance?",
            "options": [
              "HPL requires NVLink",
              "GPU-to-GPU data movement is critical for large matrices",
              "License verification",
              "Error checking requirement"
            ],
            "correctIndex": 1,
            "explanation": "HPL involves large matrix operations distributed across GPUs. High-bandwidth NVLink connectivity is essential for efficient data exchange during computation."
          }
        },
        {
          "id": "step-3",
          "situation": "NVLink topology looks correct. Check InfiniBand for inter-node communication.",
          "task": "Verify InfiniBand is running at full bandwidth between nodes.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat for port status",
            "Check 400 Gb/s NDR",
            "Verify all ports active"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "Rate|NDR|Active"
          }
        },
        {
          "id": "step-4",
          "situation": "InfiniBand shows full 400 Gb/s NDR on all ports. Check system configuration.",
          "task": "Check CPU and memory configuration for NUMA optimization.",
          "expectedCommands": ["numactl --hardware"],
          "hints": [
            "numactl --hardware",
            "Check node distribution",
            "Verify memory bandwidth"
          ],
          "validation": {
            "type": "command",
            "command": "numactl",
            "pattern": "node|cpus|memory"
          },
          "quiz": {
            "question": "Why is NUMA configuration important for GPU benchmarks?",
            "options": [
              "NUMA controls GPU clock speed",
              "CPU-GPU affinity affects PCIe bandwidth",
              "Only matters for CPU benchmarks",
              "NVIDIA driver requirement"
            ],
            "correctIndex": 1,
            "explanation": "Proper NUMA binding ensures processes use CPUs and memory local to their assigned GPUs, maximizing PCIe bandwidth and reducing latency for data transfers."
          }
        },
        {
          "id": "step-5",
          "situation": "NUMA shows 2 nodes with 64 cores each. Need to verify process affinity.",
          "task": "Check current HPL process CPU binding using taskset or similar.",
          "expectedCommands": ["taskset -p $(pgrep hpl)"],
          "hints": [
            "taskset or numactl shows binding",
            "Check if processes are spread",
            "Should match GPU locality"
          ],
          "validation": {
            "type": "command",
            "pattern": "taskset|numactl|bind|affinity"
          }
        },
        {
          "id": "step-6",
          "situation": "Processes are not properly bound to NUMA nodes! This explains some performance loss.",
          "task": "Check system thermal state during benchmark.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors during load",
            "Check for throttling",
            "GPU temps should stay under 80C"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Core|GPU"
          }
        },
        {
          "id": "step-7",
          "situation": "Thermals show some GPUs at 83C with power throttling. Cooling adjustment needed.",
          "task": "Check GPU power and clock states during the benchmark.",
          "expectedCommands": ["nvidia-smi -q -d POWER"],
          "hints": [
            "nvidia-smi -q -d POWER",
            "Check power draw",
            "Look for throttle reasons"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "power|POWER|throttle"
          }
        },
        {
          "id": "step-8",
          "situation": "Power throttling detected. Some GPUs hitting 700W limit and throttling.",
          "task": "Check cluster-wide job status and resource utilization.",
          "expectedCommands": ["scontrol show job"],
          "hints": [
            "squeue for job status",
            "scontrol show job for details",
            "Verify exclusive access"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show job|exclusive"
          },
          "quiz": {
            "question": "Why does power throttling significantly impact HPL scores?",
            "options": [
              "HPL requires constant power",
              "Throttling reduces compute frequency",
              "Power measurement error",
              "Memory bandwidth affected"
            ],
            "correctIndex": 1,
            "explanation": "Power throttling reduces GPU clock frequencies to stay within power limits, directly reducing FLOPS. HPL is compute-bound, so any frequency reduction hurts scores."
          }
        },
        {
          "id": "step-9",
          "situation": "Issues identified: NUMA binding and thermal throttling. Fixes being applied.",
          "task": "Run DCGM diagnostics to ensure no underlying hardware issues.",
          "expectedCommands": ["dcgmi diag -r 2"],
          "hints": [
            "dcgmi diag -r 2",
            "Check all GPUs",
            "Verify no hardware problems"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          }
        },
        {
          "id": "step-10",
          "situation": "Hardware healthy. After fixes, re-run HPL verification.",
          "task": "Check final nvidia-smi status before optimized benchmark run.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for final check",
            "Verify all GPUs ready",
            "Clocks at max, temps reasonable"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Temp|Power"
          }
        }
      ]
    },
    {
      "id": "domain5-xid-investigation",
      "domain": 5,
      "title": "The XID Investigation",
      "narrative": {
        "hook": "System logs flooded with 'XID 79' errors, jobs failing randomly.",
        "setting": "Production cluster showing GPU errors that don't point to obvious hardware failure.",
        "resolution": "Trace XID 79 (GPU fallen off bus) to thermal cycling causing PCIe connector issues."
      },
      "commandFamilies": [
        "diagnostics",
        "gpu-monitoring",
        "bmc-hardware",
        "cluster-tools"
      ],
      "estimatedMinutes": 24,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "Monitoring alerts show frequent XID 79 errors on dgx-05 over the past 24 hours.",
          "task": "Check system logs for XID error patterns and frequency.",
          "expectedCommands": ["ssh dgx-05", "dmesg | grep -i xid"],
          "hints": [
            "First SSH to the affected node to check its local logs",
            "dmesg for kernel messages",
            "Look for NVRM XID errors",
            "Note error frequency"
          ],
          "validation": {
            "type": "command",
            "command": "dmesg",
            "pattern": "XID|NVRM|error"
          }
        },
        {
          "id": "step-2",
          "situation": "dmesg shows 'XID 79' errors occurring every 2-3 hours, affecting different GPUs.",
          "task": "Check current GPU status using nvidia-smi.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows GPU health",
            "Look for missing or errored GPUs",
            "Note affected GPU indices"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|ERR"
          },
          "autoFaults": [
            {
              "nodeId": "dgx-05",
              "gpuId": 2,
              "type": "xid-error",
              "severity": "critical",
              "parameters": {
                "xid": 79,
                "description": "GPU has fallen off the bus"
              }
            }
          ],
          "quiz": {
            "question": "What does XID 79 (GPU has fallen off the bus) typically indicate?",
            "options": [
              "Driver crash",
              "PCIe link failure",
              "CUDA error",
              "Memory exhaustion"
            ],
            "correctIndex": 1,
            "explanation": "XID 79 indicates the GPU is no longer responding on the PCIe bus, typically due to hardware issues, power problems, or PCIe link instability."
          }
        },
        {
          "id": "step-3",
          "situation": "Currently all 8 GPUs are visible but GPU 5 was recently reset. Pattern suggests intermittent failures.",
          "task": "Check BMC System Event Log for correlated hardware events.",
          "expectedCommands": ["ipmitool sel elist"],
          "hints": [
            "ipmitool sel elist",
            "Look for thermal or power events",
            "Match timestamps to XID errors"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sel|event"
          }
        },
        {
          "id": "step-4",
          "situation": "SEL shows thermal events preceding each XID error by 5-10 minutes.",
          "task": "Check current thermal sensor readings across the system.",
          "expectedCommands": ["sensors"],
          "hints": [
            "sensors shows temperatures",
            "Check GPU and system temps",
            "Look for hot spots"
          ],
          "validation": {
            "type": "command",
            "command": "sensors",
            "pattern": "temp|Core"
          },
          "quiz": {
            "question": "How can thermal issues cause PCIe failures?",
            "options": [
              "GPUs shut down from overheating",
              "Thermal expansion affects connectors",
              "Heat damages PCIe lanes",
              "Driver thermal protection"
            ],
            "correctIndex": 1,
            "explanation": "Thermal cycling causes expansion and contraction that can affect PCIe connector contact quality, leading to intermittent link failures."
          }
        },
        {
          "id": "step-5",
          "situation": "Ambient temperature in the rack area shows significant variation. HVAC issue suspected.",
          "task": "Check historical temperature data in the BMC.",
          "expectedCommands": ["ipmitool sensor get 'Inlet Temp'"],
          "hints": [
            "ipmitool sensor get specific sensor",
            "Look for temp trends",
            "BMC may log history"
          ],
          "validation": {
            "type": "command",
            "command": "ipmitool",
            "pattern": "sensor|temp|inlet"
          }
        },
        {
          "id": "step-6",
          "situation": "Inlet temperature varies 15C over the course of each day. HVAC cycling too aggressively.",
          "task": "Document GPU health status before scheduling maintenance.",
          "expectedCommands": ["nvidia-smi -q"],
          "hints": [
            "nvidia-smi -q for details",
            "Check ECC errors",
            "Document all 8 GPUs"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-q|ECC|error"
          }
        },
        {
          "id": "step-7",
          "situation": "GPU health looks okay but XID errors are accumulating. Create support bundle.",
          "task": "Generate nvidia-bug-report for detailed analysis.",
          "expectedCommands": ["nvidia-bug-report.sh"],
          "hints": [
            "nvidia-bug-report.sh",
            "Captures full system state",
            "Useful for RMA if needed"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-bug-report",
            "pattern": "report"
          }
        },
        {
          "id": "step-8",
          "situation": "Bug report captured. Need to stabilize the system while facilities fixes HVAC.",
          "task": "Run DCGM diagnostics to assess current GPU health.",
          "expectedCommands": ["dcgmi diag -r 2"],
          "hints": [
            "dcgmi diag -r 2",
            "Check for hardware degradation",
            "Document any failures"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag"
          },
          "quiz": {
            "question": "Why run diagnostics on a thermally-stressed system?",
            "options": [
              "Heat reveals hidden defects",
              "Establish baseline before changes",
              "Required for warranty",
              "Verify cooling is adequate"
            ],
            "correctIndex": 1,
            "explanation": "Running diagnostics establishes a baseline of current GPU health before environmental changes, helping track whether conditions improve after the HVAC fix."
          }
        },
        {
          "id": "step-9",
          "situation": "Diagnostics show borderline results on GPU 5. Recommend proactive monitoring.",
          "task": "Drain the node for inspection and potential PCIe reseat.",
          "expectedCommands": [
            "scontrol update nodename=dgx-05 state=drain reason='XID 79 PCIe reseat'"
          ],
          "hints": [
            "scontrol drain",
            "Schedule maintenance window",
            "Add clear reason"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain"
          }
        },
        {
          "id": "step-10",
          "situation": "Node drained. After PCIe reseat and HVAC fix, verify recovery.",
          "task": "Run final nvidia-smi check after maintenance.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for GPU status",
            "All 8 should be healthy",
            "Monitor for future XID errors"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|8"
          }
        }
      ]
    },
    {
      "id": "domain5-memory-mystery",
      "domain": 5,
      "title": "The Memory Mystery",
      "narrative": {
        "hook": "Training jobs crash with 'CUDA out of memory' despite plenty of GPU memory showing available.",
        "setting": "Users report impossible memory errors on jobs that ran fine last week.",
        "resolution": "Discover memory fragmentation from leaked allocations in a shared PyTorch environment."
      },
      "commandFamilies": [
        "gpu-monitoring",
        "diagnostics",
        "container-tools",
        "cluster-tools"
      ],
      "estimatedMinutes": 22,
      "difficulty": "intermediate",
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "The overnight batch of training jobs all failed with out-of-memory errors on a node that previously handled them fine.",
          "task": "Before investigating, understand how GPU memory works and what causes OOM failures on NVIDIA datacenter GPUs.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Each NVIDIA A100 GPU has 80 GB of HBM2e memory. GPU memory is divided into: framebuffer (FB) for tensor data and model weights, reserved memory for the driver and ECC overhead (~5-6%), and BAR1 memory for CPU-side mapping. Common causes of out-of-memory errors: leaked GPU contexts from previous jobs, memory fragmentation, increased batch sizes, model size changes, or actual hardware ECC issues reducing usable memory. The nvidia-smi command shows current memory usage, while nvidia-smi pmon shows per-process memory consumption.",
          "tips": [
            "GPU memory does not have swap — when it is full, allocations fail immediately",
            "Always check for orphaned GPU processes from previous jobs with nvidia-smi pmon"
          ]
        },
        {
          "id": "step-2",
          "situation": "Users report 'CUDA out of memory' when allocating 40GB on GPUs showing 70GB free.",
          "task": "Check nvidia-smi to see current GPU memory state.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows memory usage",
            "Check used vs total",
            "Note any processes"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Memory|Used|Free"
          }
        },
        {
          "id": "step-3",
          "situation": "nvidia-smi shows 10GB used but users can't allocate 40GB contiguous. Fragmentation suspected.",
          "task": "Check what processes are using GPU memory.",
          "expectedCommands": ["nvidia-smi pmon -s m"],
          "hints": [
            "nvidia-smi pmon for processes",
            "Look for orphan processes",
            "Check process owners"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "pmon|process"
          },
          "autoFaults": [
            {
              "nodeId": "dgx-00",
              "gpuId": 0,
              "type": "memory-full",
              "severity": "critical",
              "parameters": {
                "memoryUsed": 79000
              }
            }
          ],
          "quiz": {
            "question": "What causes GPU memory fragmentation?",
            "options": [
              "Hardware defect",
              "Repeated allocations and frees leaving gaps",
              "Driver bug",
              "CUDA version mismatch"
            ],
            "correctIndex": 1,
            "explanation": "Memory fragmentation occurs when repeated allocations and deallocations leave non-contiguous free blocks, preventing large contiguous allocations even with total free space."
          }
        },
        {
          "id": "step-4",
          "situation": "Several zombie processes holding small GPU memory allocations found.",
          "task": "Check which Slurm jobs these processes belong to.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue to see jobs",
            "Match PIDs to jobs",
            "Look for orphan processes"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "JOBID|USER|STATE"
          }
        },
        {
          "id": "step-5",
          "situation": "Orphan processes from completed jobs that didn't clean up properly.",
          "task": "Check if these are containerized jobs with cleanup issues.",
          "expectedCommands": ["docker ps"],
          "hints": [
            "docker ps for containers",
            "Check Slurm epilog",
            "Container cleanup script"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "ps|CONTAINER"
          },
          "quiz": {
            "question": "Why might containerized jobs leave orphan GPU processes?",
            "options": [
              "Container bug",
              "Slurm didn't run epilog properly",
              "GPU driver leak",
              "All of the above are possible"
            ],
            "correctIndex": 3,
            "explanation": "Orphan GPU processes can result from container cleanup failures, missing Slurm epilog scripts, or application code that doesn't properly release resources."
          }
        },
        {
          "id": "step-6",
          "situation": "Found 3 orphan containers with GPU processes still running.",
          "task": "Check dcgmi for detailed GPU memory state.",
          "expectedCommands": ["dcgmi dmon -e 203,204"],
          "hints": [
            "dcgmi dmon for metrics",
            "Check memory allocation",
            "Look for leaks"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "dmon|memory"
          }
        },
        {
          "id": "step-7",
          "situation": "DCGM shows fragmented memory allocation pattern. Need to clean up and reset.",
          "task": "Document current state before cleanup.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for documentation",
            "Note process IDs",
            "Record memory state"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Process|GPU|Memory"
          }
        },
        {
          "id": "step-8",
          "situation": "State documented. Time to clean up orphan processes and containers.",
          "task": "Drain the node to prevent new jobs while cleaning up.",
          "expectedCommands": [
            "scontrol update nodename=dgx-01 state=drain reason='GPU memory cleanup'"
          ],
          "hints": [
            "scontrol drain",
            "Prevents new allocations",
            "Allows cleanup"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain"
          }
        },
        {
          "id": "step-9",
          "situation": "Node drained. Clean up orphan containers.",
          "task": "Remove stopped and orphan containers.",
          "expectedCommands": ["docker container prune -f"],
          "hints": [
            "docker container prune",
            "Or docker rm specific containers",
            "Verify GPU processes stop"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "prune|rm|stop"
          },
          "quiz": {
            "question": "After cleaning containers, why might GPU memory still be fragmented?",
            "options": [
              "Containers don't use GPU memory",
              "CUDA contexts persist until process dies",
              "Memory is physically damaged",
              "Driver caches allocations"
            ],
            "correctIndex": 1,
            "explanation": "CUDA contexts and memory allocations persist until the process fully terminates. Simply stopping containers might not fully release GPU resources."
          }
        },
        {
          "id": "step-10",
          "situation": "Containers removed. Verify GPU memory is cleared.",
          "task": "Check nvidia-smi to confirm memory is freed.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for memory state",
            "Should show minimal usage",
            "All 8 GPUs should be clean"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Memory|Free"
          }
        },
        {
          "id": "step-11",
          "situation": "Memory cleared. Resume node for production use.",
          "task": "Resume the node in Slurm and verify it's ready for jobs.",
          "expectedCommands": ["scontrol update nodename=dgx-01 state=resume"],
          "hints": [
            "scontrol resume",
            "sinfo to verify",
            "Monitor for recurrence"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "resume"
          }
        }
      ]
    },
    {
      "id": "domain5-network-nightmare",
      "domain": 5,
      "title": "The Network Nightmare",
      "narrative": {
        "hook": "Multi-node training performance degrades gradually over days until jobs fail.",
        "setting": "A subtle network issue is causing progressive performance degradation that's hard to pinpoint.",
        "resolution": "Discover accumulating InfiniBand port errors from a degrading optical transceiver."
      },
      "commandFamilies": [
        "infiniband-tools",
        "gpu-monitoring",
        "cluster-tools",
        "diagnostics"
      ],
      "estimatedMinutes": 25,
      "difficulty": "advanced",
      "steps": [
        {
          "id": "step-1",
          "situation": "Users report training speed has dropped 30% over the past week with no code changes.",
          "task": "Check current GPU utilization to understand the bottleneck.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for GPU util",
            "Low util suggests waiting",
            "Check memory usage too"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "Util|GPU|%"
          }
        },
        {
          "id": "step-2",
          "situation": "GPU utilization oscillates between 95% and 40%. Communication phases are slowing down.",
          "task": "Check InfiniBand port status for any obvious issues.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows port state",
            "Verify active status",
            "Check rate and width"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Active|Rate"
          },
          "quiz": {
            "question": "Why does oscillating GPU utilization suggest a communication issue?",
            "options": [
              "GPUs are overheating",
              "Compute phases are fine but communication phases are slow",
              "Memory bandwidth issue",
              "Driver bug"
            ],
            "correctIndex": 1,
            "explanation": "Oscillating utilization with high peaks and low valleys indicates compute phases run normally but communication/synchronization phases are taking longer than expected."
          }
        },
        {
          "id": "step-3",
          "situation": "All ports show Active. But 'active' doesn't mean error-free.",
          "task": "SSH to dgx-03 and check InfiniBand error counters for accumulated errors.",
          "expectedCommands": ["ssh dgx-03", "perfquery"],
          "hints": [
            "SSH to dgx-03 to check its local error counters",
            "perfquery shows counters",
            "Look for symbol errors",
            "CRC errors indicate issues"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "error|Symbol|Rcv"
          },
          "autoFaults": [
            {
              "nodeId": "dgx-03",
              "gpuId": 0,
              "type": "nvlink-failure",
              "severity": "warning"
            }
          ]
        },
        {
          "id": "step-4",
          "situation": "perfquery shows high SymbolErrorCounter on port 2 of dgx-03. Growing errors.",
          "task": "Check historical error rate using extended counters.",
          "expectedCommands": ["perfquery -x"],
          "hints": [
            "perfquery -x for extended",
            "Compare multiple reads",
            "Calculate error rate"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "-x|extended"
          },
          "quiz": {
            "question": "What do accumulating symbol errors typically indicate?",
            "options": [
              "Software configuration issue",
              "Physical layer degradation like failing transceiver",
              "Firmware bug",
              "Overloaded switch"
            ],
            "correctIndex": 1,
            "explanation": "Symbol errors indicate bit-level corruption in the physical layer, typically caused by degrading optical transceivers, damaged cables, or dirty connectors."
          }
        },
        {
          "id": "step-5",
          "situation": "Error rate is 1000x higher than other ports. Transceiver degradation suspected.",
          "task": "Run comprehensive fabric diagnostics.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet for full check",
            "Will flag problem ports",
            "Document all issues"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|error|warning"
          }
        },
        {
          "id": "step-6",
          "situation": "ibdiagnet flags port as having excessive errors and recommends replacement.",
          "task": "Check if this port is causing packet retransmissions.",
          "expectedCommands": ["perfquery"],
          "hints": [
            "perfquery for retry counts",
            "High retries cause latency",
            "Explains performance drop"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "retry|Xmt"
          }
        },
        {
          "id": "step-7",
          "situation": "Retransmit counts confirm the issue. Each retry adds latency to communication.",
          "task": "Document the failing port for RMA.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "Note port and GUID",
            "Document error counts",
            "Screenshot or save output"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "GUID|port"
          }
        },
        {
          "id": "step-8",
          "situation": "Transceiver documented. Schedule replacement during maintenance window.",
          "task": "Check which jobs are using the affected node.",
          "expectedCommands": ["squeue -w dgx-03"],
          "hints": [
            "squeue shows jobs",
            "Filter by node",
            "Notify affected users"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "dgx-03|NODELIST"
          },
          "quiz": {
            "question": "Should you immediately drain a node with high IB errors?",
            "options": [
              "Yes, always",
              "No, wait for complete failure",
              "Depends on error rate and impact",
              "Only if users complain"
            ],
            "correctIndex": 2,
            "explanation": "The decision depends on error severity and impact. High error rates affecting performance warrant draining, but low rates might allow waiting for a maintenance window."
          }
        },
        {
          "id": "step-9",
          "situation": "Error rate is high enough to impact all multi-node jobs. Draining recommended.",
          "task": "Drain dgx-03 for transceiver replacement.",
          "expectedCommands": [
            "scontrol update nodename=dgx-03 state=drain reason='IB transceiver replacement'"
          ],
          "hints": [
            "scontrol drain",
            "Reason: IB transceiver replacement",
            "Allow jobs to migrate"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "drain|dgx-03"
          }
        },
        {
          "id": "step-10",
          "situation": "Node drained. After transceiver replacement, verify recovery.",
          "task": "After replacement, check that error counters are cleared and stable.",
          "expectedCommands": ["perfquery"],
          "hints": [
            "perfquery for error check",
            "Should be zero or low",
            "Monitor for 10 minutes"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "error|counter"
          }
        }
      ]
    },
    {
      "id": "domain1-fabric-awakening",
      "domain": 1,
      "title": "The Fabric Manager Awakening",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "A brand-new DGX cluster sits idle because Fabric Manager was never started after initial OS installation.",
        "setting": "You are the datacenter engineer responsible for bringing a 4-node DGX H100 cluster online. The OS and drivers are installed, but multi-GPU NVSwitch communication is not working because NVIDIA Fabric Manager has not been configured.",
        "resolution": "Successfully start and configure Fabric Manager, verify NVSwitch communication, and confirm all GPUs can communicate across the NVSwitch fabric."
      },
      "commandFamilies": ["gpu-monitoring", "diagnostics", "bmc-hardware"],
      "estimatedMinutes": 20,
      "steps": [
        {
          "id": "step-1",
          "situation": "The DGX H100 cluster has been racked and the OS is installed, but users report that multi-GPU jobs fail with NVLink errors.",
          "task": "Check the current status of the NVIDIA Fabric Manager service to see if it is running.",
          "expectedCommands": ["systemctl status nvidia-fabricmanager"],
          "hints": [
            "Use systemctl to check service status",
            "Look for active/inactive state",
            "Fabric Manager manages NVSwitch"
          ],
          "validation": {
            "type": "command",
            "command": "systemctl",
            "pattern": "nvidia-fabricmanager|status"
          },
          "quiz": {
            "question": "What is the primary purpose of NVIDIA Fabric Manager?",
            "options": [
              "Managing InfiniBand network fabric",
              "Coordinating NVSwitch-based GPU communication",
              "Installing GPU driver updates automatically",
              "Monitoring GPU temperatures across the cluster"
            ],
            "correctIndex": 1,
            "explanation": "NVIDIA Fabric Manager manages NVSwitch-based multi-GPU communication in DGX and HGX systems, enabling GPUs to communicate across NVSwitch fabric for high-bandwidth GPU-to-GPU transfers."
          }
        },
        {
          "id": "step-2",
          "situation": "Fabric Manager service is inactive (dead). It was never enabled after initial installation.",
          "task": "Start the NVIDIA Fabric Manager service and check that it starts successfully.",
          "expectedCommands": ["systemctl start nvidia-fabricmanager"],
          "hints": [
            "systemctl start to launch the service",
            "Check status after starting",
            "Look for any error messages"
          ],
          "validation": {
            "type": "command",
            "command": "systemctl",
            "pattern": "start|nvidia-fabricmanager"
          }
        },
        {
          "id": "step-3",
          "situation": "The service start command returned, but you need to verify it is actually running without errors.",
          "task": "Check the Fabric Manager journal logs to confirm successful initialization.",
          "expectedCommands": ["journalctl -u nvidia-fabricmanager"],
          "hints": [
            "journalctl -u for unit-specific logs",
            "Look for initialization success messages",
            "Check for NVSwitch detection entries"
          ],
          "validation": {
            "type": "command",
            "command": "journalctl",
            "pattern": "nvidia-fabricmanager|fabric"
          }
        },
        {
          "id": "step-4",
          "situation": "Logs show Fabric Manager detected all NVSwitches. Time to verify GPU visibility.",
          "task": "Run nvidia-smi to verify all GPUs are detected and check their current state.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi shows all GPUs",
            "Look for 8 H100 GPUs per node",
            "Check for any ERR! entries"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|H100"
          }
        },
        {
          "id": "step-5",
          "situation": "All 8 GPUs are visible. Now verify the NVIDIA kernel modules are properly loaded.",
          "task": "Check that all required NVIDIA kernel modules are loaded in the correct order.",
          "expectedCommands": ["lsmod | grep nvidia"],
          "hints": [
            "lsmod shows loaded kernel modules",
            "grep for nvidia to filter",
            "Should see nvidia, nvidia_uvm, nvidia_modeset"
          ],
          "validation": {
            "type": "command",
            "command": "lsmod",
            "pattern": "nvidia"
          },
          "quiz": {
            "question": "Which NVIDIA kernel module is required for NVSwitch fabric communication?",
            "options": [
              "nvidia_uvm",
              "nvidia_drm",
              "nvidia_fabricmanager (user-space, not module)",
              "nvidia_nvswitch"
            ],
            "correctIndex": 2,
            "explanation": "Fabric Manager runs as a user-space service (not a kernel module) that communicates with NVSwitch hardware through the nvidia kernel driver. The nvidia base module handles the hardware interface."
          }
        },
        {
          "id": "step-6",
          "situation": "Kernel modules look correct. Need to verify the driver version matches firmware expectations.",
          "task": "Check detailed driver and firmware version information using modinfo.",
          "expectedCommands": ["modinfo nvidia"],
          "hints": [
            "modinfo nvidia shows driver details",
            "Check version field",
            "Verify firmware compatibility"
          ],
          "validation": {
            "type": "command",
            "command": "modinfo",
            "pattern": "nvidia|version"
          }
        },
        {
          "id": "step-7",
          "situation": "Driver version is confirmed compatible. Check for any kernel-level GPU messages.",
          "task": "Check dmesg for any NVIDIA-related warnings or errors during boot and initialization.",
          "expectedCommands": ["dmesg | grep -i nvidia"],
          "hints": [
            "dmesg shows kernel messages",
            "grep -i for case-insensitive search",
            "Look for warnings or errors"
          ],
          "validation": {
            "type": "command",
            "command": "dmesg",
            "pattern": "nvidia|NVIDIA"
          }
        },
        {
          "id": "step-8",
          "situation": "No kernel errors found. Perform a final comprehensive GPU state check.",
          "task": "Run nvidia-smi -q to get detailed GPU status including NVLink and fabric information.",
          "expectedCommands": ["nvidia-smi -q"],
          "hints": [
            "nvidia-smi -q for detailed query",
            "Check NVLink section for all GPUs",
            "Verify fabric manager status field"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-q|query|NVLink"
          },
          "quiz": {
            "question": "What happens to multi-GPU jobs if Fabric Manager is not running on an NVSwitch system?",
            "options": [
              "Jobs run normally but slightly slower",
              "GPUs cannot communicate across NVSwitch, causing failures or fallback to PCIe",
              "Only single-GPU jobs are affected",
              "The system automatically starts Fabric Manager when needed"
            ],
            "correctIndex": 1,
            "explanation": "Without Fabric Manager, NVSwitch-based GPU-to-GPU communication is not available. Multi-GPU jobs either fail or fall back to much slower PCIe paths, severely degrading performance."
          }
        }
      ]
    },
    {
      "id": "domain1-network-bonding-blues",
      "domain": 1,
      "title": "Network Bonding Blues",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "New cluster nodes have Ethernet bonding misconfigured and InfiniBand ports sitting idle after rack installation.",
        "setting": "You are setting up network connectivity on freshly racked DGX nodes. Management network bonding needs verification and the InfiniBand HCAs must be configured for high-speed compute fabric.",
        "resolution": "Correctly verify network bonding configuration, bring up InfiniBand ports, and validate MST tools for firmware management."
      },
      "commandFamilies": ["infiniband-tools", "bmc-hardware", "diagnostics"],
      "estimatedMinutes": 20,
      "steps": [
        {
          "id": "step-1",
          "situation": "The new DGX nodes are racked and cabled, but the management network is intermittently dropping packets.",
          "task": "Check the network interfaces to see the current link states and IP configuration.",
          "expectedCommands": ["ip link show"],
          "hints": [
            "ip link show lists all interfaces",
            "Look for bond interfaces",
            "Check UP/DOWN state of members"
          ],
          "validation": {
            "type": "command",
            "command": "ip",
            "pattern": "link|show"
          }
        },
        {
          "id": "step-2",
          "situation": "You see bond0 exists but one member interface is DOWN. Need to check IP addressing.",
          "task": "Check IP address assignments to verify the management network configuration.",
          "expectedCommands": ["ip addr"],
          "hints": [
            "ip addr shows IP addresses",
            "Check bond0 has correct IP",
            "Verify subnet mask"
          ],
          "validation": {
            "type": "command",
            "command": "ip",
            "pattern": "addr"
          },
          "quiz": {
            "question": "What is the primary benefit of network bonding for management interfaces?",
            "options": [
              "Doubles the network speed",
              "Provides link redundancy and failover",
              "Reduces network latency by half",
              "Required for IPMI access"
            ],
            "correctIndex": 1,
            "explanation": "Network bonding provides link redundancy so that if one physical interface fails, the management network remains accessible through the surviving interface."
          }
        },
        {
          "id": "step-3",
          "situation": "IP configuration looks correct on bond0. Need to verify the bonding mode and slave status.",
          "task": "Check the bonding driver status to verify mode and member interface health.",
          "expectedCommands": ["cat /proc/net/bonding/bond0"],
          "hints": [
            "The bonding status file shows details",
            "Check bonding mode (active-backup, 802.3ad)",
            "Verify both slaves are up"
          ],
          "validation": {
            "type": "command",
            "pattern": "bonding|bond0"
          }
        },
        {
          "id": "step-4",
          "situation": "Bond is in active-backup mode with one slave down. Management network fixed. Now check InfiniBand.",
          "task": "Check InfiniBand HCA status to see if the high-speed ports are linked up.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows HCA port status",
            "Look for Active or Down state",
            "Check link width and rate"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Active|Rate"
          },
          "quiz": {
            "question": "What is the key difference between InfiniBand and Ethernet for HPC interconnects?",
            "options": [
              "InfiniBand uses fiber optic only",
              "InfiniBand provides lower latency and RDMA support natively",
              "Ethernet supports higher bandwidth",
              "InfiniBand works without switches"
            ],
            "correctIndex": 1,
            "explanation": "InfiniBand provides significantly lower latency and native RDMA (Remote Direct Memory Access) support, making it ideal for tightly-coupled HPC and AI distributed training workloads."
          }
        },
        {
          "id": "step-5",
          "situation": "InfiniBand ports are Active. Now initialize the Mellanox Software Tools for firmware management.",
          "task": "Start the MST (Mellanox Software Tools) driver to enable firmware management.",
          "expectedCommands": ["mst start"],
          "hints": [
            "mst start loads MST drivers",
            "Required for firmware queries",
            "May need root privileges"
          ],
          "validation": {
            "type": "command",
            "command": "mst",
            "pattern": "start"
          }
        },
        {
          "id": "step-6",
          "situation": "MST driver loaded. Verify the MST devices are detected correctly.",
          "task": "Check MST status to see all detected Mellanox/NVIDIA devices.",
          "expectedCommands": ["mst status"],
          "hints": [
            "mst status shows detected devices",
            "Look for HCA device paths",
            "Note /dev/mst/ device names"
          ],
          "validation": {
            "type": "command",
            "command": "mst",
            "pattern": "status"
          },
          "quiz": {
            "question": "What is the purpose of MST (Mellanox Software Tools)?",
            "options": [
              "Monitoring network traffic throughput",
              "Providing device-level access for firmware and configuration management",
              "Testing InfiniBand cable quality",
              "Managing Subnet Manager routing tables"
            ],
            "correctIndex": 1,
            "explanation": "MST provides low-level device access for firmware updates, configuration queries, and hardware diagnostics on Mellanox/NVIDIA network adapters."
          }
        },
        {
          "id": "step-7",
          "situation": "MST shows two ConnectX-7 HCA devices. Need to verify firmware configuration.",
          "task": "Query the HCA firmware configuration using mlxconfig.",
          "expectedCommands": ["mlxconfig -d /dev/mst/mt41686_pciconf0 q"],
          "hints": [
            "mlxconfig -d <device> q queries config",
            "Check link type settings",
            "Verify SRIOV and other features"
          ],
          "validation": {
            "type": "command",
            "command": "mlxconfig",
            "pattern": "query|config|-d"
          }
        },
        {
          "id": "step-8",
          "situation": "HCA configuration looks correct. Perform final firmware version verification.",
          "task": "Query the installed firmware versions on all HCAs to ensure they are up to date.",
          "expectedCommands": ["mlxfwmanager --query"],
          "hints": [
            "mlxfwmanager --query shows firmware versions",
            "Compare to latest recommended version",
            "Check all devices"
          ],
          "validation": {
            "type": "command",
            "command": "mlxfwmanager",
            "pattern": "query|firmware|version"
          }
        }
      ]
    },
    {
      "id": "domain1-bios-verification",
      "domain": 1,
      "title": "The BIOS Verification",
      "difficulty": "beginner",
      "narrative": {
        "hook": "Before powering on for production workloads, the new servers need their UEFI/BIOS settings and firmware validated against the deployment checklist.",
        "setting": "You are a junior systems engineer performing first-boot validation on new compute nodes. Your task is to verify BIOS settings, CPU configuration, and kernel/driver readiness before handing off to the cluster team.",
        "resolution": "Verify all BIOS settings match the deployment standard, confirm CPU and memory configuration, and validate that the NVIDIA driver is properly installed."
      },
      "commandFamilies": ["bmc-hardware", "gpu-monitoring", "diagnostics"],
      "estimatedMinutes": 15,
      "steps": [
        {
          "id": "step-1",
          "type": "concept",
          "situation": "Before running any commands, your mentor explains the data source behind dmidecode — the primary tool for this verification.",
          "task": "Understanding DMI/SMBIOS",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Desktop Management Interface (DMI) and System Management BIOS (SMBIOS) are standards that define data structures in a system's BIOS containing information about hardware components.\n\ndmidecode reads this information in human-readable format and is essential for:\n- Verifying hardware configuration after deployment\n- Checking BIOS/firmware versions\n- Identifying memory modules and specifications\n- Gathering serial numbers for support tickets\n\nCommon DMI Types:\n- Type 0: BIOS Information\n- Type 1: System Information\n- Type 2: Baseboard Information\n- Type 4: Processor Information\n- Type 17: Memory Device",
          "tips": [
            "dmidecode requires root privileges on most systems",
            "The -t flag filters by DMI type — e.g. dmidecode -t memory"
          ]
        },
        {
          "id": "step-2",
          "type": "concept",
          "situation": "Before querying the hardware tables, your mentor explains the data standard that makes hardware introspection possible on any x86 server.",
          "task": "Learn about the DMI/SMBIOS table types and which type numbers correspond to which hardware components.",
          "expectedCommands": [],
          "hints": [],
          "validation": {
            "type": "none"
          },
          "conceptContent": "Desktop Management Interface (DMI) tables store hardware metadata in the system BIOS. The dmidecode command reads these tables. Key DMI types: Type 0 = BIOS information, Type 1 = System info (manufacturer, serial), Type 2 = Baseboard, Type 4 = Processor, Type 17 = Memory devices, Type 38 = IPMI device. On DGX systems, dmidecode confirms correct hardware identification, BIOS version, and memory configuration.",
          "tips": [
            "dmidecode requires root privileges",
            "Use -t flag to query specific types"
          ]
        },
        {
          "id": "step-3",
          "situation": "The new compute node has booted for the first time. You need to verify the BIOS firmware version and vendor information.",
          "task": "Use dmidecode to read the BIOS information table and verify the firmware version.",
          "expectedCommands": ["dmidecode -t bios"],
          "hints": [
            "dmidecode -t bios shows BIOS info",
            "Check vendor and version fields",
            "Note the release date"
          ],
          "validation": {
            "type": "command",
            "command": "dmidecode",
            "pattern": "bios|BIOS"
          },
          "quiz": {
            "question": "What is the key difference between UEFI and Legacy BIOS?",
            "options": [
              "UEFI only works with NVIDIA hardware",
              "UEFI supports larger disks, secure boot, and faster initialization",
              "Legacy BIOS is more secure than UEFI",
              "There is no practical difference for servers"
            ],
            "correctIndex": 1,
            "explanation": "UEFI provides support for disks larger than 2TB (GPT), Secure Boot for firmware integrity, faster initialization, and a richer pre-boot environment compared to Legacy BIOS."
          }
        },
        {
          "id": "step-4",
          "situation": "BIOS version matches the deployment checklist. Now verify the system identity.",
          "task": "Check the system information table to verify manufacturer, model, and serial number.",
          "expectedCommands": ["dmidecode -t system"],
          "hints": [
            "dmidecode -t system shows system info",
            "Verify manufacturer and product name",
            "Record serial number for inventory"
          ],
          "validation": {
            "type": "command",
            "command": "dmidecode",
            "pattern": "system|manufacturer|product"
          }
        },
        {
          "id": "step-5",
          "situation": "System identification confirmed. Need to verify the UEFI boot configuration.",
          "task": "Check the EFI boot manager to verify boot order and secure boot status.",
          "expectedCommands": ["efibootmgr -v"],
          "hints": [
            "efibootmgr -v shows verbose boot entries",
            "Check BootOrder for correct sequence",
            "Verify boot entries exist"
          ],
          "validation": {
            "type": "command",
            "command": "efibootmgr",
            "pattern": "Boot|boot|EFI"
          }
        },
        {
          "id": "step-6",
          "situation": "Boot configuration verified. Time to check the CPU configuration.",
          "task": "Examine the CPU configuration to verify model, core count, and NUMA topology.",
          "expectedCommands": ["lscpu"],
          "hints": [
            "lscpu shows detailed CPU information",
            "Check model name and core count",
            "Note NUMA node configuration"
          ],
          "validation": {
            "type": "command",
            "command": "lscpu",
            "pattern": "CPU|core|NUMA"
          },
          "quiz": {
            "question": "Why is NUMA topology important for GPU workloads?",
            "options": [
              "NUMA determines GPU clock speeds",
              "CPU-GPU affinity affects PCIe and memory access latency",
              "NUMA is only relevant for CPU-only workloads",
              "GPU drivers require specific NUMA settings"
            ],
            "correctIndex": 1,
            "explanation": "Proper NUMA awareness ensures that processes use CPUs and memory that are physically close to their assigned GPUs, minimizing PCIe traversal and memory access latency."
          }
        },
        {
          "id": "step-7",
          "situation": "CPU and NUMA topology look correct. Verify the PCIe device topology next.",
          "task": "Check the PCIe device tree to see all connected devices including GPUs.",
          "expectedCommands": ["lspci -tv"],
          "hints": [
            "lspci -tv shows tree view",
            "Look for NVIDIA GPU entries",
            "Verify PCIe bridge hierarchy"
          ],
          "validation": {
            "type": "command",
            "command": "lspci",
            "pattern": "pci|NVIDIA|tree"
          }
        },
        {
          "id": "step-8",
          "situation": "PCIe topology shows all GPU slots populated. Verify system memory next.",
          "task": "Check the total system memory and verify it matches the deployment specification.",
          "expectedCommands": ["free -h"],
          "hints": [
            "free -h shows memory in human readable format",
            "Check total memory matches spec",
            "Note available vs used memory"
          ],
          "validation": {
            "type": "command",
            "command": "free",
            "pattern": "Mem|total|free"
          }
        },
        {
          "id": "step-9",
          "situation": "System memory matches spec. Now verify the running kernel version.",
          "task": "Check the kernel version to ensure it matches the approved version for GPU drivers.",
          "expectedCommands": ["uname -r"],
          "hints": [
            "uname -r shows kernel release",
            "Must match driver compatibility",
            "Check against deployment checklist"
          ],
          "validation": {
            "type": "command",
            "command": "uname",
            "pattern": "kernel|release|-r"
          }
        },
        {
          "id": "step-10",
          "situation": "Kernel version is correct. Final step: verify the NVIDIA driver installation.",
          "task": "Check the installed NVIDIA driver version to confirm it matches the cluster standard.",
          "expectedCommands": ["cat /proc/driver/nvidia/version"],
          "hints": [
            "The proc filesystem shows driver info",
            "Check both kernel and user-space versions",
            "Should match cluster standard"
          ],
          "validation": {
            "type": "command",
            "pattern": "nvidia|version|driver"
          },
          "quiz": {
            "question": "Which dmidecode table type provides BIOS/UEFI firmware information?",
            "options": [
              "dmidecode -t 0 (BIOS)",
              "dmidecode -t 1 (System)",
              "dmidecode -t 2 (Baseboard)",
              "dmidecode -t 4 (Processor)"
            ],
            "correctIndex": 0,
            "explanation": "DMI table type 0 contains BIOS information including vendor, version, and release date. Type 1 is system info, type 2 is baseboard, and type 4 is processor."
          }
        }
      ]
    },
    {
      "id": "domain3-job-scheduler",
      "domain": 3,
      "title": "The Job Scheduler Setup",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "A research institution just received their first GPU cluster, and the job scheduler needs to be configured from scratch for GPU-aware workload management.",
        "setting": "You are the HPC administrator tasked with configuring Slurm for GPU workloads on a new 8-node DGX A100 cluster. GRES, partitions, and accounting all need to be set up and validated.",
        "resolution": "Successfully configure Slurm GRES for GPU scheduling, validate partitions, and confirm that GPU jobs can be submitted and tracked."
      },
      "commandFamilies": ["cluster-tools", "gpu-monitoring", "diagnostics"],
      "estimatedMinutes": 25,
      "steps": [
        {
          "id": "step-1",
          "situation": "Slurm is installed on the cluster but GPU-aware scheduling is not yet configured. Users cannot request GPUs in their jobs.",
          "task": "Check the detailed node information to understand current resource availability.",
          "expectedCommands": ["sinfo -Nel"],
          "hints": [
            "sinfo -Nel shows detailed node list",
            "Check GRES column for GPU info",
            "Note current partition setup"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "-N|-l|node"
          }
        },
        {
          "id": "step-2",
          "situation": "sinfo shows nodes but no GRES (GPU) resources listed. Configuration needed.",
          "task": "Review the current Slurm configuration to understand existing settings.",
          "expectedCommands": ["scontrol show config"],
          "hints": [
            "scontrol show config shows all settings",
            "Look for GresTypes line",
            "Check AccountingStorageType"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show config|configuration"
          },
          "quiz": {
            "question": "What does GRES stand for in Slurm terminology?",
            "options": [
              "GPU Resource Extension System",
              "Generic RESource",
              "Graphics Rendering Execution Scheduler",
              "Group Resource Environment Setting"
            ],
            "correctIndex": 1,
            "explanation": "GRES stands for Generic RESource, a Slurm mechanism for managing consumable resources like GPUs, MICs, or any custom hardware beyond CPUs and memory."
          }
        },
        {
          "id": "step-3",
          "situation": "Configuration shows GresTypes=gpu is defined but partitions lack GPU resource allocation.",
          "task": "Check the current partition configuration for GPU resource specifications.",
          "expectedCommands": ["scontrol show partition"],
          "hints": [
            "scontrol show partition shows all partitions",
            "Check TRESBillingWeights for GPU",
            "Verify AllowedGRES settings"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show partition|partition"
          }
        },
        {
          "id": "step-4",
          "situation": "Partitions exist but GRES is not properly mapped. Need to verify the gres.conf file.",
          "task": "Examine the Slurm GRES configuration file to verify GPU device mappings.",
          "expectedCommands": ["cat /etc/slurm/gres.conf"],
          "hints": [
            "gres.conf maps GPUs to devices",
            "Check NodeName and device paths",
            "Verify GPU count per node"
          ],
          "validation": {
            "type": "command",
            "pattern": "gres.conf|gres|GPU"
          }
        },
        {
          "id": "step-5",
          "situation": "GRES configuration file updated with correct GPU mappings. Time to test GPU job submission.",
          "task": "Submit a test job that requests one GPU to verify GRES scheduling works.",
          "expectedCommands": ["sbatch --gres=gpu:1"],
          "hints": [
            "sbatch --gres=gpu:N requests GPUs",
            "Start with 1 GPU to test",
            "Check job ID returned"
          ],
          "validation": {
            "type": "command",
            "command": "sbatch",
            "pattern": "gres|gpu"
          },
          "quiz": {
            "question": "What happens if you submit a job with --gres=gpu:1 but GRES is not configured?",
            "options": [
              "Job runs without GPU access",
              "Job is queued forever in PENDING state",
              "Slurm automatically detects GPUs",
              "Job fails immediately with an error"
            ],
            "correctIndex": 1,
            "explanation": "Without GRES configured, Slurm cannot match the GPU resource request to any node, so the job stays in PENDING state indefinitely with reason 'ReqNodeNotAvail' or 'Resources'."
          }
        },
        {
          "id": "step-6",
          "situation": "Test job submitted successfully. Need to verify it is running with GPU allocated.",
          "task": "Check the job queue to see the status of the submitted GPU test job.",
          "expectedCommands": ["squeue"],
          "hints": [
            "squeue shows all jobs",
            "Look for RUNNING state",
            "Check NODELIST assignment"
          ],
          "validation": {
            "type": "command",
            "command": "squeue",
            "pattern": "RUNNING|JOBID"
          }
        },
        {
          "id": "step-7",
          "situation": "Job is running. Need to verify accounting is tracking GPU usage properly.",
          "task": "Check job accounting data to verify GPU resource tracking is working.",
          "expectedCommands": ["sacct -j"],
          "hints": [
            "sacct -j JOBID shows accounting",
            "Check AllocTRES for gpu count",
            "Verify elapsed time tracking"
          ],
          "validation": {
            "type": "command",
            "command": "sacct",
            "pattern": "job|TRES|gpu"
          }
        },
        {
          "id": "step-8",
          "situation": "Accounting shows GPU allocation. Verify user associations are configured for GPU billing.",
          "task": "Check Slurm accounting associations to ensure GPU billing is set up.",
          "expectedCommands": ["sacctmgr show assoc"],
          "hints": [
            "sacctmgr show assoc lists associations",
            "Check GrpTRES limits",
            "Verify account structure"
          ],
          "validation": {
            "type": "command",
            "command": "sacctmgr",
            "pattern": "show|assoc|account"
          }
        },
        {
          "id": "step-9",
          "situation": "Accounting and associations configured. Verify individual node GPU configuration.",
          "task": "Check detailed node information to confirm GRES is properly reported.",
          "expectedCommands": ["scontrol show node"],
          "hints": [
            "scontrol show node shows details",
            "Look for Gres= and GresUsed=",
            "Verify gpu:8 on each node"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show node|Gres"
          },
          "quiz": {
            "question": "Why is it important to configure Slurm partitions with GPU resources?",
            "options": [
              "GPUs won't work without partitions",
              "Partitions enable fair scheduling, resource limits, and access control for GPU resources",
              "NVIDIA requires partition-based scheduling",
              "Partitions improve GPU performance"
            ],
            "correctIndex": 1,
            "explanation": "Slurm partitions allow administrators to set resource limits, access controls, and scheduling priorities, ensuring fair GPU allocation across research groups and workloads."
          }
        },
        {
          "id": "step-10",
          "situation": "Node GRES configuration verified. Run a final end-to-end test with GPU validation.",
          "task": "Run an interactive job that requests a GPU and runs nvidia-smi to validate the complete pipeline.",
          "expectedCommands": ["srun --gres=gpu:1 nvidia-smi"],
          "hints": [
            "srun for interactive execution",
            "Request 1 GPU with --gres",
            "nvidia-smi should show the allocated GPU"
          ],
          "validation": {
            "type": "command",
            "command": "srun",
            "pattern": "gres|gpu|nvidia-smi"
          }
        }
      ]
    },
    {
      "id": "domain3-container-orchestration",
      "domain": 3,
      "title": "Container Orchestration",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "The AI team needs containerized PyTorch and TensorFlow environments, but the cluster has never run GPU containers before.",
        "setting": "You are setting up the container infrastructure on a DGX cluster for the first time. Docker, Enroot, and the NVIDIA Container Toolkit all need to be verified and integrated with Slurm.",
        "resolution": "Successfully configure and validate GPU container support with Docker, Enroot, and the NVIDIA Container Toolkit for production AI workloads."
      },
      "commandFamilies": ["container-tools", "gpu-monitoring", "cluster-tools"],
      "estimatedMinutes": 20,
      "steps": [
        {
          "id": "step-1",
          "situation": "The cluster has Docker installed but GPU container support has never been tested.",
          "task": "Check Docker daemon information to verify it is running and see runtime configuration.",
          "expectedCommands": ["docker info"],
          "hints": [
            "docker info shows daemon details",
            "Look for Runtime section",
            "Check for nvidia runtime entry"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "info|runtime"
          },
          "quiz": {
            "question": "What is the NVIDIA Container Toolkit's role in GPU containers?",
            "options": [
              "It installs GPU drivers inside containers",
              "It provides a container runtime hook that mounts GPU drivers and devices into containers",
              "It manages container networking for GPUs",
              "It compiles CUDA code for container environments"
            ],
            "correctIndex": 1,
            "explanation": "The NVIDIA Container Toolkit provides a runtime hook (nvidia-container-runtime) that automatically mounts the host GPU drivers, CUDA libraries, and device nodes into containers."
          }
        },
        {
          "id": "step-2",
          "situation": "Docker is running with nvidia runtime available. Time to pull an NGC container image.",
          "task": "Pull the official NVIDIA PyTorch container from NGC (NVIDIA GPU Cloud).",
          "expectedCommands": ["docker pull nvcr.io/nvidia/pytorch"],
          "hints": [
            "nvcr.io is the NGC registry",
            "Pull takes time for large images",
            "Check for authentication if needed"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "pull|nvcr|pytorch"
          }
        },
        {
          "id": "step-3",
          "situation": "PyTorch container pulled successfully. Test GPU access inside the container.",
          "task": "Run the container with GPU access to verify GPUs are visible inside.",
          "expectedCommands": ["docker run --rm --gpus all"],
          "hints": [
            "docker run --rm --gpus all",
            "Run nvidia-smi inside container",
            "Should show all host GPUs"
          ],
          "validation": {
            "type": "command",
            "command": "docker",
            "pattern": "run|gpus all"
          },
          "quiz": {
            "question": "What does NGC (NVIDIA GPU Cloud) provide for AI workflows?",
            "options": [
              "Free cloud GPU instances for training",
              "Pre-built, optimized container images for AI frameworks and tools",
              "GPU driver updates via cloud sync",
              "Cloud-based GPU monitoring dashboards"
            ],
            "correctIndex": 1,
            "explanation": "NGC provides pre-built, tested, and optimized container images for popular AI frameworks like PyTorch, TensorFlow, and specialized tools, saving significant setup time."
          }
        },
        {
          "id": "step-4",
          "situation": "Docker GPU access works. Now set up Enroot for HPC container workflows.",
          "task": "Import a container image into Enroot format for use with Slurm.",
          "expectedCommands": ["enroot import"],
          "hints": [
            "enroot import converts Docker images",
            "Creates .sqsh squashfs file",
            "More efficient for HPC workloads"
          ],
          "validation": {
            "type": "command",
            "command": "enroot",
            "pattern": "import"
          }
        },
        {
          "id": "step-5",
          "situation": "Container imported as squashfs image. Create an Enroot container from it.",
          "task": "Create an Enroot container from the imported image for job execution.",
          "expectedCommands": ["enroot create"],
          "hints": [
            "enroot create from .sqsh file",
            "Creates container root filesystem",
            "Name it for easy reference"
          ],
          "validation": {
            "type": "command",
            "command": "enroot",
            "pattern": "create"
          }
        },
        {
          "id": "step-6",
          "situation": "Container created. Verify it appears in the Enroot container list.",
          "task": "List available Enroot containers to confirm the creation was successful.",
          "expectedCommands": ["enroot list"],
          "hints": [
            "enroot list shows containers",
            "Should see the new container",
            "Note the container name"
          ],
          "validation": {
            "type": "command",
            "command": "enroot",
            "pattern": "list"
          },
          "quiz": {
            "question": "Why is Enroot preferred over Docker for HPC container workloads?",
            "options": [
              "Enroot has better GPU support",
              "Enroot runs unprivileged and integrates with Slurm via Pyxis",
              "Docker cannot run on HPC clusters",
              "Enroot containers are smaller"
            ],
            "correctIndex": 1,
            "explanation": "Enroot runs containers without root privileges and integrates natively with Slurm through the Pyxis plugin, making it suitable for multi-user HPC environments where Docker's root requirement is a security concern."
          }
        },
        {
          "id": "step-7",
          "situation": "Enroot containers are ready. Test running a container job through Slurm with Pyxis.",
          "task": "Submit a Slurm job that uses a container image to run nvidia-smi inside the container.",
          "expectedCommands": ["srun --container-image"],
          "hints": [
            "srun --container-image= specifies image",
            "Pyxis plugin handles container launch",
            "GPU should be visible inside"
          ],
          "validation": {
            "type": "command",
            "command": "srun",
            "pattern": "container|image"
          }
        },
        {
          "id": "step-8",
          "situation": "Slurm container integration works. Perform a final validation of the container toolkit.",
          "task": "Check the nvidia-container-cli to verify GPU device and driver information.",
          "expectedCommands": ["nvidia-container-cli info"],
          "hints": [
            "nvidia-container-cli info shows runtime details",
            "Verifies driver compatibility",
            "Lists available GPU devices"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-container-cli",
            "pattern": "info|driver|device"
          }
        }
      ]
    },
    {
      "id": "domain3-storage-architect",
      "domain": 3,
      "title": "The Storage Architect",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "Training jobs are completing 40% slower than expected, and initial investigation points to storage as the bottleneck.",
        "setting": "You are troubleshooting storage performance on a cluster that uses both Lustre parallel filesystem and NFS. Data loading is not keeping up with GPU compute, and you need to diagnose and optimize the storage configuration.",
        "resolution": "Identify Lustre striping misconfiguration and NFS mount options causing poor performance, then optimize both for AI training workloads."
      },
      "commandFamilies": ["cluster-tools", "diagnostics", "bmc-hardware"],
      "estimatedMinutes": 20,
      "steps": [
        {
          "id": "step-1",
          "situation": "Users report that large model training takes 40% longer than benchmarks suggest. GPU utilization drops periodically during data loading phases.",
          "task": "Check the Lustre filesystem capacity and health to identify any obvious issues.",
          "expectedCommands": ["lfs df -h"],
          "hints": [
            "lfs df -h shows filesystem usage",
            "Check each OST capacity",
            "Look for imbalanced or full OSTs"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "df"
          },
          "quiz": {
            "question": "What is Lustre's primary advantage over NFS for AI training data?",
            "options": [
              "Better security features",
              "Parallel I/O across multiple storage targets for higher aggregate bandwidth",
              "Simpler configuration and management",
              "Built-in data compression"
            ],
            "correctIndex": 1,
            "explanation": "Lustre stripes data across multiple OSTs (Object Storage Targets), enabling parallel I/O that provides much higher aggregate bandwidth than single-server NFS for large sequential reads."
          }
        },
        {
          "id": "step-2",
          "situation": "Lustre shows some OSTs near capacity while others have plenty of space. Possible imbalance.",
          "task": "Verify the health of Lustre servers to ensure all are responding properly.",
          "expectedCommands": ["lfs check servers"],
          "hints": [
            "lfs check servers pings all servers",
            "All MDT and OST servers should respond",
            "Note any timeouts"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "check|servers"
          }
        },
        {
          "id": "step-3",
          "situation": "All Lustre servers are healthy. The issue may be in how files are striped across OSTs.",
          "task": "Check the stripe configuration of the training data directory.",
          "expectedCommands": ["lfs getstripe"],
          "hints": [
            "lfs getstripe shows file layout",
            "Check stripe count and size",
            "Compare to optimal settings"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "getstripe|stripe"
          },
          "quiz": {
            "question": "What does Lustre stripe count control?",
            "options": [
              "The number of backup copies of each file",
              "The number of OSTs a file's data is distributed across",
              "The maximum file size allowed",
              "The number of concurrent readers"
            ],
            "correctIndex": 1,
            "explanation": "Stripe count determines how many OSTs a file's data is spread across. Higher stripe counts enable parallel I/O from more storage servers, increasing aggregate bandwidth for large files."
          }
        },
        {
          "id": "step-4",
          "situation": "Training data directory has stripe count of 1 - all data on a single OST. This is the bottleneck.",
          "task": "Set optimal Lustre striping for the training data directory to use all available OSTs.",
          "expectedCommands": ["lfs setstripe"],
          "hints": [
            "lfs setstripe sets stripe params",
            "Use -c -1 for all OSTs",
            "Set appropriate stripe size"
          ],
          "validation": {
            "type": "command",
            "command": "lfs",
            "pattern": "setstripe|stripe"
          }
        },
        {
          "id": "step-5",
          "situation": "Lustre striping fixed for new files. Now check the NFS-mounted home directories.",
          "task": "Check NFS client statistics to see if NFS is also contributing to performance issues.",
          "expectedCommands": ["nfsstat -c"],
          "hints": [
            "nfsstat -c shows client stats",
            "Look for retransmissions",
            "Check RPC statistics"
          ],
          "validation": {
            "type": "command",
            "command": "nfsstat",
            "pattern": "client|-c|nfs"
          }
        },
        {
          "id": "step-6",
          "situation": "NFS client stats show elevated retransmissions. Check mount options.",
          "task": "Check the NFS mount options to see if they are optimized for performance.",
          "expectedCommands": ["nfsstat -m"],
          "hints": [
            "nfsstat -m shows mount options",
            "Check rsize and wsize values",
            "Verify NFS version (v4 preferred)"
          ],
          "validation": {
            "type": "command",
            "command": "nfsstat",
            "pattern": "-m|mount"
          }
        },
        {
          "id": "step-7",
          "situation": "NFS mount options show suboptimal rsize/wsize. Also check current NFS mounts.",
          "task": "View all NFS mounts to verify correct servers and export paths.",
          "expectedCommands": ["mount | grep nfs"],
          "hints": [
            "mount shows all filesystems",
            "grep nfs to filter",
            "Check server addresses"
          ],
          "validation": {
            "type": "command",
            "pattern": "mount|nfs"
          }
        },
        {
          "id": "step-8",
          "situation": "NFS mounts identified. Perform final filesystem capacity check across all types.",
          "task": "Check disk usage across all mounted filesystems to get a complete storage picture.",
          "expectedCommands": ["df -hT"],
          "hints": [
            "df -hT shows filesystem types",
            "Compare Lustre and NFS usage",
            "Check for any full filesystems"
          ],
          "validation": {
            "type": "command",
            "command": "df",
            "pattern": "Filesystem|Type|Size"
          },
          "quiz": {
            "question": "Why might AI training jobs use both Lustre and NFS on the same cluster?",
            "options": [
              "Redundancy in case one fails",
              "Lustre for high-bandwidth training data, NFS for home directories and code",
              "They provide the same performance",
              "NFS is used only for logging"
            ],
            "correctIndex": 1,
            "explanation": "Lustre provides high-bandwidth parallel I/O ideal for large training datasets, while NFS is simpler and sufficient for smaller files like code, configs, and user home directories."
          }
        }
      ]
    },
    {
      "id": "domain4-nccl-championship",
      "domain": 4,
      "title": "The NCCL Championship",
      "difficulty": "advanced",
      "narrative": {
        "hook": "The new 16-node DGX H100 cluster needs to pass NCCL all-reduce benchmarks before production sign-off.",
        "setting": "You are the validation engineer responsible for running and optimizing NCCL collective communication benchmarks across the entire cluster. The acceptance criteria require 90% of theoretical bandwidth.",
        "resolution": "Tune NCCL environment variables, optimize transport selection, and achieve target bandwidth across all nodes."
      },
      "commandFamilies": ["gpu-monitoring", "infiniband-tools", "diagnostics"],
      "estimatedMinutes": 25,
      "steps": [
        {
          "id": "step-1",
          "situation": "The cluster is assembled and basic tests pass, but you need to validate the NCCL communication library is properly installed.",
          "task": "Check that the NCCL library is installed and verify its version.",
          "expectedCommands": ["ldconfig -p | grep libnccl"],
          "hints": [
            "ldconfig -p lists shared libraries",
            "grep for libnccl to find NCCL",
            "Note the version number"
          ],
          "validation": {
            "type": "command",
            "command": "ldconfig",
            "pattern": "nccl|libnccl"
          }
        },
        {
          "id": "step-2",
          "situation": "NCCL 2.18 is installed. Time to run the initial all-reduce benchmark on a single node.",
          "task": "Run the all_reduce_perf benchmark to establish single-node baseline performance.",
          "expectedCommands": ["all_reduce_perf -b 8 -e 128M"],
          "hints": [
            "all_reduce_perf is the NCCL benchmark",
            "-b is begin size, -e is end size",
            "Look at bus bandwidth column"
          ],
          "validation": {
            "type": "command",
            "command": "all_reduce_perf",
            "pattern": "reduce|bandwidth|-b|-e"
          },
          "quiz": {
            "question": "What does all-reduce do in distributed training?",
            "options": [
              "Reduces the number of GPUs used",
              "Sums gradients across all GPUs and distributes the result to all",
              "Compresses model weights to reduce memory",
              "Reduces training time by skipping iterations"
            ],
            "correctIndex": 1,
            "explanation": "All-reduce is a collective operation that sums (or reduces) data across all participating GPUs and distributes the result back to every GPU, essential for synchronizing gradients in distributed training."
          }
        },
        {
          "id": "step-3",
          "situation": "Single-node benchmark shows good NVLink bandwidth. Need to enable NCCL debug logging for multi-node tests.",
          "task": "Set NCCL debug environment variable to get detailed transport information.",
          "expectedCommands": ["NCCL_DEBUG=INFO"],
          "hints": [
            "NCCL_DEBUG=INFO enables logging",
            "Shows transport selection details",
            "Useful for diagnosing issues"
          ],
          "validation": {
            "type": "command",
            "pattern": "NCCL_DEBUG|INFO"
          }
        },
        {
          "id": "step-4",
          "situation": "Debug logging configured. Verify InfiniBand transport is not accidentally disabled.",
          "task": "Check and set the NCCL InfiniBand transport environment variable.",
          "expectedCommands": ["NCCL_IB_DISABLE=0"],
          "hints": [
            "NCCL_IB_DISABLE=0 enables IB",
            "Set to 1 would force TCP fallback",
            "IB is critical for performance"
          ],
          "validation": {
            "type": "command",
            "pattern": "NCCL_IB|DISABLE"
          },
          "quiz": {
            "question": "Why is InfiniBand preferred over TCP for NCCL multi-node communication?",
            "options": [
              "TCP is not supported by NCCL",
              "InfiniBand provides RDMA with lower latency and higher bandwidth",
              "TCP requires more GPU memory",
              "InfiniBand works without a switch"
            ],
            "correctIndex": 1,
            "explanation": "InfiniBand with RDMA (Remote Direct Memory Access) bypasses the CPU for data transfers, providing significantly lower latency and higher bandwidth than TCP/IP for GPU-to-GPU communication."
          }
        },
        {
          "id": "step-5",
          "situation": "IB transport enabled. Also verify peer-to-peer GPU communication is enabled.",
          "task": "Check the NCCL P2P (peer-to-peer) configuration for intra-node communication.",
          "expectedCommands": ["NCCL_P2P_DISABLE=0"],
          "hints": [
            "NCCL_P2P_DISABLE=0 enables P2P",
            "P2P uses NVLink for intra-node",
            "Important for single-node perf"
          ],
          "validation": {
            "type": "command",
            "pattern": "NCCL_P2P|DISABLE"
          }
        },
        {
          "id": "step-6",
          "situation": "All NCCL settings configured. Verify the complete NCCL environment before the big test.",
          "task": "Display all current NCCL-related environment variables to confirm the configuration.",
          "expectedCommands": ["env | grep NCCL"],
          "hints": [
            "env shows all environment variables",
            "grep NCCL to filter",
            "Verify all settings are correct"
          ],
          "validation": {
            "type": "command",
            "pattern": "env|NCCL"
          }
        },
        {
          "id": "step-7",
          "situation": "Environment is configured. Time for the full multi-node NCCL benchmark.",
          "task": "Run the multi-node all-reduce benchmark across all 16 nodes using MPI.",
          "expectedCommands": ["mpirun -np 16 -H node1,node2 all_reduce_perf"],
          "hints": [
            "mpirun launches across nodes",
            "-np for number of processes",
            "-H specifies hosts"
          ],
          "validation": {
            "type": "command",
            "command": "mpirun",
            "pattern": "all_reduce|perf|node"
          }
        },
        {
          "id": "step-8",
          "situation": "Multi-node benchmark shows 85% of theoretical bandwidth. Need to check GPU topology for optimization.",
          "task": "Check GPU topology to verify optimal NVLink and PCIe affinity for NCCL.",
          "expectedCommands": ["nvidia-smi topo -m"],
          "hints": [
            "nvidia-smi topo -m shows connections",
            "Check for NVLink between all GPUs",
            "Verify CPU affinity"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "topo|-m|NV"
          }
        },
        {
          "id": "step-9",
          "situation": "Topology shows good NVLink connectivity. Check InfiniBand fabric for any issues.",
          "task": "Verify InfiniBand port status on a representative node.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows port status",
            "Verify NDR 400 Gb/s rate",
            "All ports should be Active"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Active|Rate"
          }
        },
        {
          "id": "step-10",
          "situation": "All checks pass. After tuning NCCL tree/ring algorithms, bandwidth reaches 92%. Document results.",
          "task": "Capture final GPU and topology state for the acceptance documentation.",
          "expectedCommands": ["nvidia-smi topo -m"],
          "hints": [
            "Document topology matrix",
            "Record benchmark results",
            "Include NCCL settings used"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "topo"
          },
          "quiz": {
            "question": "What are the two main NCCL algorithms for all-reduce?",
            "options": [
              "Broadcast and scatter",
              "Ring and tree",
              "Direct and relay",
              "Mesh and star"
            ],
            "correctIndex": 1,
            "explanation": "NCCL uses ring and tree algorithms for all-reduce. Ring provides optimal bandwidth utilization, while tree provides lower latency for smaller messages. NCCL automatically selects the best algorithm."
          }
        }
      ]
    },
    {
      "id": "domain4-linpack-showdown",
      "domain": 4,
      "title": "Linpack Showdown",
      "difficulty": "advanced",
      "narrative": {
        "hook": "The cluster acceptance test requires hitting 85% of theoretical peak FLOPS on the HPL benchmark, and the first run achieved only 65%.",
        "setting": "You are the performance engineer tasked with running and optimizing HPL (High Performance Linpack) on a DGX H100 cluster for the formal acceptance test. The customer will not accept the system until performance targets are met.",
        "resolution": "Diagnose NUMA binding issues and thermal throttling, optimize HPL parameters, and achieve the required performance target."
      },
      "commandFamilies": ["gpu-monitoring", "diagnostics", "cluster-tools"],
      "estimatedMinutes": 25,
      "steps": [
        {
          "id": "step-1",
          "situation": "First HPL run achieved only 65% of theoretical peak. Need to understand the HPL configuration.",
          "task": "Examine the HPL input file to check problem size and process grid configuration.",
          "expectedCommands": ["cat HPL.dat"],
          "hints": [
            "HPL.dat contains benchmark parameters",
            "Check N (problem size)",
            "Verify P x Q process grid"
          ],
          "validation": {
            "type": "command",
            "pattern": "HPL.dat|HPL"
          },
          "quiz": {
            "question": "Why does HPL problem size (N) affect benchmark performance?",
            "options": [
              "Larger N uses more network bandwidth",
              "N must be a prime number for best performance",
              "Larger N increases compute-to-communication ratio, improving efficiency",
              "Problem size has no significant effect"
            ],
            "correctIndex": 2,
            "explanation": "Larger problem sizes increase the ratio of computation to communication, reducing the relative overhead of data exchange and improving overall efficiency up to memory limits."
          }
        },
        {
          "id": "step-2",
          "situation": "HPL.dat shows a modest problem size. Need to monitor GPU behavior during the benchmark.",
          "task": "Start GPU device monitoring to watch utilization, temperature, and power during the run.",
          "expectedCommands": ["nvidia-smi dmon"],
          "hints": [
            "nvidia-smi dmon for live monitoring",
            "Watch temperature and power columns",
            "Look for throttling indicators"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "dmon|monitor"
          }
        },
        {
          "id": "step-3",
          "situation": "Monitoring shows periodic power throttling on some GPUs. Need detailed DCGM stats.",
          "task": "Enable DCGM statistics collection for detailed GPU performance analysis.",
          "expectedCommands": ["dcgmi stats --enable"],
          "hints": [
            "dcgmi stats --enable starts collection",
            "Will track performance metrics",
            "Run before benchmark"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "stats|enable"
          }
        },
        {
          "id": "step-4",
          "situation": "DCGM stats collection enabled. Check current statistics baseline.",
          "task": "View the DCGM statistics to see baseline GPU performance metrics.",
          "expectedCommands": ["dcgmi stats"],
          "hints": [
            "dcgmi stats shows collected data",
            "Check compute utilization",
            "Look for memory bandwidth usage"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "stats"
          },
          "quiz": {
            "question": "What is the relationship between Rmax and Rpeak in HPL?",
            "options": [
              "Rmax is always greater than Rpeak",
              "Rpeak is theoretical maximum, Rmax is measured achievement",
              "They are the same metric in different units",
              "Rmax measures memory, Rpeak measures compute"
            ],
            "correctIndex": 1,
            "explanation": "Rpeak is the theoretical peak FLOPS based on hardware specs, while Rmax is the maximum achieved FLOPS during the HPL benchmark. The ratio Rmax/Rpeak indicates system efficiency."
          }
        },
        {
          "id": "step-5",
          "situation": "Baseline stats captured. Check NUMA hardware topology to optimize process placement.",
          "task": "Check the NUMA hardware configuration to plan optimal HPL process binding.",
          "expectedCommands": ["numactl --hardware"],
          "hints": [
            "numactl --hardware shows NUMA topology",
            "Note CPU and memory per node",
            "Plan GPU-to-NUMA affinity"
          ],
          "validation": {
            "type": "command",
            "command": "numactl",
            "pattern": "hardware|node|cpu"
          }
        },
        {
          "id": "step-6",
          "situation": "NUMA topology understood. Ready to run an optimized HPL with proper process binding.",
          "task": "Launch the HPL benchmark with MPI and proper NUMA binding.",
          "expectedCommands": ["mpirun -np 8 ./hpl"],
          "hints": [
            "mpirun for multi-process launch",
            "Add --bind-to for CPU binding",
            "Match processes to GPU count"
          ],
          "validation": {
            "type": "command",
            "command": "mpirun",
            "pattern": "hpl|np"
          }
        },
        {
          "id": "step-7",
          "situation": "HPL running with better binding. Monitor GPU performance states during the benchmark.",
          "task": "Check GPU performance state and throttle reasons during the active benchmark.",
          "expectedCommands": ["nvidia-smi -q -d PERFORMANCE"],
          "hints": [
            "nvidia-smi -q -d PERFORMANCE for perf data",
            "Check for throttle reasons",
            "Note clock speeds"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "PERFORMANCE|-d|throttle"
          },
          "quiz": {
            "question": "What is the most common reason GPUs underperform during HPL?",
            "options": [
              "Insufficient GPU memory",
              "Network bandwidth limitations",
              "Thermal or power throttling reducing clock speeds",
              "Incompatible CUDA version"
            ],
            "correctIndex": 2,
            "explanation": "HPL is extremely compute-intensive, pushing GPUs to maximum power consumption. Thermal or power limits cause the GPU to reduce clock speeds (throttle), directly reducing FLOPS."
          }
        },
        {
          "id": "step-8",
          "situation": "Some throttling observed. Run gpu-burn to stress test thermal limits separately.",
          "task": "Run gpu-burn to validate GPU thermal performance under sustained full load.",
          "expectedCommands": ["gpu-burn"],
          "hints": [
            "gpu-burn applies sustained GPU load",
            "Monitor temperatures during test",
            "Check for thermal throttling"
          ],
          "validation": {
            "type": "command",
            "command": "gpu-burn",
            "pattern": "burn|test"
          }
        },
        {
          "id": "step-9",
          "situation": "Thermal analysis complete. Check all DCGM stats after the benchmark runs.",
          "task": "Review final DCGM statistics to document performance analysis.",
          "expectedCommands": ["dcgmi stats"],
          "hints": [
            "dcgmi stats for final data",
            "Compare to baseline",
            "Document improvements"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "stats"
          }
        },
        {
          "id": "step-10",
          "situation": "After optimization, HPL achieves 87% of Rpeak. Final verification needed.",
          "task": "Run nvidia-smi to capture final GPU state for acceptance documentation.",
          "expectedCommands": ["nvidia-smi"],
          "hints": [
            "nvidia-smi for final snapshot",
            "Document all GPU states",
            "Include in acceptance report"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "GPU|Power|Temp"
          }
        }
      ]
    },
    {
      "id": "domain4-health-inspector",
      "domain": 4,
      "title": "The Health Inspector",
      "difficulty": "intermediate",
      "narrative": {
        "hook": "Quarterly GPU health audit is due, and every GPU in the 8-node cluster must pass comprehensive DCGM diagnostics.",
        "setting": "You are running the quarterly health validation using DCGM (Data Center GPU Manager). Every GPU must pass all diagnostic levels, and you need to set up proper monitoring groups and health watches.",
        "resolution": "Complete a full DCGM diagnostic sweep across all nodes, set up monitoring groups, and document the health status of every GPU."
      },
      "commandFamilies": ["diagnostics", "gpu-monitoring", "infiniband-tools"],
      "estimatedMinutes": 20,
      "steps": [
        {
          "id": "step-1",
          "situation": "Quarterly health audit begins. First, discover all GPUs that DCGM can manage on this node.",
          "task": "Use DCGM to list all GPU devices available for diagnostics.",
          "expectedCommands": ["dcgmi discovery -l"],
          "hints": [
            "dcgmi discovery -l lists GPUs",
            "Should show all 8 GPUs",
            "Note device IDs"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "discovery|-l|list"
          }
        },
        {
          "id": "step-2",
          "situation": "All 8 GPUs discovered. Now check compute capabilities and detailed GPU information.",
          "task": "Run DCGM discovery in compute mode to see GPU capabilities.",
          "expectedCommands": ["dcgmi discovery -c"],
          "hints": [
            "dcgmi discovery -c for compute info",
            "Shows CUDA compute capability",
            "Verifies GPU type"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "discovery|-c|compute"
          },
          "quiz": {
            "question": "What are the three diagnostic levels in DCGM?",
            "options": [
              "Basic, Standard, Premium",
              "Quick (1), Medium (2), Extended (3)",
              "Hardware, Software, Network",
              "Level A, Level B, Level C"
            ],
            "correctIndex": 1,
            "explanation": "DCGM provides three diagnostic levels: Quick (level 1) for fast checks, Medium (level 2) for moderate stress tests, and Extended (level 3) for comprehensive hardware validation."
          }
        },
        {
          "id": "step-3",
          "situation": "GPU capabilities confirmed. Start with the quick diagnostic level.",
          "task": "Run DCGM level 1 (quick) diagnostics on all GPUs.",
          "expectedCommands": ["dcgmi diag -r 1"],
          "hints": [
            "dcgmi diag -r 1 for quick test",
            "Takes about 30 seconds",
            "All GPUs should pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 1"
          }
        },
        {
          "id": "step-4",
          "situation": "Quick diagnostics pass. Proceed to medium-level stress test.",
          "task": "Run DCGM level 2 (medium) diagnostics for more thorough testing.",
          "expectedCommands": ["dcgmi diag -r 2"],
          "hints": [
            "dcgmi diag -r 2 for medium test",
            "Takes several minutes",
            "Includes memory stress tests"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 2"
          }
        },
        {
          "id": "step-5",
          "situation": "Medium diagnostics pass. Now run the full extended diagnostic sweep.",
          "task": "Run DCGM level 3 (extended) diagnostics for comprehensive hardware validation.",
          "expectedCommands": ["dcgmi diag -r 3"],
          "hints": [
            "dcgmi diag -r 3 for full test",
            "Takes 10-15 minutes",
            "Tests PCIe, memory, compute"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 3"
          },
          "quiz": {
            "question": "What does DCGM level 3 diagnostics test that level 1 does not?",
            "options": [
              "GPU temperature reading",
              "Extended memory tests, PCIe bandwidth, and sustained compute stress",
              "Driver version verification",
              "Network connectivity"
            ],
            "correctIndex": 1,
            "explanation": "Level 3 includes comprehensive tests like targeted stress testing of GPU memory, PCIe bandwidth verification, sustained compute tests, and diagnostic pattern detection that level 1 skips for speed."
          }
        },
        {
          "id": "step-6",
          "situation": "Extended diagnostics complete. Set up DCGM health monitoring for ongoing checks.",
          "task": "Configure DCGM health watches for continuous GPU monitoring.",
          "expectedCommands": ["dcgmi health -c"],
          "hints": [
            "dcgmi health -c configures watches",
            "Enables background monitoring",
            "Catches issues between audits"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "health|-c|watch"
          }
        },
        {
          "id": "step-7",
          "situation": "Health watches configured. Now create a GPU group for organized monitoring.",
          "task": "Create a DCGM group for the production GPUs to manage them as a unit.",
          "expectedCommands": ["dcgmi group -c"],
          "hints": [
            "dcgmi group -c creates a group",
            "Name it descriptively",
            "Groups organize GPU management"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "group|-c|create"
          }
        },
        {
          "id": "step-8",
          "situation": "Group created. List existing groups to verify it was created correctly.",
          "task": "List all DCGM groups to verify the new group exists.",
          "expectedCommands": ["dcgmi group -l"],
          "hints": [
            "dcgmi group -l lists all groups",
            "Should see the new group",
            "Note the group ID"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "group|-l|list"
          },
          "quiz": {
            "question": "Why use DCGM groups for GPU management?",
            "options": [
              "Groups make GPUs run faster",
              "Groups enable bulk operations like diagnostics, monitoring, and policy on multiple GPUs at once",
              "DCGM requires groups to function",
              "Groups reduce GPU memory usage"
            ],
            "correctIndex": 1,
            "explanation": "DCGM groups allow administrators to perform bulk operations like running diagnostics, setting health watches, and applying policies across multiple GPUs simultaneously."
          }
        },
        {
          "id": "step-9",
          "situation": "Group listed. Add all GPUs to the monitoring group.",
          "task": "Add GPU devices to the production monitoring group.",
          "expectedCommands": ["dcgmi group -a"],
          "hints": [
            "dcgmi group -a adds GPUs",
            "Specify group ID and GPU IDs",
            "Add all 8 GPUs"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "group|-a|add"
          }
        },
        {
          "id": "step-10",
          "situation": "GPUs added to group. Start continuous monitoring for the audit report.",
          "task": "Start DCGM device monitoring to collect real-time metrics for the audit.",
          "expectedCommands": ["dcgmi dmon"],
          "hints": [
            "dcgmi dmon for live monitoring",
            "Shows temp, power, utilization",
            "Capture data for report"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "dmon|monitor"
          }
        }
      ]
    },
    {
      "id": "domain4-fabric-pressure",
      "domain": 4,
      "title": "Fabric Under Pressure",
      "difficulty": "advanced",
      "narrative": {
        "hook": "The InfiniBand fabric validation tests are failing intermittently on the new 32-node cluster, blocking production deployment.",
        "setting": "You are the fabric validation engineer for a large cluster deployment. The InfiniBand fabric must pass comprehensive tests including bandwidth, latency, and error-free operation before the cluster can go live.",
        "resolution": "Identify and resolve cable errors, validate full fabric bandwidth, and certify the InfiniBand fabric for production use."
      },
      "commandFamilies": ["infiniband-tools", "diagnostics", "gpu-monitoring"],
      "estimatedMinutes": 25,
      "steps": [
        {
          "id": "step-1",
          "situation": "New 32-node cluster with NDR InfiniBand fabric needs validation before going live.",
          "task": "Discover all hosts connected to the InfiniBand fabric.",
          "expectedCommands": ["ibhosts"],
          "hints": [
            "ibhosts lists all HCA nodes",
            "Should show all 32 nodes",
            "Check for missing entries"
          ],
          "validation": {
            "type": "command",
            "command": "ibhosts",
            "pattern": "host|HCA"
          }
        },
        {
          "id": "step-2",
          "situation": "ibhosts shows all 32 nodes. Verify the switch infrastructure next.",
          "task": "Discover all InfiniBand switches in the fabric topology.",
          "expectedCommands": ["ibswitches"],
          "hints": [
            "ibswitches lists all switches",
            "Check switch count matches design",
            "Note switch GUIDs"
          ],
          "validation": {
            "type": "command",
            "command": "ibswitches",
            "pattern": "switch|Switch"
          },
          "quiz": {
            "question": "What is the typical InfiniBand topology for large GPU clusters?",
            "options": [
              "Star topology with one central switch",
              "Fat-tree topology with leaf and spine switches",
              "Ring topology connecting all nodes",
              "Mesh topology with direct connections"
            ],
            "correctIndex": 1,
            "explanation": "Large GPU clusters typically use fat-tree topology with leaf switches (connecting to hosts) and spine switches (connecting leaf switches), providing full bisection bandwidth."
          }
        },
        {
          "id": "step-3",
          "situation": "All switches detected. Map the complete fabric topology.",
          "task": "Discover the full fabric topology to understand the connection map.",
          "expectedCommands": ["ibnetdiscover"],
          "hints": [
            "ibnetdiscover maps full topology",
            "Shows connections between nodes and switches",
            "Verify no missing links"
          ],
          "validation": {
            "type": "command",
            "command": "ibnetdiscover",
            "pattern": "discover|topology"
          }
        },
        {
          "id": "step-4",
          "situation": "Topology mapped. Check for any ports reporting errors across the fabric.",
          "task": "Check all ports in the fabric for accumulated errors.",
          "expectedCommands": ["ibporterrors"],
          "hints": [
            "ibporterrors scans all ports",
            "Flags ports with error counters",
            "Critical for fabric health"
          ],
          "validation": {
            "type": "command",
            "command": "ibporterrors",
            "pattern": "error|port"
          },
          "quiz": {
            "question": "Which port error type is most indicative of a physical cable problem?",
            "options": [
              "PortXmitWait (congestion)",
              "SymbolErrorCounter (signal integrity)",
              "VL15Dropped (management traffic)",
              "PortRcvSwitchRelayErrors (routing)"
            ],
            "correctIndex": 1,
            "explanation": "SymbolErrorCounter indicates bit-level signal integrity problems in the physical layer, typically caused by damaged cables, dirty connectors, or failing transceivers."
          }
        },
        {
          "id": "step-5",
          "situation": "A few ports flagged with errors. Run bandwidth tests on suspicious links.",
          "task": "Run an InfiniBand write bandwidth test between two nodes to validate link performance.",
          "expectedCommands": ["ib_write_bw"],
          "hints": [
            "ib_write_bw tests write bandwidth",
            "Run server on one end, client on other",
            "Compare to expected NDR bandwidth"
          ],
          "validation": {
            "type": "command",
            "command": "ib_write_bw",
            "pattern": "write|bandwidth|bw"
          }
        },
        {
          "id": "step-6",
          "situation": "Write bandwidth test shows good throughput. Now test read bandwidth.",
          "task": "Run an InfiniBand read bandwidth test to validate bidirectional performance.",
          "expectedCommands": ["ib_read_bw"],
          "hints": [
            "ib_read_bw tests read bandwidth",
            "Should be similar to write",
            "Asymmetry indicates issues"
          ],
          "validation": {
            "type": "command",
            "command": "ib_read_bw",
            "pattern": "read|bandwidth|bw"
          }
        },
        {
          "id": "step-7",
          "situation": "Bandwidth tests pass. Check extended performance counters for deeper analysis.",
          "task": "Query extended performance counters on a port with elevated errors.",
          "expectedCommands": ["perfquery -x"],
          "hints": [
            "perfquery -x for extended counters",
            "Shows 64-bit counters",
            "More accurate than basic counters"
          ],
          "validation": {
            "type": "command",
            "command": "perfquery",
            "pattern": "-x|extended"
          },
          "quiz": {
            "question": "What is the difference between ib_write_bw and ib_read_bw tests?",
            "options": [
              "They test different cable types",
              "Write tests RDMA write, read tests RDMA read - testing different DMA engines",
              "Write is faster than read by design",
              "They test the same thing from different perspectives"
            ],
            "correctIndex": 1,
            "explanation": "ib_write_bw uses RDMA Write operations while ib_read_bw uses RDMA Read operations. These exercise different DMA engines in the HCA, and asymmetric results can indicate hardware issues."
          }
        },
        {
          "id": "step-8",
          "situation": "Extended counters show one link with elevated error rate. Check all links systematically.",
          "task": "Check link information across all switch ports to find degraded connections.",
          "expectedCommands": ["iblinkinfo -l"],
          "hints": [
            "iblinkinfo -l shows link details",
            "Check width and speed for each",
            "Look for reduced link width"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "-l|link|width"
          }
        },
        {
          "id": "step-9",
          "situation": "One link running at reduced width found. Run comprehensive fabric diagnostics.",
          "task": "Run the ibdiagnet comprehensive fabric diagnostic tool.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet runs full fabric test",
            "Checks topology, errors, routing",
            "Creates detailed report"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|fabric"
          }
        },
        {
          "id": "step-10",
          "situation": "ibdiagnet identified the problematic cable. Check for any cable-specific error reporting.",
          "task": "Query cable error information to document the failing cable for replacement.",
          "expectedCommands": ["ibcableerrors"],
          "hints": [
            "ibcableerrors shows cable-specific data",
            "Document cable location",
            "Note port and GUID for replacement"
          ],
          "validation": {
            "type": "command",
            "command": "ibcableerrors",
            "pattern": "cable|error"
          }
        }
      ]
    },
    {
      "id": "domain4-cluster-certification",
      "domain": 4,
      "title": "The Cluster Certification",
      "difficulty": "advanced",
      "narrative": {
        "hook": "The 64-GPU DGX SuperPOD must pass the formal certification checklist before the customer accepts delivery.",
        "setting": "You are leading the acceptance testing for a major DGX SuperPOD deployment. Every subsystem must be validated: GPUs, NVLink, InfiniBand, storage, and job scheduler. Failure means delayed delivery and penalties.",
        "resolution": "Systematically validate all cluster subsystems, document test results, and certify the cluster for production AI workloads."
      },
      "commandFamilies": [
        "diagnostics",
        "gpu-monitoring",
        "infiniband-tools",
        "cluster-tools"
      ],
      "estimatedMinutes": 30,
      "steps": [
        {
          "id": "step-1",
          "situation": "Certification day. Start with comprehensive GPU inventory and health status across all nodes.",
          "task": "Run detailed GPU query to verify all GPUs are present and reporting correctly.",
          "expectedCommands": ["nvidia-smi -q"],
          "hints": [
            "nvidia-smi -q for detailed info",
            "Check each GPU's ECC status",
            "Verify driver and firmware versions"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "-q|query|GPU"
          },
          "quiz": {
            "question": "What is the first thing to verify in a cluster acceptance test?",
            "options": [
              "Network bandwidth",
              "Complete hardware inventory matches the purchase order",
              "Software version compatibility",
              "User access permissions"
            ],
            "correctIndex": 1,
            "explanation": "The first step in acceptance testing is verifying that all purchased hardware is present and detected correctly, including GPU count, memory size, and model matching specifications."
          }
        },
        {
          "id": "step-2",
          "situation": "All 64 GPUs accounted for across 8 nodes. Run comprehensive GPU diagnostics.",
          "task": "Run DCGM level 3 extended diagnostics on all GPUs to validate hardware health.",
          "expectedCommands": ["dcgmi diag -r 3"],
          "hints": [
            "dcgmi diag -r 3 for full test",
            "Run on each node",
            "All tests must pass"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "diag|-r 3"
          }
        },
        {
          "id": "step-3",
          "situation": "GPU diagnostics pass. Now validate the InfiniBand fabric.",
          "task": "Run comprehensive InfiniBand fabric diagnostics to validate the network.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet for full fabric test",
            "Check for errors and warnings",
            "Document any issues"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|fabric"
          },
          "quiz": {
            "question": "Why must InfiniBand validation be part of GPU cluster certification?",
            "options": [
              "InfiniBand is optional for GPU clusters",
              "Multi-node GPU training depends entirely on inter-node fabric performance",
              "It is only needed for non-GPU workloads",
              "InfiniBand affects GPU clock speeds"
            ],
            "correctIndex": 1,
            "explanation": "Distributed AI training relies on high-bandwidth, low-latency inter-node communication. InfiniBand fabric performance directly determines multi-node scaling efficiency."
          }
        },
        {
          "id": "step-4",
          "situation": "Fabric diagnostics clean. Verify the Slurm job scheduler sees all resources.",
          "task": "Check detailed Slurm node information to verify all compute resources are registered.",
          "expectedCommands": ["sinfo -Nel"],
          "hints": [
            "sinfo -Nel for detailed node list",
            "Verify GPU GRES on each node",
            "All nodes should be idle"
          ],
          "validation": {
            "type": "command",
            "command": "sinfo",
            "pattern": "-N|-l|GRES"
          }
        },
        {
          "id": "step-5",
          "situation": "Slurm shows all 8 nodes with 8 GPUs each. Verify individual node configuration.",
          "task": "Check detailed node configuration to verify GRES and feature settings.",
          "expectedCommands": ["scontrol show node"],
          "hints": [
            "scontrol show node shows details",
            "Verify Gres=gpu:8",
            "Check Features field"
          ],
          "validation": {
            "type": "command",
            "command": "scontrol",
            "pattern": "show node"
          }
        },
        {
          "id": "step-6",
          "situation": "Slurm configuration verified. Run NCCL all-reduce benchmark for multi-node validation.",
          "task": "Run the NCCL all-reduce performance test across all nodes to validate communication.",
          "expectedCommands": ["all_reduce_perf"],
          "hints": [
            "all_reduce_perf across all nodes",
            "Check bus bandwidth results",
            "Must meet acceptance criteria"
          ],
          "validation": {
            "type": "command",
            "command": "all_reduce_perf",
            "pattern": "reduce|perf|bandwidth"
          }
        },
        {
          "id": "step-7",
          "situation": "NCCL tests pass. Run sustained GPU stress test for thermal validation.",
          "task": "Run gpu-burn to verify sustained GPU compute under thermal load.",
          "expectedCommands": ["gpu-burn"],
          "hints": [
            "gpu-burn for sustained stress",
            "Run for at least 10 minutes",
            "Monitor for thermal throttling"
          ],
          "validation": {
            "type": "command",
            "command": "gpu-burn",
            "pattern": "burn|stress"
          },
          "quiz": {
            "question": "Why is sustained stress testing important for cluster certification?",
            "options": [
              "It makes GPUs faster over time",
              "Thermal and power issues only manifest under sustained full load",
              "It validates the software installation",
              "Stress testing is optional for certification"
            ],
            "correctIndex": 1,
            "explanation": "Some hardware issues like thermal throttling, power delivery problems, and marginal components only manifest under sustained full load, making stress testing essential for reliability validation."
          }
        },
        {
          "id": "step-8",
          "situation": "Stress test complete with no thermal issues. Configure health monitoring for production.",
          "task": "Set up DCGM health watches for ongoing production monitoring.",
          "expectedCommands": ["dcgmi health -c"],
          "hints": [
            "dcgmi health -c configures watches",
            "Set for all GPUs",
            "Enables proactive monitoring"
          ],
          "validation": {
            "type": "command",
            "command": "dcgmi",
            "pattern": "health|-c"
          }
        },
        {
          "id": "step-9",
          "situation": "Health monitoring configured. Final check on InfiniBand port errors after all tests.",
          "task": "Check InfiniBand port errors one final time after all testing is complete.",
          "expectedCommands": ["ibporterrors"],
          "hints": [
            "ibporterrors after stress tests",
            "Compare to pre-test baseline",
            "No new errors expected"
          ],
          "validation": {
            "type": "command",
            "command": "ibporterrors",
            "pattern": "error|port"
          }
        },
        {
          "id": "step-10",
          "situation": "All tests pass. Final NVLink verification for complete documentation.",
          "task": "Check NVLink status across all GPUs for the final certification report.",
          "expectedCommands": ["nvidia-smi nvlink -s"],
          "hints": [
            "nvidia-smi nvlink -s for status",
            "All links should be active",
            "Document for acceptance report"
          ],
          "validation": {
            "type": "command",
            "command": "nvidia-smi",
            "pattern": "nvlink|-s|status"
          }
        }
      ]
    },
    {
      "id": "domain5-cable-detective",
      "domain": 5,
      "title": "Cable Detective",
      "difficulty": "advanced",
      "narrative": {
        "hook": "Random training failures and link flaps traced to physical layer issues in the InfiniBand fabric, but which cable is the culprit?",
        "setting": "You are troubleshooting intermittent InfiniBand link failures that are causing distributed training jobs to fail. The fabric has hundreds of cables and you need to identify the specific failing cable and connection.",
        "resolution": "Systematically trace the failing link through the fabric, identify the degraded cable using diagnostic tools, and validate the replacement."
      },
      "commandFamilies": ["infiniband-tools", "diagnostics", "bmc-hardware"],
      "estimatedMinutes": 25,
      "steps": [
        {
          "id": "step-1",
          "situation": "InfiniBand link flaps have been reported on the monitoring dashboard. Jobs are failing during multi-node communication phases.",
          "task": "Check the local InfiniBand port status to see if any ports are experiencing issues.",
          "expectedCommands": ["ibstat"],
          "hints": [
            "ibstat shows port status",
            "Check State and PhysicalState",
            "Look for ports not in Active state"
          ],
          "validation": {
            "type": "command",
            "command": "ibstat",
            "pattern": "State|Active|Physical"
          }
        },
        {
          "id": "step-2",
          "situation": "ibstat shows all local ports Active but one port keeps flapping. Need to see the full fabric link status.",
          "task": "SSH to dgx-05 and check link information to identify degraded connections.",
          "expectedCommands": ["ssh dgx-05", "iblinkinfo"],
          "hints": [
            "SSH to dgx-05 to examine its link from the local perspective",
            "iblinkinfo shows all fabric links",
            "Look for reduced width or speed",
            "Note port and GUID of issues"
          ],
          "validation": {
            "type": "command",
            "command": "iblinkinfo",
            "pattern": "link|width|speed"
          },
          "autoFaults": [
            {
              "nodeId": "dgx-05",
              "gpuId": 3,
              "type": "thermal",
              "severity": "warning",
              "parameters": {
                "targetTemp": 92
              }
            }
          ],
          "quiz": {
            "question": "What are the common types of InfiniBand cables used in datacenter clusters?",
            "options": [
              "Cat6 and Cat7 copper cables",
              "DAC (Direct Attach Copper), AOC (Active Optical), and fiber with transceivers",
              "Coaxial and twisted pair",
              "USB-C and Thunderbolt"
            ],
            "correctIndex": 1,
            "explanation": "Datacenter InfiniBand uses DAC for short runs (up to 3m), AOC for medium distances, and fiber optic cables with separate transceivers for longer runs. Each has different failure modes."
          }
        },
        {
          "id": "step-3",
          "situation": "iblinkinfo flags a link between dgx-05 port 2 and leaf switch port 15 with intermittent width changes.",
          "task": "Check cable-specific error information to assess the cable health.",
          "expectedCommands": ["ibcableerrors"],
          "hints": [
            "ibcableerrors shows cable diagnostics",
            "Focus on the flagged connection",
            "High errors indicate cable issues"
          ],
          "validation": {
            "type": "command",
            "command": "ibcableerrors",
            "pattern": "cable|error"
          }
        },
        {
          "id": "step-4",
          "situation": "Cable errors confirm signal integrity degradation on that link. Need deeper diagnostics.",
          "task": "Use mlxlink to get detailed physical layer diagnostics on the suspect port.",
          "expectedCommands": ["mlxlink -d mlx5_0"],
          "hints": [
            "mlxlink -d shows link details",
            "Check eye opening margins",
            "Look for PHY error counters"
          ],
          "validation": {
            "type": "command",
            "command": "mlxlink",
            "pattern": "-d|mlx5|link"
          },
          "quiz": {
            "question": "What does signal integrity mean in the context of InfiniBand cables?",
            "options": [
              "The cable is encrypted for security",
              "The electrical/optical signal quality at the receiver meets minimum thresholds for reliable data transfer",
              "The cable brand is verified as genuine",
              "Signal integrity measures cable length accuracy"
            ],
            "correctIndex": 1,
            "explanation": "Signal integrity refers to the quality of the electrical or optical signal at the receiver. Degraded signal integrity causes bit errors, retransmissions, and ultimately link failures."
          }
        },
        {
          "id": "step-5",
          "situation": "mlxlink shows degraded eye opening margins on the suspect link. Test basic connectivity.",
          "task": "Use ibping to test basic InfiniBand connectivity to the switch port.",
          "expectedCommands": ["ibping"],
          "hints": [
            "ibping tests IB connectivity",
            "Similar to network ping",
            "Check for dropped packets"
          ],
          "validation": {
            "type": "command",
            "command": "ibping",
            "pattern": "ping|latency"
          }
        },
        {
          "id": "step-6",
          "situation": "ibping shows intermittent timeouts. Trace the path through the fabric.",
          "task": "Trace the InfiniBand route from the affected node to another node to map the path.",
          "expectedCommands": ["ibtracert"],
          "hints": [
            "ibtracert traces IB routes",
            "Shows switches in the path",
            "Identify which hop has the issue"
          ],
          "validation": {
            "type": "command",
            "command": "ibtracert",
            "pattern": "trace|route|hop"
          }
        },
        {
          "id": "step-7",
          "situation": "Trace confirms the problematic link. Check Subnet Manager status to ensure routing is stable.",
          "task": "Verify the Subnet Manager is healthy and check for any routing issues.",
          "expectedCommands": ["sminfo"],
          "hints": [
            "sminfo shows SM status",
            "Verify SM is master",
            "Check for SM failover events"
          ],
          "validation": {
            "type": "command",
            "command": "sminfo",
            "pattern": "SM|master|state"
          },
          "quiz": {
            "question": "What happens when the InfiniBand Subnet Manager fails over?",
            "options": [
              "All connections are permanently lost",
              "A standby SM takes over and reconfigures routing tables",
              "GPUs stop working",
              "Manual intervention is always required"
            ],
            "correctIndex": 1,
            "explanation": "InfiniBand supports SM redundancy. When the master SM fails, a standby SM takes over, reconfiguring LID assignments and routing tables. This causes a brief disruption but restores fabric operation."
          }
        },
        {
          "id": "step-8",
          "situation": "SM is stable. Verify the OFED driver stack version for compatibility information.",
          "task": "Check the installed OFED (OpenFabrics Enterprise Distribution) version.",
          "expectedCommands": ["ofed_info"],
          "hints": [
            "ofed_info shows OFED version",
            "Important for compatibility",
            "Document for support ticket"
          ],
          "validation": {
            "type": "command",
            "command": "ofed_info",
            "pattern": "OFED|version|MLNX"
          }
        },
        {
          "id": "step-9",
          "situation": "OFED version documented. Check the PCIe details of the HCA for hardware-level issues.",
          "task": "Verify the HCA PCIe link status to rule out host-side issues.",
          "expectedCommands": ["lspci -vv"],
          "hints": [
            "lspci -vv for detailed PCI info",
            "Check link speed and width",
            "Verify PCIe Gen and lanes"
          ],
          "validation": {
            "type": "command",
            "command": "lspci",
            "pattern": "ConnectX|Mellanox|Width"
          }
        },
        {
          "id": "step-10",
          "situation": "PCIe looks fine. Cable is confirmed as the issue. Run final comprehensive fabric diagnostic.",
          "task": "Run ibdiagnet one final time to create a complete diagnostic report for the cable replacement.",
          "expectedCommands": ["ibdiagnet"],
          "hints": [
            "ibdiagnet for full report",
            "Document before cable swap",
            "Compare after replacement"
          ],
          "validation": {
            "type": "command",
            "command": "ibdiagnet",
            "pattern": "diag|report"
          }
        }
      ]
    }
  ]
}
