{
  "id": "domain4-gpu-reset",
  "title": "GPU Reset and Recovery Procedures",
  "domain": "domain4",
  "difficulty": "intermediate",
  "description": "Learn the proper procedures for resetting GPUs when they become unresponsive or enter a bad state. This includes understanding when to use GPU reset, the different reset methods available, and how to recover from common GPU hang scenarios. GPU reset is a critical skill for maintaining cluster availability.",
  "learningObjectives": [
    "Understand when GPU reset is appropriate vs when reboot is required",
    "Execute GPU reset using nvidia-smi",
    "Configure and use GPU persistence mode",
    "Recover from common GPU hang scenarios (XID 43, 31)",
    "Understand the impact of GPU reset on running processes"
  ],
  "faults": [
    {
      "nodeId": "dgx-01",
      "gpuId": 5,
      "type": "gpu-hang",
      "severity": "critical",
      "parameters": {
        "xidCode": 43,
        "description": "GPU stopped responding"
      }
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Identify GPU Issues",
      "description": "Before attempting a reset, identify which GPU is experiencing issues and understand the nature of the problem.",
      "objectives": [
        "Check GPU status with nvidia-smi",
        "Identify unresponsive or errored GPUs",
        "Check system logs for XID errors",
        "Determine if reset is appropriate"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "nvidia-smi -q -d PAGE_RETIREMENT",
        "dmesg | grep -i xid"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check GPU status",
          "expectedCommands": ["nvidia-smi", "dmesg | grep"]
        }
      ],
      "hints": [
        "Look for 'ERR!' or 'Unknown Error' in nvidia-smi output",
        "XID 43 = GPU stopped responding (reset may help)",
        "XID 79 = GPU fallen off bus (reboot required, reset won't work)",
        "If nvidia-smi hangs, the GPU is likely in a bad state",
        "Check which processes are using the GPU with 'nvidia-smi pmon'"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Reset Documentation",
          "url": "https://docs.nvidia.com/deploy/gpu-reset/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Understand GPU Reset Requirements",
      "description": "GPU reset has specific requirements and limitations. Understand what conditions must be met for a successful reset.",
      "objectives": [
        "Check if GPU supports reset",
        "Verify no processes are using the GPU",
        "Understand reset limitations",
        "Check persistence mode status"
      ],
      "expectedCommands": [
        "nvidia-smi -i 5 -q",
        "nvidia-smi pmon -i 5",
        "nvidia-smi -i 5 -q | grep -i persistence"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify reset requirements",
          "expectedCommands": ["nvidia-smi -i", "nvidia-smi pmon"]
        }
      ],
      "hints": [
        "All processes using the GPU must be terminated before reset",
        "GPU reset is only supported on certain GPU architectures",
        "Persistence mode must be disabled for reset (will be re-enabled after)",
        "Display-attached GPUs may have limitations",
        "NVML must be able to communicate with the GPU",
        "If nvidia-smi is completely hung, reset won't work - reboot needed"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Reset Requirements",
          "url": "https://docs.nvidia.com/deploy/gpu-reset/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Terminate Processes Using GPU",
      "description": "Before resetting, all processes using the target GPU must be terminated to release the GPU resources.",
      "objectives": [
        "List processes using the GPU",
        "Identify process owners",
        "Terminate processes gracefully",
        "Force kill if necessary"
      ],
      "expectedCommands": [
        "nvidia-smi pmon -i 5",
        "nvidia-smi -i 5 --query-compute-apps=pid,process_name,used_memory --format=csv",
        "kill -TERM",
        "kill -9"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check and terminate processes",
          "expectedCommands": ["nvidia-smi pmon", "nvidia-smi --query-compute-apps"]
        }
      ],
      "hints": [
        "Use 'nvidia-smi pmon' to monitor processes in real-time",
        "Use --query-compute-apps for CSV output of process info",
        "First try SIGTERM (kill -15) for graceful shutdown",
        "If process doesn't respond, use SIGKILL (kill -9)",
        "Verify processes are terminated with 'nvidia-smi pmon'",
        "Container processes may require container stop/kill"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Process Management",
          "url": "https://docs.nvidia.com/deploy/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Execute GPU Reset",
      "description": "Perform the GPU reset using nvidia-smi. The reset will reinitialize the GPU hardware and driver state.",
      "objectives": [
        "Execute GPU reset command",
        "Monitor reset progress",
        "Verify reset completion",
        "Check GPU status after reset"
      ],
      "expectedCommands": [
        "nvidia-smi --gpu-reset -i 5",
        "nvidia-smi -i 5 -q"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must execute GPU reset",
          "expectedCommands": ["nvidia-smi --gpu-reset", "nvidia-smi -i"]
        }
      ],
      "hints": [
        "Use 'nvidia-smi --gpu-reset -i GPU_ID' to reset specific GPU",
        "Reset typically takes 5-30 seconds",
        "Output will show 'GPU reset successful' on success",
        "If reset fails, a system reboot may be required",
        "After reset, verify GPU status with 'nvidia-smi'",
        "ECC counters (volatile) will be cleared by reset"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "nvidia-smi GPU Reset",
          "url": "https://developer.nvidia.com/nvidia-system-management-interface"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Configure Persistence Mode",
      "description": "GPU Persistence Mode keeps the NVIDIA driver loaded when no applications are using the GPU, reducing initialization time and preventing some issues.",
      "objectives": [
        "Understand persistence mode benefits",
        "Enable persistence mode",
        "Verify persistence mode is active",
        "Configure persistence for boot"
      ],
      "expectedCommands": [
        "nvidia-smi -pm 1",
        "nvidia-smi -q | grep -i persistence",
        "systemctl status nvidia-persistenced"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must configure persistence mode",
          "expectedCommands": ["nvidia-smi -pm", "nvidia-smi -q | grep"]
        }
      ],
      "hints": [
        "Use 'nvidia-smi -pm 1' to enable persistence mode",
        "Use 'nvidia-smi -pm 0' to disable persistence mode",
        "Persistence mode keeps driver loaded = faster GPU access",
        "Without persistence, driver unloads when GPU idle",
        "nvidia-persistenced daemon provides same functionality",
        "Enabled by default on DGX systems for production use"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Persistence Mode",
          "url": "https://docs.nvidia.com/deploy/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Verify GPU Recovery",
      "description": "After reset, thoroughly verify that the GPU has recovered and is functioning correctly.",
      "objectives": [
        "Verify GPU is responding normally",
        "Run quick diagnostic",
        "Check ECC status",
        "Verify GPU can run workloads"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "nvidia-smi -q -d ECC",
        "dcgmi diag -r 1"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify GPU recovery",
          "expectedCommands": ["nvidia-smi", "dcgmi diag -r 1"]
        }
      ],
      "hints": [
        "nvidia-smi should show normal output without errors",
        "Temperature and power readings should be reasonable",
        "Run 'dcgmi diag -r 1' for quick health check",
        "Volatile ECC counters will be zero after reset",
        "If issues persist after reset, hardware problem is likely",
        "Multiple reset failures = escalate to reboot or RMA"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Health Verification",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "When Reset Won't Work",
      "description": "Understand scenarios where GPU reset is not possible and what alternatives exist.",
      "objectives": [
        "Recognize XID 79 (GPU fallen off bus)",
        "Understand when reboot is required",
        "Know hardware failure indicators",
        "Document for escalation"
      ],
      "expectedCommands": [
        "dmesg | grep -i 'fallen off'",
        "ipmitool sel list",
        "nvsm show health"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must understand reset limitations",
          "expectedCommands": ["dmesg | grep", "ipmitool sel", "nvsm show health"]
        }
      ],
      "hints": [
        "XID 79 = GPU fallen off PCIe bus - REBOOT REQUIRED",
        "If nvidia-smi completely hangs - REBOOT REQUIRED",
        "Multiple reset failures = likely hardware issue",
        "Check ipmitool sel for thermal/power events",
        "Document GPU UUID, XID codes, timestamps for RMA",
        "After reboot, run full diagnostics to verify health"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Troubleshooting",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Identified GPU issues requiring reset",
    "Verified reset requirements and terminated processes",
    "Successfully executed GPU reset",
    "Configured persistence mode",
    "Verified GPU recovery with diagnostics",
    "Understood scenarios where reset won't work"
  ],
  "estimatedTime": 37,
  "prerequisites": [
    "domain1-driver-install"
  ],
  "tags": [
    "gpu-reset",
    "recovery",
    "persistence-mode",
    "xid-43",
    "troubleshooting",
    "intermediate",
    "exam-critical"
  ]
}
