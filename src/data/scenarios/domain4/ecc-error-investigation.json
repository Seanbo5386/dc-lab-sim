{
  "id": "domain4-ecc-investigation",
  "title": "ECC Error Investigation and Row Remapping Analysis",
  "domain": "domain4",
  "difficulty": "advanced",
  "description": "Learn to investigate ECC (Error Correcting Code) errors on NVIDIA GPUs, understand the difference between SRAM and DRAM ECC errors, analyze row remapping status, and determine when GPU replacement is required. This is a critical skill for the NCP-AII certification as ECC errors are one of the most common hardware issues in data center environments.",
  "learningObjectives": [
    "Understand ECC error types: volatile vs aggregate, SRAM vs DRAM",
    "Query and interpret ECC error counters using nvidia-smi",
    "Analyze row remapping status and thresholds",
    "Determine when ECC errors require hardware replacement",
    "Create a monitoring strategy for ECC errors",
    "Understand the relationship between ECC errors and XID codes"
  ],
  "faults": [
    {
      "nodeId": "dgx-02",
      "gpuId": 3,
      "type": "ecc-error",
      "severity": "warning",
      "parameters": {
        "singleBitCount": 47,
        "doubleBitCount": 0,
        "rowRemappingPending": true,
        "rowRemappingFailure": false
      }
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Understand ECC Memory Architecture",
      "description": "Before investigating ECC errors, understand how ECC works on NVIDIA GPUs. GPUs have two types of memory with ECC protection: SRAM (on-chip caches and register files) and DRAM (HBM2/HBM2e high bandwidth memory).",
      "objectives": [
        "Query ECC status on all GPUs",
        "Identify ECC-enabled GPUs",
        "Understand ECC memory types"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ECC",
        "nvidia-smi --query-gpu=ecc.mode.current --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must query ECC status",
          "expectedCommands": ["nvidia-smi -q -d ECC", "nvidia-smi -q"]
        }
      ],
      "hints": [
        "Use 'nvidia-smi -q -d ECC' to display ECC-specific information",
        "ECC Mode shows 'Enabled' or 'Disabled' for each GPU",
        "Data center GPUs (A100, H100) have ECC enabled by default",
        "SRAM ECC protects L1/L2 caches, shared memory, register files",
        "DRAM ECC protects the HBM (High Bandwidth Memory)",
        "ECC can detect and correct single-bit errors, detect double-bit errors"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU ECC Memory",
          "url": "https://docs.nvidia.com/deploy/gpu-memory-errors/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Query ECC Error Counters",
      "description": "NVIDIA GPUs maintain ECC error counters for both volatile (since last driver load) and aggregate (lifetime) errors. Query these counters to understand the error history.",
      "objectives": [
        "View volatile ECC error counts",
        "View aggregate ECC error counts",
        "Understand the difference between volatile and aggregate",
        "Identify which GPUs have errors"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ECC",
        "nvidia-smi --query-gpu=index,ecc.errors.corrected.volatile.total,ecc.errors.uncorrected.volatile.total --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must query ECC error counts",
          "expectedCommands": ["nvidia-smi -q -d ECC"]
        }
      ],
      "hints": [
        "Volatile counters reset on driver reload or system reboot",
        "Aggregate counters persist across reboots (stored in GPU InfoROM)",
        "Single Bit errors are CORRECTED (data integrity maintained)",
        "Double Bit errors are UNCORRECTED (data corruption possible)",
        "Look for 'Corrected' and 'Uncorrected' error counts",
        "Non-zero aggregate counts indicate historical issues"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "ECC Error Counters",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Investigate SRAM vs DRAM Errors",
      "description": "Drill down into the ECC error details to determine if errors are in SRAM (caches) or DRAM (HBM). This distinction is important for determining the severity and required action.",
      "objectives": [
        "Identify SRAM ECC errors",
        "Identify DRAM ECC errors",
        "Understand error location significance",
        "Compare L1/L2 cache vs texture memory vs device memory errors"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ECC",
        "nvidia-smi -i 3 -q -d ECC"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must investigate ECC error locations",
          "expectedCommands": ["nvidia-smi -q -d ECC", "nvidia-smi -i"]
        }
      ],
      "hints": [
        "SRAM errors occur in: L1 Cache, L2 Cache, Register File, Shared Memory",
        "DRAM errors occur in: Device Memory (HBM/GDDR)",
        "SRAM single-bit errors are generally less concerning",
        "DRAM errors, especially recurring ones, indicate memory cell degradation",
        "Use '-i GPU_ID' to query a specific GPU",
        "Multiple DRAM errors on same GPU = potential hardware issue"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "GPU Memory Architecture",
          "url": "https://docs.nvidia.com/deploy/gpu-memory-errors/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Check Row Remapping Status",
      "description": "Modern NVIDIA GPUs (Ampere and newer) feature Row Remapping - a mechanism that retires faulty memory rows and replaces them with spare rows. Check the row remapping status to understand memory health.",
      "objectives": [
        "Query row remapping status",
        "Understand pending vs completed remaps",
        "Check remaining spare row capacity",
        "Identify if any remappings have failed"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ROW_REMAPPER",
        "nvidia-smi --query-gpu=index,remapped_rows.pending,remapped_rows.failure --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check row remapping status",
          "expectedCommands": ["nvidia-smi -q -d ROW_REMAPPER", "nvidia-smi"]
        }
      ],
      "hints": [
        "Row Remapping: Spare Rows Available, Remapped Rows, Pending, Failure",
        "'Pending: Yes' means reboot required to apply remapping",
        "'Failure: Yes' means spare rows exhausted - GPU REPLACEMENT NEEDED",
        "Remapping is transparent to applications once applied",
        "Each GPU has limited spare rows (typically ~40-60)",
        "XID 64 indicates row remapping threshold reached",
        "XID 63 indicates row remapping failure - CRITICAL"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "Row Remapping",
          "url": "https://docs.nvidia.com/deploy/gpu-memory-errors/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Correlate ECC Errors with XID Codes",
      "description": "ECC errors generate specific XID error codes in the system logs. Learn to correlate ECC counters with XID messages for comprehensive diagnostics.",
      "objectives": [
        "Search system logs for ECC-related XID errors",
        "Understand XID 48 (Double-bit ECC)",
        "Understand XID 92/94/95 (Contained/Uncontained ECC)",
        "Understand XID 63/64 (Row Remapping)"
      ],
      "expectedCommands": [
        "dmesg | grep -i xid",
        "dmesg | grep -i ecc",
        "journalctl -k | grep -i nvrm"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check system logs for ECC-related XID",
          "expectedCommands": ["dmesg | grep -i xid", "dmesg | grep"]
        }
      ],
      "hints": [
        "XID 48: Double-bit ECC error - UNCORRECTABLE, data corruption",
        "XID 63: Row remapping failure - GPU REPLACEMENT REQUIRED",
        "XID 64: Row remapping threshold reached - Plan replacement",
        "XID 92: High single-bit ECC error rate - Monitor closely",
        "XID 94: Contained ECC error - Workload may continue",
        "XID 95: Uncontained ECC error - Data integrity compromised",
        "Multiple XID 48 = immediate hardware replacement"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "XID Error Reference",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Run DCGM Memory Diagnostics",
      "description": "Use DCGM to run targeted memory diagnostics that stress-test GPU memory and detect latent ECC issues.",
      "objectives": [
        "Run DCGM Level 2 diagnostics for memory test",
        "Run DCGM Level 3 for extended memory stress",
        "Interpret diagnostic memory test results",
        "Identify memory subsystem issues"
      ],
      "expectedCommands": [
        "dcgmi diag -r 2",
        "dcgmi diag -r 3"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run DCGM memory diagnostics",
          "expectedCommands": ["dcgmi diag -r 2", "dcgmi diag"]
        }
      ],
      "hints": [
        "Level 2 includes quick memory bandwidth test",
        "Level 3 includes extended memory stress test",
        "Memory diagnostic tests entire GPU memory",
        "Test will FAIL if uncorrectable errors detected",
        "ECC errors during diagnostic indicate hardware issue",
        "Run diagnostics when GPU is idle for accurate results"
      ],
      "estimatedDuration": 10,
      "documentationLinks": [
        {
          "title": "DCGM Diagnostics",
          "url": "https://docs.nvidia.com/datacenter/dcgm/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Determine Required Action",
      "description": "Based on the ECC error investigation, determine the appropriate action: monitor, schedule maintenance, or immediate replacement.",
      "objectives": [
        "Assess error severity and trend",
        "Determine if immediate action required",
        "Document findings for RMA if needed",
        "Create monitoring plan"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d ECC",
        "nvidia-smi -q -d ROW_REMAPPER",
        "nvsm show health"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must assess final status",
          "expectedCommands": ["nvidia-smi -q", "nvsm show health"]
        }
      ],
      "hints": [
        "IMMEDIATE REPLACEMENT: Double-bit errors, row remapping failure",
        "SCHEDULE REPLACEMENT: High single-bit rate, row remapping pending",
        "MONITOR: Occasional single-bit errors, stable over time",
        "Document: GPU UUID, error counts, timestamps for RMA",
        "Clear volatile counters with 'nvidia-smi -e 0' then '-e 1'",
        "Resetting counters helps track error rate going forward"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU RMA Process",
          "url": "https://www.nvidia.com/en-us/support/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Queried and understood ECC error types (volatile vs aggregate)",
    "Identified SRAM vs DRAM error locations",
    "Checked row remapping status and thresholds",
    "Correlated ECC counters with XID error codes",
    "Ran DCGM memory diagnostics",
    "Determined appropriate action based on findings"
  ],
  "estimatedTime": 50,
  "prerequisites": [
    "domain4-dcgmi-diag"
  ],
  "tags": [
    "ecc",
    "memory-errors",
    "row-remapping",
    "xid",
    "hardware-diagnostics",
    "rma",
    "advanced",
    "exam-critical"
  ]
}
