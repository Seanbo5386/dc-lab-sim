{
  "id": "domain2-nvlink-topo",
  "title": "NVLink Topology Verification",
  "domain": "domain2",
  "difficulty": "intermediate",
  "description": "Learn how to verify and validate NVLink topology on DGX systems. NVLink provides high-speed GPU-to-GPU interconnect that is critical for multi-GPU training and HPC workloads.",
  "learningObjectives": [
    "Understand NVLink architecture and topology",
    "Verify NVLink connectivity between GPUs",
    "Validate NVLink link status and bandwidth",
    "Identify NVLink configuration issues"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Check NVLink Support and Version",
      "description": "Verify that your GPUs support NVLink and identify the NVLink version (Gen3, Gen4, etc.) and maximum link count.",
      "objectives": [
        "Query GPU NVLink capability",
        "Identify NVLink generation",
        "Determine maximum NVLink connections per GPU"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink --capabilities",
        "nvidia-smi --query-gpu=gpu_name,nvlink.link_count --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check NVLink support",
          "expectedCommands": [
            "nvidia-smi nvlink --capabilities",
            "nvidia-smi --query-gpu=nvlink.link_count"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink --capabilities' to see NVLink features",
        "H100 GPUs support 18 NVLink 4.0 links",
        "A100 GPUs support 12 NVLink 3.0 links",
        "NVLink 4.0 provides 50 GB/s per link per direction (900 GB/s bidirectional per GPU on H100)",
        "Consumer GPUs (RTX series) do not support NVLink in most cases"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVLink Architecture",
          "url": "https://www.nvidia.com/en-us/data-center/nvlink/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Display NVLink Topology Matrix",
      "description": "Generate the NVLink topology matrix showing GPU-to-GPU connectivity. This matrix shows which GPUs are directly connected via NVLink.",
      "objectives": [
        "Generate GPU topology matrix",
        "Understand topology matrix notation",
        "Identify NVLink connections between GPUs"
      ],
      "expectedCommands": ["nvidia-smi topo -m", "nvidia-smi topo --matrix"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must display topology matrix",
          "expectedCommands": [
            "nvidia-smi topo -m",
            "nvidia-smi topo --matrix"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi topo -m' for the topology matrix",
        "Matrix notation: 'NV#' = NVLink (# indicates number of links)",
        "Matrix notation: 'SYS' = Connection through PCIe host bridge",
        "Matrix notation: 'NODE' = Connection through PCIe + NUMA",
        "Matrix notation: 'PHB' = Connection through PCIe host bridge",
        "On DGX H100, expect 'NV18' between adjacent GPUs in same baseboard"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Topology",
          "url": "https://docs.nvidia.com/cuda/cuda-c-programming-guide/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Check Individual NVLink Status",
      "description": "Query the status of each individual NVLink connection to ensure all links are active and healthy.",
      "objectives": [
        "Query NVLink status for all GPUs",
        "Verify all links are in active state",
        "Identify any disabled or failed links"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink --status",
        "nvidia-smi nvlink -g 0 --status"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check NVLink link status",
          "expectedCommands": [
            "nvidia-smi nvlink --status",
            "nvidia-smi nvlink -g 0 --status"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink --status' to see all NVLink statuses",
        "Use 'nvidia-smi nvlink -g 0 --status' for GPU 0 only",
        "Link states: 'Active (0)' = Operational, '(0)' means link 0",
        "Link states: 'Disabled' = Link is not configured/available",
        "Link states: 'Down' = Link is expected but not functioning (ERROR)",
        "All configured links should show 'Active' status"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NVLink Status Monitoring",
          "url": "https://docs.nvidia.com/datacenter/tesla/pdf/fabric-manager-user-guide.pdf"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Query NVLink Error Counters",
      "description": "Check NVLink error counters to identify any transmission errors, CRC errors, or replay events that might indicate link quality issues.",
      "objectives": [
        "Query NVLink error counters",
        "Understand different error types",
        "Identify problematic links"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink -e",
        "nvidia-smi nvlink --errors"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check NVLink error counters",
          "expectedCommands": [
            "nvidia-smi nvlink -e",
            "nvidia-smi nvlink --errors"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink -e' to display error counters",
        "Error types: CRC errors, ECC errors, replay events",
        "Low replay counts (<100) are normal during initialization",
        "High or increasing CRC errors indicate cable/signal integrity issues",
        "Use 'nvidia-smi nvlink -r' to reset error counters",
        "Monitor counters over time to identify degrading links"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NVLink Error Monitoring",
          "url": "https://docs.nvidia.com/datacenter/tesla/pdf/fabric-manager-user-guide.pdf"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Verify NVLink Bandwidth",
      "description": "Check the configured bandwidth for NVLink connections and ensure they're operating at expected speeds.",
      "objectives": [
        "Query NVLink bandwidth per link",
        "Calculate total NVLink bandwidth per GPU",
        "Compare against specification"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink -g 0 --status",
        "nvidia-smi --query-gpu=gpu_name,nvlink.link_count --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify NVLink bandwidth",
          "expectedCommands": [
            "nvidia-smi nvlink",
            "nvidia-smi --query-gpu=nvlink"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NVLink 4.0 (H100): 25 GB/s per link per direction",
        "H100 with 18 links: 18 Ã— 25 GB/s = 450 GB/s per direction",
        "Total bidirectional bandwidth: 900 GB/s per H100 GPU",
        "NVLink 3.0 (A100): 25 GB/s per link per direction (600 GB/s bidirectional)",
        "Compare active link count against GPU specification"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVLink Specifications",
          "url": "https://www.nvidia.com/en-us/data-center/nvlink/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Understand NVSwitch Topology (If Applicable)",
      "description": "On systems with NVSwitch (like DGX H100), understand how NVSwitch provides full GPU-to-GPU connectivity.",
      "objectives": [
        "Understand NVSwitch architecture",
        "Identify NVSwitch connectivity",
        "Verify all GPUs can communicate at full NVLink speed"
      ],
      "expectedCommands": [
        "nvidia-smi topo -m",
        "nvidia-smi --query-gpu=name --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must understand NVSwitch topology",
          "expectedCommands": ["nvidia-smi topo -m"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NVSwitch provides full bisection bandwidth between all GPUs",
        "DGX H100 has 4 NVSwitches providing 18 TB/s aggregate bandwidth",
        "With NVSwitch, all GPU-to-GPU connections show as 'NV18' in topology",
        "Without NVSwitch, only adjacent GPUs have NVLink connections",
        "NVSwitch eliminates PCIe bottlenecks for multi-GPU communication"
      ],
      "estimatedDuration": 6,
      "documentationLinks": [
        {
          "title": "NVSwitch Architecture",
          "url": "https://www.nvidia.com/en-us/data-center/nvlink/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified NVLink support and version",
    "Generated and understood topology matrix",
    "Checked NVLink status on all GPUs",
    "Reviewed NVLink error counters",
    "Verified NVLink bandwidth configuration",
    "Understood NVSwitch topology (if applicable)"
  ],
  "estimatedTime": 35,
  "prerequisites": ["domain1-driver-install"],
  "tags": [
    "nvlink",
    "topology",
    "gpu-interconnect",
    "nvswitch",
    "intermediate"
  ],
  "tier": 2,
  "commandFamilies": ["gpu-monitoring"],
  "explanationGateId": "gate-domain2-nvlink-topo"
}
