{
  "id": "domain2-nvlink-recovery",
  "title": "NVLink Error Detection and Recovery",
  "domain": "domain2",
  "difficulty": "advanced",
  "description": "Learn how to detect, diagnose, and recover from NVLink errors in DGX systems. NVLink provides high-bandwidth GPU-to-GPU communication and errors can significantly impact performance.",
  "learningObjectives": [
    "Monitor NVLink error counters",
    "Interpret NVLink error types",
    "Diagnose NVLink degradation",
    "Perform NVLink recovery procedures",
    "Understand when hardware replacement is needed"
  ],
  "faults": [
    {
      "type": "nvlink_error",
      "gpuIndex": 2,
      "linkIndex": 3,
      "severity": "warning"
    }
  ],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Check NVLink Status Overview",
      "description": "Get an overview of NVLink status across all GPUs in the system.",
      "objectives": [
        "View NVLink topology",
        "Check link status",
        "Identify potential issues"
      ],
      "expectedCommands": ["nvidia-smi nvlink --status", "nvidia-smi topo -m"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check NVLink status",
          "expectedCommands": [
            "nvidia-smi nvlink --status",
            "nvidia-smi topo -m"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink --status' for link status",
        "All links should show 'Active' state",
        "Inactive links indicate problems",
        "Topology matrix shows NV# for NVLink connections",
        "DGX A100: 12 NVLink connections per GPU"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVLink Status",
          "url": "https://docs.nvidia.com/datacenter/nvlink/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Query NVLink Error Counters",
      "description": "Check NVLink error counters to identify problematic links.",
      "objectives": [
        "Query error counters for all links",
        "Identify links with errors",
        "Understand error types"
      ],
      "expectedCommands": ["nvidia-smi nvlink -e", "nvidia-smi nvlink -e -i 0"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must query error counters",
          "expectedCommands": ["nvidia-smi nvlink -e"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink -e' for error counters",
        "CRC errors indicate signal integrity issues",
        "Replay errors show retransmissions",
        "Recovery errors are more serious",
        "Non-zero counters warrant investigation"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVLink Error Counters",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Identify Affected Links",
      "description": "Determine which specific links are experiencing errors.",
      "objectives": [
        "Map error counters to physical links",
        "Identify affected GPU pairs",
        "Determine error severity"
      ],
      "expectedCommands": [
        "nvidia-smi nvlink -e -i 2",
        "nvidia-smi nvlink --status -i 2"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must identify affected links",
          "expectedCommands": [
            "nvidia-smi nvlink -e -i 2",
            "nvidia-smi nvlink --status -i 2"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use -i to specify GPU index",
        "Link index corresponds to physical NVLink lane",
        "Remote GPU shown for each link",
        "High CRC counts suggest cable or connector issues",
        "Correlate with thermal issues"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NVLink Troubleshooting",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Check for Thermal Issues",
      "description": "Verify that NVLink errors are not caused by thermal problems.",
      "objectives": [
        "Check GPU temperatures",
        "Review thermal throttling",
        "Check cooling system"
      ],
      "expectedCommands": [
        "nvidia-smi -q -d TEMPERATURE",
        "nvidia-smi --query-gpu=temperature.gpu --format=csv",
        "ipmitool sensor list | grep -i temp"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check thermal status",
          "expectedCommands": [
            "nvidia-smi -q -d TEMPERATURE",
            "ipmitool sensor list"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "GPU temperature should be < 83Â°C",
        "NVLink has separate thermal limits",
        "High temperatures can cause signal integrity issues",
        "Check ambient temperature and airflow",
        "Thermal throttling reduces performance"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Thermal Management",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Reset NVLink Error Counters",
      "description": "Clear error counters to establish a baseline for monitoring.",
      "objectives": [
        "Reset error counters",
        "Establish monitoring baseline",
        "Prepare for error rate measurement"
      ],
      "expectedCommands": ["nvidia-smi nvlink -r", "nvidia-smi nvlink -e"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must reset error counters",
          "expectedCommands": ["nvidia-smi nvlink -r", "nvidia-smi nvlink -e"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi nvlink -r' to reset counters",
        "Document pre-reset values first",
        "Monitor counters after reset",
        "Rapidly increasing counters indicate active issues",
        "Compare error rates across links"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVLink Monitoring",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Test NVLink Bandwidth",
      "description": "Verify NVLink bandwidth to detect performance degradation.",
      "objectives": [
        "Understand bandwidth expectations",
        "Identify bandwidth reduction",
        "Correlate with error counters"
      ],
      "expectedCommands": ["nvidia-smi topo -m", "nvidia-smi nvlink --status"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must test NVLink bandwidth",
          "expectedCommands": [
            "nvidia-smi topo -m",
            "nvidia-smi nvlink --status"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "P2P bandwidth test in CUDA samples",
        "A100: ~600 GB/s bidirectional expected",
        "H100: ~900 GB/s bidirectional expected",
        "Degraded links show reduced bandwidth",
        "NCCL tests also reveal NVLink issues"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "NVLink Bandwidth",
          "url": "https://docs.nvidia.com/datacenter/nvlink/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Attempt Software Recovery",
      "description": "Try software-based recovery methods for NVLink issues.",
      "objectives": [
        "Reset GPU with NVLink errors",
        "Restart Fabric Manager",
        "Verify recovery"
      ],
      "expectedCommands": [
        "nvidia-smi -r -i 2",
        "systemctl restart nvidia-fabricmanager",
        "nvidia-smi nvlink --status"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must attempt recovery",
          "expectedCommands": [
            "nvidia-smi -r",
            "systemctl restart nvidia-fabricmanager"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "GPU reset may clear transient errors",
        "Fabric Manager restart reinitializes NVSwitch",
        "Requires no active workloads on GPU",
        "Check error counters after recovery",
        "Persistent errors indicate hardware issues"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "GPU Reset",
          "url": "https://docs.nvidia.com/datacenter/"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Document and Escalate",
      "description": "If errors persist, document findings and prepare for hardware escalation.",
      "objectives": [
        "Generate diagnostic report",
        "Document error patterns",
        "Prepare for support escalation"
      ],
      "expectedCommands": [
        "nvidia-bug-report.sh",
        "nvidia-smi nvlink -e",
        "nvidia-smi -q"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must document findings",
          "expectedCommands": ["nvidia-bug-report.sh", "nvidia-smi nvlink -e"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Run nvidia-bug-report.sh for comprehensive diagnostics",
        "Document error patterns and timestamps",
        "Note environmental conditions",
        "Persistent NVLink errors may require GPU replacement",
        "Contact NVIDIA Enterprise Support"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "NVIDIA Support",
          "url": "https://enterprise-support.nvidia.com/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Checked NVLink status overview",
    "Queried NVLink error counters",
    "Identified affected links",
    "Checked for thermal issues",
    "Reset error counters",
    "Tested NVLink bandwidth",
    "Attempted software recovery",
    "Documented findings for escalation"
  ],
  "estimatedTime": 50,
  "prerequisites": ["domain2-nvlink-topo"],
  "tags": ["nvlink", "error-recovery", "troubleshooting", "fabric", "advanced"],
  "tier": 3,
  "commandFamilies": ["gpu-monitoring", "bmc-hardware", "diagnostics"],
  "explanationGateId": "gate-domain2-nvlink-recovery",
  "toolHints": false
}
