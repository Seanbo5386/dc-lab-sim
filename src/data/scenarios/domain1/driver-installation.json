{
  "id": "domain1-driver-install",
  "title": "NVIDIA Driver Installation and Validation",
  "domain": "domain1",
  "difficulty": "intermediate",
  "description": "Learn the proper procedures for installing, verifying, and validating NVIDIA GPU drivers on DGX systems. This scenario covers driver installation best practices, verification methods, and common troubleshooting steps.",
  "learningObjectives": [
    "Understand NVIDIA driver versioning and compatibility",
    "Verify driver installation and loading",
    "Validate GPU detection and driver binding",
    "Troubleshoot common driver installation issues"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Check Current Driver Status",
      "description": "Before installing or updating drivers, verify the current driver installation status, version, and whether the driver is properly loaded.",
      "objectives": [
        "Check if NVIDIA driver is currently loaded",
        "Verify current driver version",
        "List loaded NVIDIA kernel modules"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "lsmod | grep nvidia",
        "modinfo nvidia"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must check driver status with nvidia-smi",
          "expectedCommands": ["nvidia-smi"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi' to check if driver is loaded and see version",
        "Use 'lsmod | grep nvidia' to see loaded NVIDIA kernel modules",
        "Use 'modinfo nvidia' to see detailed driver module information",
        "If nvidia-smi fails, the driver may not be loaded or installed"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVIDIA Driver Documentation",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Verify GPU Hardware Detection",
      "description": "Confirm that all GPU devices are detected by the PCIe bus and visible to the operating system, independent of driver installation.",
      "objectives": [
        "List all NVIDIA GPU devices on PCIe bus",
        "Verify GPU PCIe addresses",
        "Check PCIe link speed and width"
      ],
      "expectedCommands": ["lspci | grep -i nvidia", "lspci -d 10de: -vv"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must list NVIDIA PCIe devices",
          "expectedCommands": ["lspci | grep -i nvidia", "lspci -d 10de:"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'lspci | grep -i nvidia' to list NVIDIA devices",
        "10de is NVIDIA's PCI vendor ID",
        "Use 'lspci -d 10de: -vv' for verbose output including link speed",
        "Expected link speed: Gen4 x16 for H100 GPUs",
        "Count of devices should match physical GPU installation"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "PCIe Device Detection",
          "url": "https://linux.die.net/man/8/lspci"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Verify Driver Version Compatibility",
      "description": "Check that the installed driver version is compatible with your GPU model, CUDA version, and operating system.",
      "objectives": [
        "Identify GPU architecture and compute capability",
        "Verify driver version matches CUDA toolkit requirements",
        "Check OS kernel compatibility"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=gpu_name,driver_version,cuda_version --format=csv",
        "uname -r"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify driver and CUDA compatibility",
          "expectedCommands": [
            "nvidia-smi --query-gpu=gpu_name,driver_version,cuda_version --format=csv",
            "nvidia-smi --query-gpu"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use nvidia-smi with --query-gpu to get structured version information",
        "H100 GPUs require driver version 450.80.02 or later",
        "CUDA version shown by nvidia-smi is the maximum CUDA runtime version supported",
        "Check NVIDIA driver release notes for kernel compatibility matrix"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "CUDA Compatibility Guide",
          "url": "https://docs.nvidia.com/deploy/cuda-compatibility/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Validate GPU Functionality",
      "description": "After verifying driver installation, test basic GPU functionality to ensure the driver can communicate with the GPUs and perform operations.",
      "objectives": [
        "Query all GPU properties",
        "Test GPU memory visibility",
        "Verify persistence mode is enabled",
        "Check ECC status"
      ],
      "expectedCommands": [
        "nvidia-smi -q",
        "nvidia-smi -q -d MEMORY",
        "nvidia-smi -pm 1"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must query GPU properties",
          "expectedCommands": ["nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi -q' for detailed GPU query (all properties)",
        "Use 'nvidia-smi -q -d MEMORY' to focus on memory information",
        "Persistence mode keeps driver loaded between jobs (recommended for HPC)",
        "Use 'nvidia-smi -pm 1' to enable persistence mode",
        "ECC should be enabled on datacenter GPUs for reliability"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "nvidia-smi Documentation",
          "url": "https://developer.nvidia.com/nvidia-system-management-interface"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Check Driver Log Messages",
      "description": "Review kernel and driver log messages to identify any warnings, errors, or issues that occurred during driver loading or GPU initialization.",
      "objectives": [
        "Review kernel messages related to NVIDIA driver",
        "Identify any driver loading errors",
        "Check for GPU initialization warnings",
        "Verify no PCIe errors occurred"
      ],
      "expectedCommands": [
        "dmesg | grep -i nvidia",
        "dmesg | grep -i nvrm",
        "journalctl -k | grep -i nvidia"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must review driver logs",
          "expectedCommands": [
            "dmesg | grep -i nvidia",
            "dmesg | grep -i nvrm"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'dmesg | grep -i nvidia' to see NVIDIA-related kernel messages",
        "NVRM is the NVIDIA Resource Manager (driver) prefix",
        "Look for messages like 'NVRM: loading NVIDIA UNIX x86_64 Kernel Module'",
        "Watch for any 'Xid' error messages (GPU hardware errors)",
        "PCIe link warnings may indicate hardware or BIOS configuration issues"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVIDIA Driver Troubleshooting",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Verify GPU Topology",
      "description": "Verify GPU topology and connectivity to understand NVLink and PCIe interconnects between GPUs.",
      "objectives": [
        "Display GPU topology matrix",
        "Verify NVLink connections",
        "Check PCIe topology"
      ],
      "expectedCommands": ["nvidia-smi topo -m", "nvidia-smi topo --matrix"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must verify GPU topology",
          "expectedCommands": [
            "nvidia-smi topo -m",
            "nvidia-smi topo --matrix"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'nvidia-smi topo -m' to see GPU topology matrix",
        "NVLink connections should show as 'NV#' in topology matrix",
        "PCIe connections show as 'PHB' or 'PIX' in topology",
        "All GPUs should be interconnected for optimal multi-GPU performance"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Topology",
          "url": "https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA-Multi-Instance-GPU-User-Guide.pdf"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Configure Driver Persistence Mode",
      "description": "Set up NVIDIA persistence mode to ensure the driver remains loaded between GPU jobs, reducing initialization latency.",
      "objectives": [
        "Understand persistence mode benefits",
        "Enable persistence mode on all GPUs",
        "Verify persistence daemon is running"
      ],
      "expectedCommands": [
        "nvidia-smi -pm 1",
        "nvidia-smi -q | grep -i persistence"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must enable persistence mode",
          "expectedCommands": ["nvidia-smi -pm 1"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Persistence mode keeps driver loaded even when no processes are using GPUs",
        "Use 'nvidia-smi -pm 1' to enable persistence mode",
        "Use 'nvidia-smi -pm 0' to disable persistence mode",
        "Check status with 'nvidia-smi -q | grep -i persistence'",
        "Persistence mode is critical for shared HPC systems to reduce job startup time"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVIDIA Persistence Daemon",
          "url": "https://docs.nvidia.com/deploy/driver-persistence/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified current driver status and version",
    "Confirmed all GPUs are detected via PCIe",
    "Validated driver version compatibility",
    "Tested basic GPU functionality with nvidia-smi",
    "Reviewed driver logs for errors",
    "Verified GPU topology and connectivity",
    "Configured persistence mode for optimal performance"
  ],
  "estimatedTime": 37,
  "prerequisites": ["domain1-server-post"],
  "tags": [
    "driver",
    "nvidia-smi",
    "cuda",
    "gpu",
    "installation",
    "validation",
    "intermediate"
  ],
  "tier": 1,
  "commandFamilies": ["gpu-monitoring", "diagnostics"],
  "toolHints": true
}
