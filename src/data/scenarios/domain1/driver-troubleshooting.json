{
  "id": "domain1-driver-troubleshoot",
  "title": "GPU Driver Troubleshooting",
  "domain": "domain1",
  "difficulty": "intermediate",
  "description": "Learn to diagnose and resolve common GPU driver issues. This scenario simulates real-world driver problems including version mismatches, missing modules, and GPU initialization failures.",
  "learningObjectives": [
    "Diagnose driver loading failures",
    "Resolve version compatibility issues",
    "Fix GPU initialization problems",
    "Understand driver error messages in logs",
    "Learn systematic troubleshooting methodology"
  ],
  "faults": [
    {
      "nodeId": "dgx-00",
      "gpuId": 2,
      "type": "driver-issue",
      "parameters": {
        "issue": "fallen-off-bus",
        "description": "GPU 2 has fallen off the PCIe bus"
      }
    },
    {
      "nodeId": "dgx-01",
      "type": "driver-issue",
      "parameters": {
        "issue": "module-mismatch",
        "description": "Kernel module version mismatch"
      }
    }
  ],
  "initialClusterState": {
    "simulatedErrors": {
      "dgx-00": {
        "driverIssue": "GPU 2 not responding",
        "errorInDmesg": true
      },
      "dgx-01": {
        "driverIssue": "Module version mismatch",
        "nvidiaSmiFails": true
      }
    }
  },
  "steps": [
    {
      "id": "step1",
      "title": "Initial Problem Discovery",
      "description": "Users are reporting that some GPUs are not available. Start by identifying which nodes have issues.",
      "objectives": [
        "Check nvidia-smi on all nodes",
        "Identify which nodes have GPU problems",
        "Document the symptoms"
      ],
      "expectedCommands": ["nvidia-smi", "nvidia-smi -L"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check GPU status",
          "expectedCommands": ["nvidia-smi"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Run nvidia-smi on dgx-00 and dgx-01",
        "Look for error messages or missing GPUs",
        "On dgx-00, one GPU might be missing from the list",
        "On dgx-01, nvidia-smi might fail entirely"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Common nvidia-smi Errors",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#common-errors"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Check Kernel Messages",
      "description": "Examine kernel logs to understand what's happening at the driver level.",
      "objectives": [
        "Review dmesg for NVIDIA driver messages",
        "Look for XID errors or initialization failures",
        "Identify the root cause from error messages"
      ],
      "expectedCommands": [
        "dmesg | grep -i nvidia",
        "dmesg | grep -i nvrm",
        "dmesg | grep -i xid"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check kernel logs",
          "expectedCommands": [
            "dmesg | grep -i nvidia",
            "dmesg | grep -i nvrm"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Look for 'NVRM' messages - these are from the NVIDIA driver",
        "XID errors indicate hardware or driver issues",
        "'fallen off the bus' means PCIe communication lost",
        "Version mismatch errors show incompatible components"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Understanding XID Errors",
          "url": "https://docs.nvidia.com/deploy/xid-errors/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Verify Driver Module Status",
      "description": "Check if the NVIDIA kernel modules are loaded correctly and their versions match.",
      "objectives": [
        "List loaded NVIDIA modules",
        "Check module versions",
        "Verify module dependencies"
      ],
      "expectedCommands": [
        "lsmod | grep nvidia",
        "modinfo nvidia",
        "cat /proc/driver/nvidia/version"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check kernel modules",
          "expectedCommands": ["lsmod | grep nvidia", "modinfo nvidia"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Compare module version with driver version",
        "All nvidia modules should be from same version",
        "Missing modules indicate incomplete installation",
        "Check for 'nvidia', 'nvidia_modeset', 'nvidia_uvm' modules"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NVIDIA Kernel Modules",
          "url": "https://download.nvidia.com/XFree86/Linux-x86_64/550.54.15/README/kernel_modules.html"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Diagnose PCIe Issues (dgx-00)",
      "description": "For the GPU that has 'fallen off the bus', investigate PCIe connectivity issues.",
      "objectives": [
        "Check PCIe device visibility",
        "Verify PCIe link status",
        "Attempt to reset the PCIe device"
      ],
      "expectedCommands": [
        "lspci | grep -i nvidia",
        "lspci -vv -s <bus_id>",
        "nvidia-smi -r -i 2"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check PCIe status",
          "expectedCommands": ["lspci | grep -i nvidia"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Compare lspci output with nvidia-smi -L",
        "Missing GPU in nvidia-smi but present in lspci indicates driver issue",
        "Check PCIe link speed and width",
        "GPU reset might restore connectivity"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "PCIe Troubleshooting",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#pcie-troubleshooting"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Fix Module Version Mismatch (dgx-01)",
      "description": "Resolve the kernel module version mismatch preventing driver from loading.",
      "objectives": [
        "Identify mismatched versions",
        "Unload incorrect modules",
        "Reload correct driver version"
      ],
      "expectedCommands": [
        "rmmod nvidia_uvm nvidia_modeset nvidia",
        "modprobe nvidia",
        "nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Reload driver modules",
          "expectedCommands": ["modprobe nvidia", "nvidia-smi"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Modules must be unloaded in reverse dependency order",
        "Start with nvidia_uvm, then nvidia_modeset, then nvidia",
        "After unloading, use modprobe to load correct version",
        "Verify with nvidia-smi after reloading"
      ],
      "estimatedDuration": 6,
      "documentationLinks": [
        {
          "title": "Module Loading/Unloading",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#kernel-module-loading"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Reset and Recover GPU (dgx-00)",
      "description": "Attempt to recover the GPU that fell off the bus through various reset methods.",
      "objectives": [
        "Try GPU reset via nvidia-smi",
        "If that fails, try PCIe rescan",
        "Verify GPU is back online"
      ],
      "expectedCommands": [
        "nvidia-smi -r -i 2",
        "echo 1 > /sys/bus/pci/rescan",
        "nvidia-smi -L"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Attempt GPU recovery",
          "expectedCommands": ["nvidia-smi -r", "echo 1 > /sys/bus/pci/rescan"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "nvidia-smi reset might fail if GPU is truly offline",
        "PCIe rescan forces the kernel to re-enumerate devices",
        "May need to reload driver after PCIe rescan",
        "In production, might require node reboot"
      ],
      "estimatedDuration": 6,
      "documentationLinks": [
        {
          "title": "GPU Reset Procedures",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#gpu-reset"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Verify Resolution and Document",
      "description": "Confirm all GPUs are operational and document the troubleshooting steps for future reference.",
      "objectives": [
        "Verify all GPUs are visible and functional",
        "Run basic GPU test to confirm operation",
        "Document the issue and resolution"
      ],
      "expectedCommands": [
        "nvidia-smi",
        "nvidia-smi -q | grep -i error",
        "nvidia-smi --query-gpu=index,name,persistence_mode --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Verify GPU recovery",
          "expectedCommands": ["nvidia-smi", "nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "All 8 GPUs should be visible on each node",
        "No errors should appear in nvidia-smi -q output",
        "Consider enabling persistence mode to prevent future issues",
        "Document which solutions worked for each problem"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Post-Recovery Validation",
          "url": "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html#verification"
        }
      ]
    }
  ],
  "successCriteria": [
    "Identified GPU fallen off bus on dgx-00",
    "Diagnosed module version mismatch on dgx-01",
    "Successfully recovered GPU on dgx-00",
    "Resolved driver loading issue on dgx-01",
    "All GPUs operational on both nodes",
    "Documented troubleshooting methodology"
  ],
  "estimatedTime": 44,
  "prerequisites": ["domain1-driver-install"],
  "tags": [
    "troubleshooting",
    "driver",
    "recovery",
    "debugging",
    "intermediate"
  ],
  "tier": 2,
  "commandFamilies": ["gpu-monitoring", "diagnostics"],
  "explanationGateId": "gate-domain1-driver-troubleshoot"
}
