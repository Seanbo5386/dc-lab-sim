{
  "id": "domain1-gpu-discovery",
  "title": "GPU Feature Discovery and Capabilities",
  "domain": "domain1",
  "difficulty": "intermediate",
  "description": "Bridge the gap between basic driver installation and advanced GPU features. Explore GPU capabilities, understand different GPU architectures, and learn when to use advanced features like MIG, NVLink, and GPU Direct.",
  "learningObjectives": [
    "Discover and understand GPU compute capabilities",
    "Compare different GPU architectures and their strengths",
    "Identify which advanced features are available on your GPUs",
    "Understand use cases for MIG, NVLink, and GPU Direct",
    "Learn GPU memory hierarchy and bandwidth characteristics"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Explore GPU Architecture Details",
      "description": "Start by discovering what type of GPUs you have and their architectural capabilities. Understanding your hardware is crucial before using advanced features.",
      "objectives": [
        "Query detailed GPU information including compute capability",
        "Identify GPU architecture generation (Ampere, Hopper, etc.)",
        "Check available GPU memory and bandwidth"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv",
        "nvidia-smi -q -d SUPPORTED_CLOCKS,MEMORY"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Query GPU architecture details",
          "expectedCommands": ["nvidia-smi --query-gpu", "nvidia-smi -q"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Start with nvidia-smi --query-gpu to get structured information",
        "Compute capability tells you the feature set (8.0 = A100, 9.0 = H100)",
        "Use nvidia-smi -q for comprehensive GPU properties",
        "Memory bandwidth is critical for AI/ML workload performance"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "CUDA Compute Capabilities",
          "url": "https://developer.nvidia.com/cuda-gpus"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Discover NVLink Capabilities",
      "description": "Check if your GPUs support NVLink and understand the interconnect topology. NVLink provides high-bandwidth GPU-to-GPU communication.",
      "objectives": [
        "Verify NVLink support and version",
        "Check NVLink topology between GPUs",
        "Understand bandwidth differences vs PCIe"
      ],
      "expectedCommands": ["nvidia-smi nvlink --status", "nvidia-smi topo -m"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check NVLink capabilities",
          "expectedCommands": ["nvidia-smi nvlink", "nvidia-smi topo"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NVLink provides up to 900 GB/s bandwidth between GPUs",
        "Use 'nvidia-smi nvlink --status' to check link health",
        "Topology matrix shows connection types (NV# = NVLink, PIX = PCIe)",
        "Not all GPUs in a system may be fully connected via NVLink"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Understanding NVLink",
          "url": "https://www.nvidia.com/en-us/data-center/nvlink/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Check MIG Support and Capabilities",
      "description": "Determine if your GPUs support Multi-Instance GPU (MIG) mode. MIG allows you to partition GPUs for multi-tenant use cases.",
      "objectives": [
        "Check if GPU model supports MIG",
        "Understand MIG use cases and limitations",
        "View available MIG profiles"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=mig.mode.current,mig.mode.pending --format=csv",
        "nvidia-smi mig -lgip"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check MIG support",
          "expectedCommands": [
            "nvidia-smi --query-gpu=mig.mode",
            "nvidia-smi mig"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Only A100 and H100 GPUs support MIG mode",
        "MIG is ideal for multi-tenant inference or smaller training jobs",
        "MIG instances are isolated with guaranteed QoS",
        "Each MIG instance gets dedicated SM, memory, and bandwidth"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "MIG User Guide",
          "url": "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Explore GPU Direct Technologies",
      "description": "Discover GPU Direct capabilities including RDMA and Storage. These technologies enable direct data transfers bypassing CPU.",
      "objectives": [
        "Check for GPU Direct RDMA support",
        "Understand GPU Direct Storage capabilities",
        "Identify network adapters that support GPU Direct"
      ],
      "expectedCommands": [
        "nvidia-smi -q | grep -i 'GPU Direct'",
        "lspci | grep -i mellanox",
        "ibstat"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check GPU Direct capabilities",
          "expectedCommands": [
            "nvidia-smi -q",
            "lspci | grep -i mellanox",
            "ibstat"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "GPU Direct RDMA requires InfiniBand or RoCE adapters",
        "GPU Direct Storage enables direct SSD to GPU transfers",
        "These features dramatically reduce latency in distributed training",
        "Check for Mellanox/NVIDIA network adapters for RDMA support"
      ],
      "estimatedDuration": 6,
      "documentationLinks": [
        {
          "title": "GPU Direct Overview",
          "url": "https://developer.nvidia.com/gpudirect"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Measure Memory Bandwidth Performance",
      "description": "Understand your GPU's memory subsystem performance characteristics. This is crucial for optimizing workloads.",
      "objectives": [
        "Check current memory frequency and bandwidth",
        "Understand ECC impact on performance",
        "Monitor memory utilization patterns"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=clocks.current.memory,memory.total,memory.free --format=csv",
        "nvidia-smi -q -d MEMORY",
        "nvidia-smi --query-gpu=ecc.mode.current --format=csv"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Measure memory characteristics",
          "expectedCommands": [
            "nvidia-smi --query-gpu=clocks",
            "nvidia-smi -q -d MEMORY"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "H100 has 3.35 TB/s of memory bandwidth with HBM3",
        "ECC reduces memory capacity by ~6% but is essential for reliability",
        "Memory frequency directly impacts bandwidth-bound workloads",
        "Monitor memory usage to identify bottlenecks"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Memory Architecture",
          "url": "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Compare GPU Generations",
      "description": "Understand the differences between GPU generations in your cluster and when to use each type.",
      "objectives": [
        "List all GPU models in the cluster",
        "Compare specifications across models",
        "Understand workload placement strategies"
      ],
      "expectedCommands": [
        "nvidia-smi --query-gpu=index,name,compute_cap,memory.total --format=csv",
        "nvidia-smi -L"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Compare GPU models",
          "expectedCommands": ["nvidia-smi --query-gpu", "nvidia-smi -L"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Newer GPUs (H100) excel at large model training with FP8",
        "Older GPUs may be more cost-effective for inference",
        "Compute capability determines supported features",
        "Match workload requirements to GPU capabilities"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "GPU Architecture Comparison",
          "url": "https://www.nvidia.com/en-us/data-center/resources/datacenter-gpu-comparison/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Decision Matrix: When to Use Advanced Features",
      "description": "Create a decision framework for when to enable advanced GPU features based on workload characteristics.",
      "objectives": [
        "Understand MIG vs full GPU trade-offs",
        "Know when NVLink is beneficial",
        "Identify GPU Direct use cases"
      ],
      "expectedCommands": ["nvidia-smi", "nvidia-smi topo -m"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Review GPU configuration",
          "expectedCommands": ["nvidia-smi"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use MIG for: Multi-tenant inference, development environments, small batch training",
        "Use NVLink for: Large model training, multi-GPU collective operations",
        "Use GPU Direct for: Distributed training, high-frequency trading, real-time analytics",
        "Consider workload memory requirements, communication patterns, and isolation needs"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "GPU Feature Selection Guide",
          "url": "https://docs.nvidia.com/datacenter/tesla/"
        }
      ]
    }
  ],
  "successCriteria": [
    "Discovered GPU compute capabilities and architecture",
    "Verified NVLink topology and bandwidth",
    "Checked MIG support and available profiles",
    "Explored GPU Direct technologies",
    "Measured memory subsystem performance",
    "Compared different GPU generations",
    "Created decision framework for feature usage"
  ],
  "estimatedTime": 38,
  "prerequisites": ["domain1-driver-install"],
  "tags": [
    "gpu",
    "architecture",
    "discovery",
    "capabilities",
    "features",
    "intermediate"
  ],
  "tier": 2,
  "commandFamilies": ["gpu-monitoring", "infiniband-tools"],
  "explanationGateId": "gate-domain1-gpu-discovery"
}
