{
  "id": "domain3-ngc-pipeline",
  "title": "NGC Container Deployment Pipeline",
  "domain": "domain3",
  "difficulty": "intermediate",
  "description": "Learn how to set up a complete NGC container deployment pipeline including authentication, pulling containers, running GPU workloads, and integrating with cluster schedulers.",
  "learningObjectives": [
    "Configure NGC authentication",
    "Pull and manage NGC containers",
    "Run GPU workloads in containers",
    "Integrate NGC with Slurm",
    "Optimize container performance"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Set Up NGC Authentication",
      "description": "Configure NGC API key for accessing NVIDIA container registry.",
      "objectives": [
        "Generate NGC API key",
        "Configure Docker credentials",
        "Verify NGC access"
      ],
      "expectedCommands": [
        "docker login nvcr.io",
        "cat ~/.docker/config.json | grep -i nvcr"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must set up NGC authentication",
          "expectedCommands": ["docker login nvcr.io"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Generate API key at ngc.nvidia.com",
        "Username: $oauthtoken",
        "Password: Your NGC API key",
        "docker login nvcr.io -u '$oauthtoken' -p <key>",
        "Credentials stored in ~/.docker/config.json"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NGC Getting Started",
          "url": "https://ngc.nvidia.com/setup"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Explore NGC Catalog",
      "description": "Browse available containers in the NGC catalog.",
      "objectives": [
        "Understand NGC catalog structure",
        "Find relevant containers",
        "Check container versions"
      ],
      "expectedCommands": [
        "ngc registry image list nvidia/pytorch",
        "ngc registry image info nvidia/pytorch:24.01-py3"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must explore NGC catalog",
          "expectedCommands": ["ngc registry image list"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "NGC CLI: ngc registry image list",
        "Or browse catalog.ngc.nvidia.com",
        "Containers organized by category",
        "Tags show CUDA/cuDNN versions",
        "Release notes document changes"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "NGC Catalog",
          "url": "https://catalog.ngc.nvidia.com/"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Pull NGC Containers",
      "description": "Download NGC containers to local or shared storage.",
      "objectives": [
        "Pull container images",
        "Manage local images",
        "Configure shared storage"
      ],
      "expectedCommands": [
        "docker pull nvcr.io/nvidia/pytorch:24.01-py3",
        "docker images | grep nvidia"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must pull NGC containers",
          "expectedCommands": [
            "docker pull nvcr.io/nvidia/pytorch",
            "docker images"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Pull: docker pull nvcr.io/nvidia/<container>:<tag>",
        "Images can be large (10-20GB)",
        "Consider shared storage for clusters",
        "Enroot can convert to squashfs",
        "Tag conventions: YY.MM-py3"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Pulling NGC Images",
          "url": "https://docs.nvidia.com/ngc/"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Run GPU Containers",
      "description": "Launch containers with GPU access using NVIDIA Container Toolkit.",
      "objectives": [
        "Run container with GPU access",
        "Verify GPU visibility",
        "Test CUDA functionality"
      ],
      "expectedCommands": [
        "docker run --rm --gpus all nvcr.io/nvidia/pytorch:24.01-py3 nvidia-smi",
        "docker run --rm --gpus '\"device=0,1\"' nvcr.io/nvidia/pytorch:24.01-py3 nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run GPU containers",
          "expectedCommands": [
            "docker run --rm --gpus all nvcr.io/nvidia/pytorch"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "--gpus all exposes all GPUs",
        "--gpus '\"device=0,1\"' for specific GPUs",
        "NVIDIA_VISIBLE_DEVICES also works",
        "Container sees GPU via CUDA",
        "Test with nvidia-smi inside container"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Container Toolkit",
          "url": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Configure Container Runtime Options",
      "description": "Optimize container runtime for GPU workloads.",
      "objectives": [
        "Set memory and CPU limits",
        "Configure shared memory",
        "Enable IPC for multi-GPU"
      ],
      "expectedCommands": [
        "docker run --rm --gpus all --shm-size=1g --ulimit memlock=-1 nvcr.io/nvidia/pytorch:24.01-py3 python -c 'import torch; print(torch.cuda.is_available())'",
        "docker run --rm --gpus all --ipc=host nvcr.io/nvidia/pytorch:24.01-py3 nvidia-smi"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must configure runtime options",
          "expectedCommands": [
            "docker run --rm --gpus all --shm-size",
            "docker run --rm --gpus all --ipc"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "--shm-size=1g increases shared memory",
        "--ipc=host enables IPC namespace sharing",
        "--ulimit memlock=-1 for GPU pinned memory",
        "Required for multi-GPU training",
        "PyTorch DataLoader needs shared memory"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Container Best Practices",
          "url": "https://docs.nvidia.com/deeplearning/frameworks/user-guide/"
        }
      ]
    },
    {
      "id": "step6",
      "title": "Mount Data and Model Directories",
      "description": "Configure volume mounts for data and model access.",
      "objectives": [
        "Mount host directories",
        "Configure data paths",
        "Handle permissions"
      ],
      "expectedCommands": [
        "docker run --rm --gpus all -v /data:/data -v /models:/models nvcr.io/nvidia/pytorch:24.01-py3 ls /data",
        "docker run --rm --gpus all --user $(id -u):$(id -g) -v $PWD:/workspace nvcr.io/nvidia/pytorch:24.01-py3 ls -la /workspace"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must mount directories",
          "expectedCommands": ["docker run --rm --gpus all -v"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "-v host:container mounts directory",
        "Use absolute paths for host",
        "--user matches host user permissions",
        "Mount dataset and checkpoint directories",
        "Consider read-only mounts with :ro"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Docker Volumes",
          "url": "https://docs.docker.com/storage/volumes/"
        }
      ]
    },
    {
      "id": "step7",
      "title": "Convert to Enroot for Slurm",
      "description": "Convert Docker images to Enroot format for Slurm integration.",
      "objectives": [
        "Import image to Enroot",
        "Create squashfs container",
        "Prepare for Slurm jobs"
      ],
      "expectedCommands": [
        "enroot import docker://nvcr.io/nvidia/pytorch:24.01-py3",
        "enroot list",
        "ls *.sqsh"
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must convert to Enroot",
          "expectedCommands": ["enroot import docker://nvcr.io", "enroot list"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Enroot creates portable squashfs images",
        "enroot import docker://<image>",
        "Results in .sqsh file",
        "Can import from local Docker cache",
        "Pyxis uses Enroot under the hood"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Enroot",
          "url": "https://github.com/NVIDIA/enroot"
        }
      ]
    },
    {
      "id": "step8",
      "title": "Run Container Jobs with Slurm",
      "description": "Submit containerized GPU jobs using Slurm and Pyxis.",
      "objectives": [
        "Use srun with container",
        "Submit batch container jobs",
        "Handle multi-node containers"
      ],
      "expectedCommands": [
        "srun --gres=gpu:1 --container-image=nvcr.io/nvidia/pytorch:24.01-py3 nvidia-smi",
        "sbatch --gres=gpu:2 --container-image=nvcr.io/nvidia/pytorch:24.01-py3 --wrap=\"python train.py\""
      ],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Must run container jobs with Slurm",
          "expectedCommands": [
            "srun --gres=gpu --container-image",
            "sbatch --gres=gpu --container-image"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "--container-image specifies NGC image",
        "--container-mounts mounts directories",
        "Pyxis plugin handles container setup",
        "Multi-node: uses Enroot's MPI support",
        "SLURM_CONTAINER_* environment variables"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Pyxis",
          "url": "https://github.com/NVIDIA/pyxis"
        }
      ]
    }
  ],
  "successCriteria": [
    "Set up NGC authentication",
    "Explored NGC catalog",
    "Pulled NGC containers",
    "Ran GPU containers",
    "Configured runtime options",
    "Mounted data directories",
    "Converted to Enroot format",
    "Ran container jobs with Slurm"
  ],
  "estimatedTime": 50,
  "prerequisites": ["domain3-container-runtime"],
  "tags": ["ngc", "containers", "docker", "enroot", "pyxis", "intermediate"],
  "tier": 2,
  "commandFamilies": ["container-tools"],
  "explanationGateId": "gate-domain3-ngc-pipeline"
}
