{
  "id": "domain3-slurm-config",
  "title": "Slurm Workload Manager Configuration",
  "domain": "domain3",
  "difficulty": "intermediate",
  "description": "Learn how to configure and manage Slurm workload manager on DGX systems. Slurm is the de-facto standard job scheduler for HPC clusters and is critical for multi-user GPU resource management.",
  "learningObjectives": [
    "Understand Slurm architecture (controller, compute nodes, database)",
    "Verify Slurm installation and service status",
    "Configure partitions and GPU resources (GRES)",
    "Submit and manage GPU jobs"
  ],
  "faults": [],
  "initialClusterState": {},
  "steps": [
    {
      "id": "step1",
      "title": "Verify Slurm Installation and Services",
      "description": "Check that Slurm is properly installed and all required services (slurmctld, slurmd, slurmdbd) are running.",
      "objectives": [
        "Verify Slurm is responding with 'sinfo' command",
        "Check Slurm configuration with 'scontrol show config'",
        "Verify Slurm version information"
      ],
      "expectedCommands": ["sinfo", "scontrol show config"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check Slurm is responding",
          "expectedCommands": ["sinfo"],
          "requireAllCommands": true
        },
        {
          "type": "command-executed",
          "description": "View Slurm configuration",
          "expectedCommands": ["scontrol show config"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'sinfo' to verify Slurm is responding (quick health check)",
        "Use 'scontrol show config' to see full Slurm configuration",
        "Use 'systemctl status slurmctld' on head node for controller status",
        "Use 'systemctl status slurmd' on compute nodes for daemon status",
        "Slurm version should match across all nodes"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Slurm Quick Start Guide",
          "url": "https://slurm.schedmd.com/quickstart.html"
        }
      ]
    },
    {
      "id": "step2",
      "title": "Review Cluster and Partition Configuration",
      "description": "Examine the Slurm cluster configuration including partitions, nodes, and resource limits.",
      "objectives": [
        "List partitions with detailed node information using 'sinfo -Nel'",
        "Show detailed partition configuration with 'scontrol show partition'"
      ],
      "expectedCommands": ["sinfo -Nel", "scontrol show partition"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "View detailed node list",
          "expectedCommands": ["sinfo -Nel"],
          "requireAllCommands": true
        },
        {
          "type": "command-executed",
          "description": "Show partition details",
          "expectedCommands": ["scontrol show partition"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'sinfo' for a compact view of partitions and nodes",
        "Use 'sinfo -Nel' for detailed node list with partition assignment",
        "Use 'scontrol show partition' for detailed partition configuration",
        "Default partition is marked with '*' in sinfo output",
        "Common partitions: batch, interactive, gpu, debug",
        "Time limits prevent long-running jobs from monopolizing resources"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Slurm Partitions",
          "url": "https://slurm.schedmd.com/quickstart.html"
        }
      ]
    },
    {
      "id": "step3",
      "title": "Verify GPU Resource (GRES) Configuration",
      "description": "Check that GPUs are properly configured as Generic Resources (GRES) in Slurm and are visible for job scheduling.",
      "objectives": [
        "Show GRES resources using 'sinfo -o \"%n %G\"' format",
        "Display detailed node information with 'scontrol show node dgx-00'"
      ],
      "expectedCommands": ["sinfo -o \"%n %G\"", "scontrol show node dgx-00"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Display GRES configuration",
          "expectedCommands": ["sinfo -o \"%n %G\"", "sinfo -o \"%20n %10G\""],
          "requireAllCommands": true
        },
        {
          "type": "command-executed",
          "description": "Show node details with GRES",
          "expectedCommands": [
            "scontrol show node dgx-00",
            "scontrol show node dgx-01"
          ],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use 'sinfo -o \"%n %G\"' to show nodes and their GRES",
        "GRES format: gpu:type:count (e.g., gpu:h100:8)",
        "Use 'scontrol show node <nodename>' for detailed node info",
        "Look for 'Gres=' line in output showing gpu configuration",
        "GPU count should match 'nvidia-smi -L' output on that node"
      ],
      "estimatedDuration": 7,
      "documentationLinks": [
        {
          "title": "Slurm GRES Configuration",
          "url": "https://slurm.schedmd.com/gres.html"
        }
      ]
    },
    {
      "id": "step4",
      "title": "Check Node Status and Availability",
      "description": "Verify all compute nodes are in the correct state and available for job scheduling.",
      "objectives": [
        "Review node states with 'sinfo -Nel' (should all be 'idle')",
        "Check for down/drained nodes with 'sinfo -R' (should be empty)"
      ],
      "expectedCommands": ["sinfo -Nel", "sinfo -R"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check detailed node status",
          "expectedCommands": ["sinfo -Nel"],
          "requireAllCommands": true
        },
        {
          "type": "command-executed",
          "description": "Check for node issues",
          "expectedCommands": ["sinfo -R"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Node states: idle, allocated, mixed, drain, down",
        "'idle' = Node available for jobs",
        "'allocated' = Node fully allocated to jobs",
        "'mixed' = Node partially allocated",
        "'drain' = Node completing jobs but won't accept new ones",
        "'down' = Node not responding or administratively down",
        "Use 'sinfo -R' to see reasons for drained/down nodes"
      ],
      "estimatedDuration": 5,
      "documentationLinks": [
        {
          "title": "Slurm Node States",
          "url": "https://slurm.schedmd.com/sinfo.html"
        }
      ]
    },
    {
      "id": "step5",
      "title": "Monitor Job Queue and Status",
      "description": "Learn how to monitor the job queue and check job status in Slurm.",
      "objectives": [
        "Check current job queue with 'squeue'",
        "View job accounting history with 'sacct'"
      ],
      "expectedCommands": ["squeue", "sacct"],
      "validationRules": [
        {
          "type": "command-executed",
          "description": "Check job queue",
          "expectedCommands": ["squeue"],
          "requireAllCommands": true
        },
        {
          "type": "command-executed",
          "description": "View job history",
          "expectedCommands": ["sacct"],
          "requireAllCommands": true
        }
      ],
      "hints": [
        "Use '--gres=gpu:1' to request 1 GPU",
        "Use '--gres=gpu:h100:2' to request 2 H100 GPUs specifically",
        "Use 'squeue' to see queued and running jobs",
        "Use 'squeue -u $USER' to see only your jobs",
        "Job states: PD (pending), R (running), CG (completing), CD (completed)",
        "Use 'scancel <jobid>' to cancel a job",
        "Use 'scontrol show job <jobid>' for detailed job info"
      ],
      "estimatedDuration": 8,
      "documentationLinks": [
        {
          "title": "Slurm Job Submission",
          "url": "https://slurm.schedmd.com/sbatch.html"
        }
      ]
    }
  ],
  "successCriteria": [
    "Verified Slurm services are running",
    "Reviewed partition configuration",
    "Verified GPU GRES configuration",
    "Checked all nodes are available",
    "Monitored job queue and history"
  ],
  "estimatedTime": 32,
  "prerequisites": ["domain1-driver-install"],
  "tags": [
    "slurm",
    "job-scheduler",
    "workload-manager",
    "gpu-scheduling",
    "intermediate"
  ],
  "tier": 2,
  "commandFamilies": ["cluster-tools"],
  "explanationGateId": "gate-domain3-slurm-config"
}
